# âœ… Phase 1B.1: Corrected Vast.ai Paths - Ready to Execute

**Date:** October 27, 2025  
**Critical Fix:** Vast.ai path is `/workspace/data/Cogumi-LLM`, not `/workspace/Cogumi-LLM`

---

## ðŸ”§ Files Updated

### âœ… **1. `scripts/extract_failures_from_benchmark.py`**
**Changed:**
- Vast.ai BENCHMARK_DIR: `/workspace/Cogumi-LLM/...` â†’ `/workspace/data/Cogumi-LLM/...`
- Vast.ai OUTPUT_DIR: `/workspace/Cogumi-LLM/...` â†’ `/workspace/data/Cogumi-LLM/...`

**Auto-detection logic:**
```python
if os.path.exists("/workspace"):
    # Vast.ai environment
    BENCHMARK_DIR = "/workspace/data/Cogumi-LLM/checkpoints/benchmark_results_full"
    OUTPUT_DIR = "/workspace/data/Cogumi-LLM/data/training_from_benchmark"
else:
    # Local Mac environment
    BENCHMARK_DIR = "/Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/benchmark_results_full"
    OUTPUT_DIR = "/Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/training_from_benchmark"
```

### âœ… **2. `scripts/run_phase1b_benchmark_training.sh`**
**Changed:**
- BASE_DIR: `/workspace/Cogumi-LLM` â†’ `/workspace/data/Cogumi-LLM`

**Auto-detection logic:**
```bash
if [ -d "/workspace" ]; then
    BASE_DIR="/workspace/data/Cogumi-LLM"
else
    BASE_DIR="/Users/vivekdurairaj/Projects/Cogumi-LLM"
fi
```

### âœ… **3. `train_qlora_optimized.py`**
**Added command-line argument support:**
- `--model_name` (default: meta-llama/Meta-Llama-3.1-8B-Instruct)
- `--dataset_path` (supports wildcards)
- `--output_dir`
- `--num_train_epochs`
- `--learning_rate`
- `--per_device_train_batch_size`
- `--gradient_accumulation_steps`
- `--save_strategy`
- `--logging_steps`

**Added flexible dataset format:**
- Handles Phase 1A format: `instruction` + `response`
- Handles Phase 1B format: `instruction` + `output`
- Auto-detects which field to use

### âœ… **4. Documentation Files**
**Updated:**
- `PHASE1B_READY_TO_EXECUTE.md` - All paths corrected
- `VASTAI_FILE_UPLOAD_GUIDE.md` - All paths corrected

---

## ðŸ“‚ Correct Path Structure

### **Vast.ai:**
```
/workspace/data/Cogumi-LLM/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ benchmark_results_full/
â”‚   â”‚   â”œâ”€â”€ math_intermediate.json
â”‚   â”‚   â””â”€â”€ code_intermediate.json
â”‚   â””â”€â”€ final/                    # Phase 1A model
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â””â”€â”€ adapter_model.safetensors
â”œâ”€â”€ data/
â”‚   â””â”€â”€ training_from_benchmark/  # Will be created
â”‚       â”œâ”€â”€ math_failures_from_benchmark.jsonl
â”‚       â””â”€â”€ code_failures_from_benchmark.jsonl
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_failures_from_benchmark.py
â”‚   â””â”€â”€ run_phase1b_benchmark_training.sh
â””â”€â”€ train_qlora_optimized.py
```

### **Local Mac:**
```
/Users/vivekdurairaj/Projects/Cogumi-LLM/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ phase1/
â”‚       â”œâ”€â”€ benchmark_results_full/
â”‚       â”‚   â”œâ”€â”€ math_intermediate.json
â”‚       â”‚   â””â”€â”€ code_intermediate.json
â”‚       â””â”€â”€ training_from_benchmark/
â”‚           â”œâ”€â”€ math_failures_from_benchmark.jsonl (âœ… 47 examples)
â”‚           â””â”€â”€ code_failures_from_benchmark.jsonl (âœ… 26 examples)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_failures_from_benchmark.py
â”‚   â””â”€â”€ run_phase1b_benchmark_training.sh
â””â”€â”€ train_qlora_optimized.py
```

---

## ðŸš€ Upload & Execute on Vast.ai

### **Step 1: Upload Files**

**Upload 3 files to Vast.ai:**

```bash
# On local Mac
cd /Users/vivekdurairaj/Projects/Cogumi-LLM

scp -P <vast_port> \
  train_qlora_optimized.py \
  root@<vast_ip>:/workspace/data/Cogumi-LLM/

scp -P <vast_port> \
  scripts/extract_failures_from_benchmark.py \
  scripts/run_phase1b_benchmark_training.sh \
  root@<vast_ip>:/workspace/data/Cogumi-LLM/scripts/
```

### **Step 2: Verify Upload**

```bash
# On Vast.ai SSH
ls -lh /workspace/data/Cogumi-LLM/train_qlora_optimized.py
ls -lh /workspace/data/Cogumi-LLM/scripts/extract_failures_from_benchmark.py
ls -lh /workspace/data/Cogumi-LLM/scripts/run_phase1b_benchmark_training.sh

# Make scripts executable
chmod +x /workspace/data/Cogumi-LLM/scripts/*.sh
chmod +x /workspace/data/Cogumi-LLM/scripts/*.py
```

### **Step 3: Execute One-Command Training**

```bash
# On Vast.ai SSH
cd /workspace/data/Cogumi-LLM
bash scripts/run_phase1b_benchmark_training.sh
```

**What happens automatically:**
1. âœ… Detects Vast.ai environment
2. âœ… Verifies benchmarks at `/workspace/data/Cogumi-LLM/checkpoints/benchmark_results_full/`
3. âœ… Extracts 73 examples to `/workspace/data/Cogumi-LLM/data/training_from_benchmark/`
4. âœ… Verifies Phase 1A model exists
5. âœ… Trains for 2 epochs (~15-20 min)
6. âœ… Saves to `/workspace/data/Cogumi-LLM/checkpoints/phase1b_from_benchmark/`
7. âœ… Validates model loads correctly

**Time:** 15-20 minutes  
**Cost:** $0.50-1  
**Expected:** 73 examples, ~36 steps

---

## ðŸ“Š Training Command (Automatic)

The script will run this command automatically:

```bash
python train_qlora_optimized.py \
    --model_name unsloth/meta-llama-3.1-8b-instruct-bnb-4bit \
    --dataset_path "data/training_from_benchmark/*.jsonl" \
    --output_dir checkpoints/phase1b_from_benchmark \
    --num_train_epochs 2 \
    --learning_rate 5e-6 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --save_strategy epoch \
    --logging_steps 5
```

---

## âœ… Verification Checklist

**Before execution, verify these exist on Vast.ai:**

```bash
# Benchmarks
ls /workspace/data/Cogumi-LLM/checkpoints/benchmark_results_full/math_intermediate.json
ls /workspace/data/Cogumi-LLM/checkpoints/benchmark_results_full/code_intermediate.json

# Phase 1A model
ls /workspace/data/Cogumi-LLM/checkpoints/final/adapter_model.safetensors

# Scripts
ls /workspace/data/Cogumi-LLM/scripts/extract_failures_from_benchmark.py
ls /workspace/data/Cogumi-LLM/scripts/run_phase1b_benchmark_training.sh

# Training script
ls /workspace/data/Cogumi-LLM/train_qlora_optimized.py
```

**All should exist!** If any file is missing, re-upload.

---

## ðŸŽ¯ Expected Output

### **Extraction Phase (<1 min):**
```
MATH Statistics:
  Total tested: 50
  Wins: 3 (6.0%)
  Losses: 12 (24.0%)
  Ties: 35 (70.0%)
  Extracted: 47 (94.0%)

CODE Statistics:
  Total tested: 50
  Wins: 24 (48.0%)
  Losses: 16 (32.0%)
  Ties: 10 (20.0%)
  Extracted: 26 (52.0%)

Total: 73 training examples
```

### **Training Phase (15-20 min):**
```
ðŸš€ OPTIMIZED TRAINING CONFIGURATION
Model: unsloth/meta-llama-3.1-8b-instruct-bnb-4bit
Dataset: data/training_from_benchmark/*.jsonl
Output dir: checkpoints/phase1b_from_benchmark
Epochs: 2
Learning rate: 5e-6
Per-device batch size: 4
Gradient accumulation: 4
Effective batch size: 16

âœ… Dataset loaded: 73 examples
âœ… Tokenization complete

Training...
Epoch 1/2: 100% [====================] 18/18 steps
Epoch 2/2: 100% [====================] 18/18 steps

âœ… Training complete!
```

---

## ðŸ” After Training: Validate Improvement

### **Re-run benchmarks:**

```bash
cd /workspace/data/Cogumi-LLM
python scripts/run_benchmarks.py \
  --model_path checkpoints/phase1b_from_benchmark \
  --output_dir checkpoints/benchmark_results_phase1b1
```

### **Compare results:**

| Metric | Phase 1A | Phase 1B.1 Target | Improvement |
|--------|----------|-------------------|-------------|
| **MATH wins** | 6% (3/50) | 20-30% (10-15/50) | **3x-5x** |
| **CODE wins** | 48% (24/50) | 55-65% (28-33/50) | **+15-35%** |
| **MATH ties** | 70% (35/50) | 40-50% (20-25/50) | **-20-30%** |
| **Consistency** | ~10% | 30-40% | **3x-4x** |

**Success criteria:**
- âœ… MATH wins increase by 3x-5x (6% â†’ 20-30%)
- âœ… CODE wins increase by 15-35% (48% â†’ 55-65%)
- âœ… MATH ties reduce (70% â†’ 40-50%)
- âœ… Overall consistency improves (10% â†’ 30-40%)

---

## ðŸŽ¯ Next Steps

**If Phase 1B.1 succeeds (expected):**

1. **Phase 1B.2: Expand training data**
   - Run model on GSM8K train (7,473 problems)
   - Identify ~2,000 additional failures
   - Train on 73 + 2,000 = 2,073 examples
   - Expected: MATH 55-65%, CODE 70-75%
   - Time: 11-16 hours
   - Cost: $22-30

**If Phase 1B.1 doesn't improve enough:**

1. Increase epochs (2 â†’ 3-4)
2. Adjust learning rate (5e-6 â†’ 3e-6 or 7e-6)
3. Extract more failures from other benchmark categories
4. Cost per iteration: +$0.50-1

---

## ðŸ› ï¸ Troubleshooting

**Problem:** Benchmark files not found
```bash
# Verify correct path
ls -lh /workspace/data/Cogumi-LLM/checkpoints/benchmark_results_full/
# Should see: math_intermediate.json, code_intermediate.json
```

**Problem:** Training script not found
```bash
# Verify upload
ls -lh /workspace/data/Cogumi-LLM/train_qlora_optimized.py
# If missing, re-upload from local
```

**Problem:** Permission denied on scripts
```bash
# Fix permissions
chmod +x /workspace/data/Cogumi-LLM/scripts/*.sh
chmod +x /workspace/data/Cogumi-LLM/scripts/*.py
```

**Problem:** Wrong number of examples extracted
```bash
# Check extraction output
python scripts/extract_failures_from_benchmark.py
# Should show: MATH: 47, CODE: 26, Total: 73
```

---

## ðŸ“š Summary

**All paths now corrected to `/workspace/data/Cogumi-LLM/`**

âœ… **Scripts updated** - Auto-detect correct paths  
âœ… **Documentation updated** - All references corrected  
âœ… **Training script updated** - Accepts command-line arguments  
âœ… **Ready to upload** - 3 files to Vast.ai  
âœ… **Ready to execute** - One command to run

**Upload â†’ Execute â†’ Wait 15-20 min â†’ Validate â†’ Proceed to Phase 1B.2**

---

**Everything is now ready for Vast.ai execution! ðŸš€**

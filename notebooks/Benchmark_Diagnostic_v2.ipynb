{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Environment Check\n",
        "\n",
        "**Run the cell below to verify you're using the venv (not global Python)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üêç PYTHON ENVIRONMENT CHECK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Python version\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print()\n",
        "\n",
        "# Python executable location\n",
        "executable = sys.executable\n",
        "print(f\"Python Executable: {executable}\")\n",
        "\n",
        "# Check if running from venv\n",
        "if 'venv' in executable:\n",
        "    print(\"‚úÖ Status: USING VENV (Local environment)\")\n",
        "    print(f\"   Located in: {os.path.dirname(os.path.dirname(executable))}\")\n",
        "elif 'homebrew' in executable or '/usr/bin' in executable or '/usr/local' in executable:\n",
        "    print(\"‚ùå Status: USING GLOBAL PYTHON (System-wide)\")\n",
        "    print(\"   ‚ö†Ô∏è  You may want to use a venv for isolation\")\n",
        "else:\n",
        "    print(f\"‚úÖ Status: Custom environment\")\n",
        "    print(f\"   Located in: {os.path.dirname(os.path.dirname(executable))}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Site packages location (where libraries are installed)\n",
        "import site\n",
        "site_packages = site.getsitepackages()[0]\n",
        "print(f\"Packages Location: {site_packages}\")\n",
        "\n",
        "if 'venv' in site_packages:\n",
        "    print(\"‚úÖ Packages: Installed in venv (isolated)\")\n",
        "elif 'site-packages' in site_packages:\n",
        "    print(\"‚úÖ Packages: Standard Python environment\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Packages: Unknown location\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Check key packages\n",
        "print(\"üì¶ Key Packages:\")\n",
        "try:\n",
        "    import matplotlib\n",
        "    print(f\"   ‚úÖ matplotlib {matplotlib.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ùå matplotlib NOT FOUND\")\n",
        "\n",
        "try:\n",
        "    import pandas\n",
        "    print(f\"   ‚úÖ pandas {pandas.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ùå pandas NOT FOUND\")\n",
        "\n",
        "try:\n",
        "    import numpy\n",
        "    print(f\"   ‚úÖ numpy {numpy.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ùå numpy NOT FOUND\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"   ‚úÖ torch {torch.__version__}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"      GPU: {torch.cuda.get_device_name(0)}\")\n",
        "except ImportError:\n",
        "    print(\"   ‚ö†Ô∏è  torch NOT FOUND (OK for local, needed for Vast.ai)\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1B Benchmark Diagnostic Analysis\n",
        "\n",
        "**Purpose:** Diagnose why MATH has 70% ties and CODE is at 58%\n",
        "\n",
        "**Sections:**\n",
        "1. Benchmark results visualization\n",
        "2. Model loading and testing\n",
        "3. Automated consistency tests\n",
        "4. JSON analysis with error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths and imports\n",
        "from pathlib import Path\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Detect environment\n",
        "if Path('/workspace').exists():\n",
        "    base_path = Path('/workspace/data/Cogumi-LLM')\n",
        "    print('Running on Vast.ai')\n",
        "else:\n",
        "    base_path = Path.cwd()\n",
        "    print('Running locally')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure paths\n",
        "checkpoint_dir = base_path / 'checkpoints' / 'final'\n",
        "dataset_file = base_path / 'phase1' / 'public_500k_filtered.jsonl'\n",
        "benchmark_dir = base_path / 'benchmark_results'\n",
        "\n",
        "print(f\"Checkpoint: {checkpoint_dir} (exists: {checkpoint_dir.exists()})\")\n",
        "print(f\"Dataset: {dataset_file} (exists: {dataset_file.exists()})\")\n",
        "print(f\"Benchmark: {benchmark_dir} (exists: {benchmark_dir.exists()})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Benchmark Results\n",
        "\n",
        "Manual benchmark scores:\n",
        "- MATH: 41% (70% ties - HIGH!)\n",
        "- CODE: 58% (28% ties)\n",
        "- REASONING: 86%\n",
        "- KNOWLEDGE: 90%\n",
        "- INSTRUCTION: 76%\n",
        "- CREATIVITY: 60%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize benchmark results\n",
        "results = {\n",
        "    'MATH': (41, 70), 'CODE': (58, 28), 'REASONING': (86, 10),\n",
        "    'KNOWLEDGE': (90, 8), 'INSTRUCTION': (76, 15), 'CREATIVITY': (60, 25)\n",
        "}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "categories = list(results.keys())\n",
        "scores = [results[c][0] for c in categories]\n",
        "ties = [results[c][1] for c in categories]\n",
        "\n",
        "ax1.barh(categories, scores, color=['red' if s<65 else 'orange' if s<80 else 'green' for s in scores])\n",
        "ax1.set_xlabel('Score (%)')\n",
        "ax1.set_title('Benchmark Scores')\n",
        "ax1.axvline(70, color='gray', linestyle='--', label='Target: 70%')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.barh(categories, ties, color=['red' if t>50 else 'orange' if t>25 else 'green' for t in ties])\n",
        "ax2.set_xlabel('Tie Rate (%)')\n",
        "ax2.set_title('Tie Rates')\n",
        "ax2.axvline(20, color='gray', linestyle='--', label='Expected: <20%')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Load Model (GPU Required)\n",
        "\n",
        "‚ö†Ô∏è Requires 5-6GB VRAM. Skip on local Mac, run on Vast.ai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import model libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure model paths\n",
        "base_model_name = \"unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\"\n",
        "\n",
        "if Path('/workspace/data/Cogumi-LLM/checkpoints/final').exists():\n",
        "    adapter_path = \"/workspace/data/Cogumi-LLM/checkpoints/final\"\n",
        "else:\n",
        "    adapter_path = str(checkpoint_dir)\n",
        "\n",
        "print(f\"Base model: {base_model_name}\")\n",
        "print(f\"Adapter: {adapter_path}\")\n",
        "print(f\"Adapter exists: {Path(adapter_path).exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"‚úÖ Tokenizer loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(f\"‚úÖ Base model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load adapter and merge\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model = model.merge_and_unload()\n",
        "model.eval()\n",
        "print(\"‚úÖ Model ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Consistency Tests\n",
        "\n",
        "Test 7 problems √ó 10 runs = 70 generations (~5-10 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run automated consistency tests\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "test_problems = {\n",
        "    'MATH': [\"What is 15% of 80?\", \"Calculate: (25 + 35) √ó 2\", \"Average of 10, 20, 30?\"],\n",
        "    'CODE': [\"Python function to sum two numbers\", \"Check if string is palindrome\"],\n",
        "    'CREATIVITY': [\"Write a haiku about coding\", \"Describe a futuristic city in one sentence\"]\n",
        "}\n",
        "\n",
        "def generate(prompt, temp=0.7):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=150, temperature=temp, do_sample=True)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"üîÑ Running consistency tests...\")\n",
        "print(f\"Total: {sum(len(p) for p in test_problems.values())} problems √ó 10 runs = {sum(len(p) for p in test_problems.values()) * 10} generations\")\n",
        "print()\n",
        "\n",
        "results = {}\n",
        "total_problems = sum(len(problems) for problems in test_problems.values())\n",
        "problem_count = 0\n",
        "\n",
        "for category, problems in test_problems.items():\n",
        "    results[category] = []\n",
        "    for prompt in problems:\n",
        "        problem_count += 1\n",
        "        print(f\"[{problem_count}/{total_problems}] {category} - {prompt[:50]}...\")\n",
        "        \n",
        "        # Generate 10 responses with progress\n",
        "        responses = []\n",
        "        start_time = time.time()\n",
        "        for i in range(10):\n",
        "            print(f\"  Run {i+1}/10...\", end='\\r')\n",
        "            responses.append(generate(prompt))\n",
        "        \n",
        "        # Calculate consistency\n",
        "        consistency = Counter(responses).most_common(1)[0][1] * 10\n",
        "        elapsed = time.time() - start_time\n",
        "        \n",
        "        results[category].append({'prompt': prompt, 'consistency': consistency})\n",
        "        print(f\"  ‚úÖ {consistency}% consistent ({elapsed:.1f}s for 10 runs)    \")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä AVERAGE CONSISTENCY BY CATEGORY:\")\n",
        "print(\"=\" * 70)\n",
        "for cat, res in results.items():\n",
        "    avg = sum(r['consistency'] for r in res) / len(res)\n",
        "    print(f\"  {cat:12} : {avg:.1f}%\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Analyze Benchmark JSON\n",
        "\n",
        "Load benchmark files with comprehensive error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze benchmark JSON with error handling\n",
        "def analyze_json(path):\n",
        "    try:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå JSON error: Line {e.lineno}, Col {e.colno}\")\n",
        "        print(\"üí° Check for unescaped backslashes or quotes\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {type(e).__name__}: {e}\")\n",
        "        return None\n",
        "    \n",
        "    # Detect format\n",
        "    if 'by_category' in data:\n",
        "        by_cat = data['by_category']\n",
        "    elif 'category' in data and 'results' in data:\n",
        "        by_cat = {data['category']: {'test_results': data['results']}}\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Unknown format\")\n",
        "        return None\n",
        "    \n",
        "    # Analyze each category\n",
        "    for cat, res in by_cat.items():\n",
        "        tests = res.get('test_results', [])\n",
        "        if not tests: continue\n",
        "        \n",
        "        wins = losses = ties = 0\n",
        "        for t in tests:\n",
        "            if not isinstance(t, dict): continue\n",
        "            outcome = t.get('outcome', '').lower()\n",
        "            if outcome == 'win': wins += 1\n",
        "            elif outcome == 'loss': losses += 1\n",
        "            elif outcome == 'tie': ties += 1\n",
        "        \n",
        "        total = wins + losses + ties\n",
        "        if total == 0: continue\n",
        "        \n",
        "        print(f\"\\n{cat.upper()}:\")\n",
        "        print(f\"  Wins: {wins} ({wins/total*100:.1f}%)\")\n",
        "        print(f\"  Losses: {losses} ({losses/total*100:.1f}%)\")\n",
        "        print(f\"  Ties: {ties} ({ties/total*100:.1f}%)\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Try to load\n",
        "if benchmark_dir.exists():\n",
        "    reports = sorted(benchmark_dir.glob(\"benchmark_report_*.json\"))\n",
        "    if reports:\n",
        "        print(f\"Analyzing: {reports[-1].name}\")\n",
        "        analyze_json(reports[-1])\n",
        "    else:\n",
        "        print(\"No benchmark reports found\")\n",
        "else:\n",
        "    print(\"Benchmark directory not found\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

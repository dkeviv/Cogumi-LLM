{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4036d2cf",
   "metadata": {},
   "source": [
    "# Phase 1B: Benchmark Trained Model vs GPT-4\n",
    "\n",
    "**âœ… Run this on H100 after Phase 1A training completes**\n",
    "\n",
    "**What this does:**\n",
    "1. Loads your trained model from `/data/Cogumi-LLM/checkpoints/final/`\n",
    "2. Tests on 50 diverse examples per category (6 categories = 300 total)\n",
    "3. Compares against GPT-4 baseline\n",
    "4. Generates performance report\n",
    "\n",
    "**Requirements:**\n",
    "- âœ… Completed Phase 1A training\n",
    "- ðŸ”‘ OpenAI API key\n",
    "- ðŸ’° ~$5-10 in OpenAI credits\n",
    "- â±ï¸ ~30-60 minutes runtime\n",
    "\n",
    "**Expected Results:**\n",
    "- **Target:** 75-82% of GPT-4 performance\n",
    "- **Good:** â‰¥80%\n",
    "- **Excellent:** â‰¥85%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cfecd",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment\n",
    "\n",
    "Check that your model exists and environment is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf66b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check model exists - try multiple possible locations\n",
    "possible_paths = [\n",
    "    \"/workspace/data/Cogumi-LLM/checkpoints/final\",           # Your actual location!\n",
    "    \"/workspace/data/Cogumi-LLM/checkpoints/checkpoint-240240\",\n",
    "    \"/workspace/Cogumi-LLM/checkpoints/final\",                # Alternative paths\n",
    "    \"/data/Cogumi-LLM/checkpoints/final\",\n",
    "    \"/workspace/checkpoints/final\",\n",
    "    \"/data/Cogumi-LLM/checkpoints/checkpoint-240240\",\n",
    "]\n",
    "\n",
    "model_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        model_path = path\n",
    "        print(f\"âœ… Model found at: {model_path}\")\n",
    "        break\n",
    "\n",
    "if model_path:\n",
    "    print(f\"\\nðŸ“ Model contents:\")\n",
    "    !ls -lh {model_path}\n",
    "    \n",
    "    # Show disk info\n",
    "    print(f\"\\nðŸ“Š Disk usage:\")\n",
    "    !df -h {model_path}\n",
    "    \n",
    "    # Check model size\n",
    "    print(f\"\\nðŸ“¦ Total model size:\")\n",
    "    !du -sh {model_path}\n",
    "else:\n",
    "    print(\"âŒ ERROR: Model not found in any expected location!\")\n",
    "    print(\"\\nðŸ” Checked these locations:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"   âŒ {path}\")\n",
    "    print(\"\\nðŸ’¡ Let's find where your model actually is:\")\n",
    "    print(\"   Run this in a new cell: !find /workspace -name 'adapter_model.safetensors' 2>/dev/null\")\n",
    "    raise FileNotFoundError(\"Model not found at any expected path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a470753",
   "metadata": {},
   "source": [
    "## Step 2: Verify Dependencies\n",
    "\n",
    "Check that PyTorch, Unsloth, and other packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all required packages are installed\n",
    "import sys\n",
    "\n",
    "print(\"ðŸ” Checking installed packages...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ PyTorch not installed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    import unsloth\n",
    "    print(f\"âœ… Unsloth: Installed\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Unsloth not installed - installing now...\")\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"âœ… Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Transformers not installed\")\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    print(f\"âœ… OpenAI SDK: {openai.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  OpenAI SDK not installed - installing...\")\n",
    "    !pip install openai\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"âœ… Datasets: {datasets.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Datasets not installed - installing...\")\n",
    "    !pip install datasets\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    print(f\"âœ… Pandas: {pandas.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Pandas not installed - installing...\")\n",
    "    !pip install pandas\n",
    "\n",
    "try:\n",
    "    import rich\n",
    "    print(f\"âœ… Rich: Installed\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Rich not installed - installing...\")\n",
    "    !pip install rich\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… All core packages verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582a3b6",
   "metadata": {},
   "source": [
    "## Step 3: Configure OpenAI API Key\n",
    "\n",
    "**Get your API key from:** https://platform.openai.com/api-keys\n",
    "\n",
    "**Estimated cost:** $5-10 for 300 GPT-4 comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Enter your OpenAI API key (paste when prompted)\n",
    "OPENAI_API_KEY = getpass(\"Enter OpenAI API key: \")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print(\"âœ… API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c38ba",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Benchmark Suite\n",
    "\n",
    "Load your model and prepare for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f026a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project directory to path - check multiple locations\n",
    "project_paths = [\n",
    "    \"/workspace/data/Cogumi-LLM\",       # Your actual location!\n",
    "    \"/workspace/Cogumi-LLM\",\n",
    "    \"/data/Cogumi-LLM\",\n",
    "    \"/workspace\",\n",
    "]\n",
    "\n",
    "project_dir = None\n",
    "for proj_path in project_paths:\n",
    "    scripts_path = os.path.join(proj_path, \"scripts\", \"automated_gpt4_benchmark.py\")\n",
    "    if os.path.exists(scripts_path):\n",
    "        sys.path.append(proj_path)\n",
    "        project_dir = proj_path\n",
    "        print(f\"âœ… Project found at: {proj_path}\")\n",
    "        print(f\"âœ… Benchmark script exists: {scripts_path}\")\n",
    "        break\n",
    "\n",
    "if not project_dir:\n",
    "    print(\"âš ï¸ Warning: Could not find automated_gpt4_benchmark.py\")\n",
    "    print(\"Attempting to add common paths...\")\n",
    "    sys.path.append(\"/workspace/data/Cogumi-LLM\")\n",
    "\n",
    "from scripts.automated_gpt4_benchmark import BenchmarkSuite\n",
    "\n",
    "# Create output directory (next to model)\n",
    "output_dir = str(Path(model_path).parent.parent / \"benchmark_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nðŸ”„ Initializing benchmark suite...\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   Output: {output_dir}\")\n",
    "print(\"\")\n",
    "\n",
    "# Initialize (this loads your model on GPU)\n",
    "suite = BenchmarkSuite(\n",
    "    model_path=model_path,\n",
    "    openai_key=OPENAI_API_KEY,\n",
    "    output_dir=output_dir,\n",
    "    device=\"auto\"  # Uses GPU automatically\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Benchmark suite ready!\")\n",
    "print(f\"\\nðŸ“Š Will test on {len(suite.categories)} categories:\")\n",
    "for cat, desc in suite.categories.items():\n",
    "    print(f\"   - {cat}: {desc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d043e",
   "metadata": {},
   "source": [
    "## Step 5: Run Benchmark\n",
    "\n",
    "**This will take ~30-60 minutes**\n",
    "\n",
    "Tests your model against GPT-4 on:\n",
    "- 50 samples per category\n",
    "- 6 categories = 300 total comparisons\n",
    "- Side-by-side GPT-4 judging\n",
    "\n",
    "**Progress will be shown below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Safety check - ensure suite is initialized\n",
    "if 'suite' not in globals():\n",
    "    print(\"âŒ ERROR: BenchmarkSuite not initialized!\")\n",
    "    print(\"\\nðŸ”§ Please run Cell 4 first to initialize the benchmark suite.\")\n",
    "    print(\"\\nðŸ“ Make sure you've:\")\n",
    "    print(\"   1. Uploaded the fixed automated_gpt4_benchmark.py script\")\n",
    "    print(\"   2. Successfully run Cell 4 (Initialize Benchmark Suite)\")\n",
    "    raise NameError(\"suite not defined - run Cell 4 first\")\n",
    "\n",
    "print(\"ðŸš€ Starting benchmark...\")\n",
    "print(f\"   Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Categories: {list(suite.categories.keys())}\")\n",
    "print(f\"   Samples per category: 50\")\n",
    "print(f\"   Total comparisons: 300\")\n",
    "print(\"\")\n",
    "print(\"â³ This will take ~30-60 minutes. Progress shown below:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run full benchmark\n",
    "results = suite.run_full_benchmark(\n",
    "    categories=list(suite.categories.keys()),\n",
    "    samples_per_category=50\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Benchmark complete!\")\n",
    "print(f\"   End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8d107",
   "metadata": {},
   "source": [
    "## Step 6: View Results\n",
    "\n",
    "Analyze performance vs GPT-4 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load latest benchmark report\n",
    "reports = sorted(Path(output_dir).glob(\"benchmark_report_*.json\"))\n",
    "\n",
    "if not reports:\n",
    "    print(\"âŒ No benchmark reports found\")\n",
    "else:\n",
    "    # Load most recent report\n",
    "    with open(reports[-1]) as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    # Display overall performance\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š PHASE 1B BENCHMARK RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸŽ¯ Overall Performance: {report['overall']['score_vs_gpt4']:.1f}% of GPT-4\")\n",
    "    print(f\"   Rating: {report['overall']['performance_rating']}\")\n",
    "    print(f\"   Total comparisons: {report['overall']['total_comparisons']}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Create detailed table\n",
    "    df = pd.DataFrame(report['by_category']).T\n",
    "    df = df.sort_values('score', ascending=False)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Results by Category:\")\n",
    "    print(\"=\"*70)\n",
    "    for cat in df.index:\n",
    "        score = df.loc[cat, 'score']\n",
    "        wins = df.loc[cat, 'wins']\n",
    "        losses = df.loc[cat, 'losses']\n",
    "        ties = df.loc[cat, 'ties']\n",
    "        \n",
    "        # Status indicator\n",
    "        if score >= 85:\n",
    "            status = \"âœ… Excellent\"\n",
    "        elif score >= 75:\n",
    "            status = \"ðŸŸ¢ Good\"\n",
    "        elif score >= 65:\n",
    "            status = \"ðŸŸ¡ Needs work\"\n",
    "        else:\n",
    "            status = \"ðŸ”´ Weak\"\n",
    "        \n",
    "        print(f\"{cat:12s}: {score:5.1f}% | W:{wins:2d} L:{losses:2d} T:{ties:2d} | {status}\")\n",
    "    \n",
    "    # Identify weak areas\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    weak_categories = df[df['score'] < 75].index.tolist()\n",
    "    \n",
    "    if weak_categories:\n",
    "        print(\"âš ï¸  WEAK AREAS (need Phase 1C targeted distillation):\")\n",
    "        for cat in weak_categories:\n",
    "            print(f\"   - {cat}: {df.loc[cat, 'score']:.1f}% (target: 85%+)\")\n",
    "    else:\n",
    "        print(\"âœ… NO WEAK AREAS - All categories â‰¥75%!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"\\nðŸ“ Full report saved to: {reports[-1]}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f1e5f",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Failure Patterns\n",
    "\n",
    "Identify specific examples where GPT-4 outperformed your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract failure examples\n",
    "failure_examples = []\n",
    "\n",
    "for category, data in suite.results.items():\n",
    "    if 'details' in data:\n",
    "        for result in data['details']:\n",
    "            if result.get('judgment', {}).get('winner') == 'B':  # GPT-4 won\n",
    "                failure_examples.append({\n",
    "                    'category': category,\n",
    "                    'prompt': result['prompt'],\n",
    "                    'local_response': result['local_response'],\n",
    "                    'gpt4_response': result['gpt4_response'],\n",
    "                    'reasoning': result['judgment'].get('reasoning', '')\n",
    "                })\n",
    "\n",
    "print(f\"\\nðŸ“Š Failure Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total failures: {len(failure_examples)} / {report['overall']['total_comparisons']}\")\n",
    "print(f\"Failure rate: {len(failure_examples) / report['overall']['total_comparisons'] * 100:.1f}%\")\n",
    "print(\"\")\n",
    "\n",
    "# Breakdown by category\n",
    "print(\"Failures by category:\")\n",
    "category_counts = Counter(ex['category'] for ex in failure_examples)\n",
    "for cat, count in category_counts.most_common():\n",
    "    pct = count / 50 * 100  # 50 samples per category\n",
    "    print(f\"   {cat:12s}: {count:2d} failures ({pct:.1f}%)\")\n",
    "\n",
    "# Save failures for Phase 1C\n",
    "failure_file = Path(output_dir) / \"failure_examples.json\"\n",
    "with open(failure_file, 'w') as f:\n",
    "    json.dump(failure_examples, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Failure examples saved to: {failure_file}\")\n",
    "print(\"   Use these for Phase 1C targeted distillation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e845d",
   "metadata": {},
   "source": [
    "## Step 8: Sample Failure Examples\n",
    "\n",
    "Review a few examples where your model lost to GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 3 random failure examples\n",
    "import random\n",
    "\n",
    "if failure_examples:\n",
    "    print(\"\\nðŸ“ Sample Failure Examples (3 random):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, example in enumerate(random.sample(failure_examples, min(3, len(failure_examples))), 1):\n",
    "        print(f\"\\n[Example {i}] Category: {example['category']}\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"Prompt: {example['prompt'][:200]}...\")\n",
    "        print(f\"\\nYour model: {example['local_response'][:200]}...\")\n",
    "        print(f\"\\nGPT-4: {example['gpt4_response'][:200]}...\")\n",
    "        print(f\"\\nWhy GPT-4 won: {example['reasoning'][:150]}...\")\n",
    "        print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ No failures - your model matched or beat GPT-4 on all examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2b13a",
   "metadata": {},
   "source": [
    "## Step 9: Download Results\n",
    "\n",
    "**Before terminating the instance, download these files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d408e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ Files to Download (from Jupyter file browser):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n1. Benchmark report: {reports[-1] if reports else 'N/A'}\")\n",
    "print(f\"2. Failure examples: {failure_file}\")\n",
    "print(f\"3. All results: {output_dir}/\")\n",
    "print(\"\\nðŸ’¡ Right-click these folders in Jupyter and select 'Download as Archive'\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a766bfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "### âœ… If Overall Score â‰¥ 80%:\n",
    "**Proceed to Phase 2: Compression**\n",
    "- Your model is ready for Neural Magic pruning\n",
    "- Expected: 10GB â†’ 520MB with minimal quality loss\n",
    "\n",
    "### ðŸ“Š If Overall Score 75-80%:\n",
    "**Optional: Phase 1C Targeted Distillation**\n",
    "- Generate 10-20K examples for weak categories\n",
    "- Quick fine-tune to push above 80%\n",
    "- Cost: ~$50-100, Time: ~4-6 hours\n",
    "\n",
    "### âš ï¸ If Overall Score < 75%:\n",
    "**Required: Phase 1C Full Distillation**\n",
    "- Generate 40K GPT-5 examples for weak areas\n",
    "- Comprehensive fine-tune\n",
    "- Target: 88-100% GPT-4 baseline\n",
    "- Cost: ~$280, Time: ~5 days\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽŠ Congratulations!\n",
    "\n",
    "You've completed **Phase 1B - Benchmarking**!\n",
    "\n",
    "**What you achieved:**\n",
    "- âœ… Trained an 8B model on 640K examples\n",
    "- âœ… Benchmarked against GPT-4 baseline\n",
    "- âœ… Identified performance gaps\n",
    "- âœ… Ready for next phase!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

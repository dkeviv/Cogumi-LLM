{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46907863",
   "metadata": {},
   "source": [
    "# Phase 1A: LLAMA-3.1-8B QLoRA Training\n",
    "\n",
    "**Project:** Cogumi-LLM  \n",
    "**Phase:** 1A - Base Model Distillation  \n",
    "**Model:** Meta-Llama-3.1-8B-Instruct (text-only)  \n",
    "**Duration:** 36-48 hours  \n",
    "**GPU Required:** A100 40GB  \n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Select Runtime**: Runtime ‚Üí Change runtime type ‚Üí A100 GPU\n",
    "2. **Connect to GPU**: Click Connect in top-right\n",
    "3. **Run all cells sequentially**\n",
    "4. **Monitor training**: Check TensorBoard and logs\n",
    "\n",
    "‚ö†Ô∏è **Important**: Colab Pro+ allows up to 24 hours per session. Training takes 36-48 hours, so you'll need to resume from checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d2e19",
   "metadata": {},
   "source": [
    "## üìã Best Practices for Long-Running Tasks\n",
    "\n",
    "**Background Execution**: For verification and monitoring tasks, use `nohup` to run in background:\n",
    "```bash\n",
    "# Run dataset verification in background\n",
    "nohup python src/phase0_dataset/verify_dataset.py --sample-size 10000 > verify.log 2>&1 &\n",
    "\n",
    "# Check progress anytime\n",
    "tail -f verify.log\n",
    "\n",
    "# Check if still running\n",
    "ps aux | grep verify_dataset\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- ‚úÖ Continue working on other setup tasks\n",
    "- ‚úÖ Process survives if you switch cells\n",
    "- ‚úÖ Can monitor multiple tasks simultaneously\n",
    "- ‚úÖ Logs saved for later review\n",
    "\n",
    "**When to Use Background**:\n",
    "- Dataset verification (5-10 minutes)\n",
    "- Model downloads (10-15 minutes)\n",
    "- Benchmark evaluations (15-30 minutes)\n",
    "- **NOT for training** (use TensorBoard for monitoring)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d50bc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44345b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have A100\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Verify it's A100\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "if 'A100' not in gpu_name:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: You need A100 GPU for this training!\")\n",
    "    print(\"Go to Runtime ‚Üí Change runtime type ‚Üí Select A100\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ A100 GPU detected! Ready to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42a279",
   "metadata": {},
   "source": [
    "## üîç BEST PRACTICE: Verify Model Requirements\n",
    "\n",
    "**Before installing dependencies, ALWAYS check the model's HuggingFace page for official requirements!**\n",
    "\n",
    "### For this notebook (LLAMA-3.1-8B):\n",
    "üëâ **Visit**: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "**What to check:**\n",
    "1. **Model card \"Files and versions\" tab** - Check which transformers version was used to upload the model\n",
    "2. **Model card README** - Look for \"Requirements\" or \"Dependencies\" section\n",
    "3. **Usage examples** - Note the transformers/torch versions in code examples\n",
    "4. **Known issues** - Check discussions/issues tab for compatibility problems\n",
    "\n",
    "### Why this matters:\n",
    "- ‚ùå **Wrong transformers version** ‚Üí Model loading errors (e.g., rope_scaling issues)\n",
    "- ‚ùå **Incompatible dependencies** ‚Üí Training crashes or poor performance\n",
    "- ‚úÖ **Correct versions** ‚Üí Smooth training experience\n",
    "\n",
    "### For LLAMA-3.1 specifically:\n",
    "- **Minimum transformers**: 4.43.0 (for rope_scaling support)\n",
    "- **Recommended transformers**: 4.46.3 (latest stable)\n",
    "- **PyTorch**: 2.4.0 (tested and stable)\n",
    "- **Key features**: Extended context length, improved rope scaling\n",
    "\n",
    "**üí° Pro tip**: If you're adapting this notebook for a different model, update Section 2 dependencies based on the model card requirements!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: Display model card info\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_url = f\"https://huggingface.co/{model_name}\"\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### üìã Model Information Check\n",
    "\n",
    "**Model**: `{model_name}`  \n",
    "**HuggingFace Page**: [{model_url}]({model_url})\n",
    "\n",
    "**‚úÖ Action Required**:\n",
    "1. Click the link above to open the model card\n",
    "2. Check the \"Files and versions\" tab for transformers version\n",
    "3. Read the README for any specific requirements\n",
    "4. Review discussions/issues for known compatibility problems\n",
    "\n",
    "**Current notebook dependencies** (Section 2):\n",
    "- transformers: 4.46.3\n",
    "- torch: 2.4.0+cu118\n",
    "- accelerate: 1.2.1\n",
    "- peft: 0.13.2\n",
    "- bitsandbytes: 0.45.0\n",
    "\n",
    "üí° **If model requirements differ, update Section 2 installation cell before proceeding!**\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ After reviewing the model card, proceed to Section 2\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002131ff",
   "metadata": {},
   "source": [
    "## ‚ùì FAQ: English-Only Training Strategy\n",
    "\n",
    "### Q: \"LLAMA-3.1-8B is multilingual. Won't training on English-only data break the model?\"\n",
    "\n",
    "**A: No! This is exactly the strategy. Here's why:**\n",
    "\n",
    "#### üéØ The Goal: 480MB English-specialized model (from 8GB base)\n",
    "\n",
    "**LLAMA-3.1-8B Capabilities:**\n",
    "- 8B parameters (~16GB fp16)\n",
    "- Trained on 100+ languages\n",
    "- Multilingual tokenizer (128K vocab)\n",
    "\n",
    "**Our Strategy: Controlled Forgetting**\n",
    "1. **Phase 1 (This notebook)**: Train ONLY on 640K English examples (99.46% pure)\n",
    "   - Model **forgets** non-English capabilities through English-focused fine-tuning\n",
    "   - Weights specialize for English patterns\n",
    "   - No explicit vocabulary trimming (breaks architecture)\n",
    "   - Result: English-optimized 8B model (~11GB after LoRA merge)\n",
    "\n",
    "2. **Phase 2 (Compression)**: Aggressive pruning possible BECAUSE model is English-only\n",
    "   - **65% neuron pruning** (vs 60% max for multilingual)\n",
    "   - Can safely remove: Chinese/Japanese/Korean/Arabic/Hebrew/Cyrillic neurons\n",
    "   - Can prune: Multi-script attention heads, language-specific embeddings\n",
    "   - Result: 480MB model (vs 720MB if we kept multilingual)\n",
    "\n",
    "3. **Why NOT vocabulary trimming?**\n",
    "   - ‚ùå Breaks LLAMA's embedding layer architecture\n",
    "   - ‚ùå Requires retraining from scratch (expensive)\n",
    "   - ‚úÖ Natural forgetting through English-only fine-tuning works better\n",
    "   - ‚úÖ Tokenizer stays intact, model learns to ignore non-English tokens\n",
    "\n",
    "#### üìä The Numbers\n",
    "\n",
    "| Approach | Base Size | After Training | After Compression | Quality |\n",
    "|----------|-----------|----------------|-------------------|---------|\n",
    "| **Multilingual** | 16GB | 14GB | 720MB | 85-87% GPT-4 |\n",
    "| **English-only (ours)** | 16GB | 11GB | 480MB | **87-89% GPT-4** |\n",
    "| **Savings** | - | 21% smaller | **33% smaller** | **+2% better** |\n",
    "\n",
    "#### üî¨ How It Works\n",
    "\n",
    "```\n",
    "LLAMA-3.1-8B (base)\n",
    "‚îî‚îÄ Has multilingual neurons (Chinese, Arabic, etc.)\n",
    "   ‚Üì\n",
    "Fine-tune ONLY on 640K English examples (Phase 1)\n",
    "‚îî‚îÄ Multilingual neurons receive zero gradient updates\n",
    "‚îî‚îÄ English neurons get stronger, others atrophy\n",
    "   ‚Üì\n",
    "Structured Pruning (Phase 2)\n",
    "‚îî‚îÄ Remove atrophied neurons (non-English)\n",
    "‚îî‚îÄ 65% pruning rate possible vs 60% multilingual\n",
    "   ‚Üì\n",
    "Result: 480MB English-only model\n",
    "‚îî‚îÄ Higher quality (87-89% vs 85-87%)\n",
    "‚îî‚îÄ 33% smaller than multilingual equivalent\n",
    "```\n",
    "\n",
    "#### ‚úÖ Your Dataset Purity\n",
    "\n",
    "Your training data (`public_500k_filtered.jsonl`):\n",
    "- **640,637 examples** from 7 curated datasets\n",
    "- **99.46% English verified** (54 non-English out of 10K sample)\n",
    "- English detection: 15% common word threshold (30% weight in quality score)\n",
    "- **Minimal non-English contamination** (0.54%) - negligible impact\n",
    "\n",
    "**This purity is CRITICAL for:**\n",
    "- Maximum neuron atrophy in non-English pathways\n",
    "- Highest possible compression rate (65% vs 60%)\n",
    "- Best quality at target size (480MB)\n",
    "\n",
    "#### üí° Key Insight\n",
    "\n",
    "> **We're not trying to preserve multilingual capabilities.**  \n",
    "> **We're deliberately trading them for smaller size + better English quality.**\n",
    "\n",
    "This is a **feature, not a bug** of the compression strategy!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Verify your dataset is English-only (run after uploading dataset)\n",
    "# This cell demonstrates the English-only nature of the training data\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def check_english_purity(filepath, sample_size=100):\n",
    "    \"\"\"Quick English purity check on dataset samples.\"\"\"\n",
    "    \n",
    "    common_english_words = {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she'\n",
    "    }\n",
    "    \n",
    "    non_english_chars = set()\n",
    "    english_word_counts = []\n",
    "    \n",
    "    print(f\"üìä Analyzing {sample_size} samples from dataset...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= sample_size:\n",
    "                break\n",
    "                \n",
    "            data = json.loads(line)\n",
    "            text = (data.get('instruction', '') + ' ' + data.get('response', '')).lower()\n",
    "            \n",
    "            # Count English words\n",
    "            words = re.findall(r'\\b\\w+\\b', text)\n",
    "            english_count = sum(1 for w in words if w in common_english_words)\n",
    "            english_ratio = english_count / len(words) if words else 0\n",
    "            english_word_counts.append(english_ratio)\n",
    "            \n",
    "            # Check for non-Latin characters (Chinese, Arabic, etc.)\n",
    "            for char in text:\n",
    "                if ord(char) > 127 and not char.isspace():  # Non-ASCII, non-space\n",
    "                    non_english_chars.add(char)\n",
    "    \n",
    "    avg_english = sum(english_word_counts) / len(english_word_counts) * 100\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results:\")\n",
    "    print(f\"  ‚Ä¢ Average English common word ratio: {avg_english:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Non-Latin characters found: {len(non_english_chars)}\")\n",
    "    \n",
    "    if non_english_chars:\n",
    "        print(f\"  ‚Ä¢ Examples: {list(non_english_chars)[:20]}\")\n",
    "        print(f\"    (Note: Might be math symbols, code, or rare technical terms)\")\n",
    "    \n",
    "    print(\"\\nüìã Assessment:\")\n",
    "    if avg_english > 12 and len(non_english_chars) < 50:\n",
    "        print(\"  ‚úÖ Dataset appears to be English-focused (suitable for compression)\")\n",
    "        print(\"  ‚úÖ Multilingual neurons will atrophy during training\")\n",
    "        print(\"  ‚úÖ Phase 2 compression can achieve 65% pruning rate\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Dataset may contain significant non-English content\")\n",
    "        print(\"  ‚ö†Ô∏è  May limit compression effectiveness\")\n",
    "        \n",
    "    print(\"\\nüí° For full verification, see: docs/ENGLISH_ONLY_COMPRESSION_STRATEGY.md\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run check (comment out if you want to skip)\n",
    "# Uncomment the line below after uploading dataset:\n",
    "# check_english_purity('data/phase1/public_500k_filtered.jsonl', sample_size=100)\n",
    "\n",
    "print(\"üí° Uncomment the line above to verify your dataset's English purity\")\n",
    "print(\"   Expected: >12% common English words, <50 non-Latin chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5c6ed",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "‚ö†Ô∏è **Important**: Colab comes with pre-installed packages (PyTorch 2.8.0) that conflict with our requirements (PyTorch 2.4.0).\n",
    "\n",
    "**üîÑ RECOMMENDED: Restart runtime FIRST, then run the cell below**\n",
    "- Go to: **Runtime ‚Üí Restart runtime**\n",
    "- Then run the dependency installation cell below\n",
    "\n",
    "This gives you a clean slate and avoids version conflicts!\n",
    "\n",
    "**‚è±Ô∏è Estimated time: 5-7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üì¶ DEPENDENCY INSTALLATION (Section 2)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Best Practice: Restart runtime BEFORE running this cell\")\n",
    "print(\"   (Runtime ‚Üí Restart runtime ‚Üí Run this cell)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì¶ Installing PyTorch 2.4.0 and dependencies...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install PyTorch 2.4.0 with CUDA 11.8 support\n",
    "!pip install -q torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"\\n‚úÖ PyTorch 2.4.0 installed!\")\n",
    "\n",
    "# Install core ML packages - UPDATED FOR LLAMA-3.1 COMPATIBILITY\n",
    "# transformers >= 4.43.0 required for LLAMA-3.1 rope_scaling support\n",
    "!pip install -q transformers==4.46.3\n",
    "!pip install -q accelerate==1.2.1\n",
    "!pip install -q peft==0.13.2\n",
    "!pip install -q bitsandbytes==0.45.0\n",
    "\n",
    "print(\"\\n‚úÖ Core ML packages installed!\")\n",
    "\n",
    "# Install data handling packages\n",
    "!pip install -q datasets==3.2.0\n",
    "!pip install -q tokenizers==0.21.0\n",
    "\n",
    "# Install monitoring packages\n",
    "!pip install -q wandb\n",
    "!pip install -q tensorboard==2.18.0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì¶ Installing additional training packages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install TRL for training utilities\n",
    "!pip install -q trl==0.12.2\n",
    "\n",
    "# Install additional utilities\n",
    "!pip install -q huggingface-hub scipy langdetect\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All dependencies installed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã Installed versions:\")\n",
    "print(f\"  ‚Ä¢ torch: 2.4.0+cu118\")\n",
    "print(f\"  ‚Ä¢ transformers: 4.46.3 (LLAMA-3.1 compatible)\")\n",
    "print(f\"  ‚Ä¢ accelerate: 1.2.1\")\n",
    "print(f\"  ‚Ä¢ peft: 0.13.2\")\n",
    "print(f\"  ‚Ä¢ bitsandbytes: 0.45.0\")\n",
    "print(f\"  ‚Ä¢ datasets: 3.2.0\")\n",
    "print(f\"  ‚Ä¢ trl: 0.12.2\")\n",
    "print(\"\\nüéâ Installation complete!\")\n",
    "print(\"‚û°Ô∏è Proceed to Section 3 (Clone Repository & Setup)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f15e8",
   "metadata": {},
   "source": [
    "## 3. Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (or pull latest changes if already exists)\n",
    "import os\n",
    "\n",
    "if os.path.exists('Cogumi-LLM'):\n",
    "    print(\"üìÇ Repository already exists, pulling latest changes...\")\n",
    "    %cd Cogumi-LLM\n",
    "    !git pull origin main\n",
    "    print(\"‚úÖ Repository updated to latest version\")\n",
    "else:\n",
    "    print(\"üì• Cloning repository...\")\n",
    "    !git clone https://github.com/dkeviv/Cogumi-LLM.git\n",
    "    %cd Cogumi-LLM\n",
    "    print(\"‚úÖ Repository cloned successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c532584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset exists\n",
    "!ls -lh data/phase1/public_500k_filtered.jsonl\n",
    "!wc -l data/phase1/public_500k_filtered.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d56f4",
   "metadata": {},
   "source": [
    "## 3b. Upload Dataset\n",
    "\n",
    "‚ö†Ô∏è **Important**: The dataset is not in the Git repository (too large). You need to upload it.\n",
    "\n",
    "**Choose the best option for you:**\n",
    "\n",
    "### Option 1: Download from Google Drive (FASTEST! ‚ö° ~2-3 minutes)\n",
    "- **File**: `public_500k_filtered.jsonl.gz` (264 MB) in your Google Drive\n",
    "- **Time**: ~2-3 minutes (no upload needed!)\n",
    "- **Best for**: If you already have the file in Google Drive\n",
    "\n",
    "### Option 2: Upload Compressed File (~9-10 minutes)\n",
    "- **File**: `public_500k_filtered.jsonl.gz` (264 MB) from local machine\n",
    "- **Time**: ~9-10 minutes\n",
    "- **Best for**: If file is on your computer, not in Drive\n",
    "\n",
    "### Option 3: Upload Original File (~30-35 minutes)\n",
    "- **File**: `public_500k_filtered.jsonl` (870 MB) from local machine\n",
    "- **Time**: ~30-35 minutes\n",
    "- **Best for**: If you only have uncompressed version locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b823f0",
   "metadata": {},
   "source": [
    "### Option 1: Download from Google Drive (FASTEST! ‚ö°)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08661216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "!mkdir -p data/phase1\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"=\" * 60)\n",
    "print(\"üì§ OPTION 1: Download from Google Drive (FASTEST!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüîå Step 1: Mounting Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã INSTRUCTIONS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Find your file in Google Drive\")\n",
    "print(\"2. Right-click ‚Üí Get link ‚Üí Copy link\")\n",
    "print(\"3. Extract the FILE_ID from the link\")\n",
    "print(\"   Example: https://drive.google.com/file/d/1ABC123XYZ/view\")\n",
    "print(\"   FILE_ID = '1ABC123XYZ'\")\n",
    "print(\"\\n4. EDIT the FILE_ID below (line with FILE_ID = ...)\")\n",
    "print(\"5. UNCOMMENT the download method you want to use\")\n",
    "print(\"6. Re-run this cell\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# PASTE YOUR GOOGLE DRIVE FILE ID HERE:\n",
    "# ============================================================\n",
    "FILE_ID = \"YOUR_FILE_ID_HERE\"\n",
    "\n",
    "# Alternative: If you know the exact path in your Drive\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/path/to/public_500k_filtered.jsonl.gz\"\n",
    "\n",
    "print(\"\\n\udd0d Debug Info:\")\n",
    "print(f\"  FILE_ID set to: {FILE_ID}\")\n",
    "print(f\"  DRIVE_PATH set to: {DRIVE_PATH}\")\n",
    "\n",
    "# ============================================================\n",
    "# METHOD A: Using FILE_ID with gdown (recommended)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ METHOD A: Using FILE_ID\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ö†Ô∏è  Currently COMMENTED OUT - Uncomment lines below to use:\")\n",
    "print()\n",
    "\n",
    "# UNCOMMENT THESE 3 LINES AFTER ADDING YOUR FILE_ID:\n",
    "# print(\"üöÄ Starting download from Google Drive...\")\n",
    "# !gdown --id {FILE_ID} -O data/phase1/public_500k_filtered.jsonl.gz\n",
    "# print(\"üì¶ Download complete! Decompressing...\")\n",
    "# !gunzip -f data/phase1/public_500k_filtered.jsonl.gz\n",
    "# print(\"‚úÖ Download and decompression complete!\")\n",
    "# !ls -lh data/phase1/public_500k_filtered.jsonl\n",
    "\n",
    "# ============================================================\n",
    "# METHOD B: Using Drive path (alternative)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ METHOD B: Using Drive Path\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ö†Ô∏è  Currently COMMENTED OUT - Uncomment lines below to use:\")\n",
    "print()\n",
    "\n",
    "# UNCOMMENT THESE LINES IF YOU PREFER DRIVE PATH:\n",
    "# print(\"üöÄ Copying from Google Drive...\")\n",
    "# !cp \"{DRIVE_PATH}\" data/phase1/\n",
    "# print(\"üì¶ Copy complete! Decompressing...\")\n",
    "# !gunzip -f data/phase1/public_500k_filtered.jsonl.gz\n",
    "# print(\"‚úÖ Copy and decompression complete!\")\n",
    "# !ls -lh data/phase1/public_500k_filtered.jsonl\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° NEXT STEPS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. ‚úèÔ∏è  Edit FILE_ID above (replace YOUR_FILE_ID_HERE)\")\n",
    "print(\"2. üîì Uncomment the method you want (remove # from lines)\")\n",
    "print(\"3. ‚ñ∂Ô∏è  Re-run this cell\")\n",
    "print(\"4. ‚úÖ You should see download progress and file listing\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e62566",
   "metadata": {},
   "source": [
    "#### Quick Debug: Check if file exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check if dataset was successfully downloaded\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç DATASET CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if file exists\n",
    "dataset_path = \"data/phase1/public_500k_filtered.jsonl\"\n",
    "compressed_path = \"data/phase1/public_500k_filtered.jsonl.gz\"\n",
    "\n",
    "print(f\"\\nüìÇ Checking directory contents:\")\n",
    "if os.path.exists(\"data/phase1\"):\n",
    "    !ls -lh data/phase1/\n",
    "else:\n",
    "    print(\"‚ùå Directory data/phase1/ doesn't exist yet\")\n",
    "\n",
    "print(f\"\\nüìÑ File status:\")\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"‚úÖ Dataset file exists: {dataset_path}\")\n",
    "    !wc -l {dataset_path}\n",
    "elif os.path.exists(compressed_path):\n",
    "    print(f\"‚ö†Ô∏è  Compressed file exists but not decompressed: {compressed_path}\")\n",
    "    print(\"üí° Run: !gunzip data/phase1/public_500k_filtered.jsonl.gz\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset NOT found\")\n",
    "    print(f\"   Expected: {dataset_path}\")\n",
    "    print(f\"   or: {compressed_path}\")\n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   1. Check your FILE_ID is correct\")\n",
    "    print(\"   2. Make sure you uncommented the download lines\")\n",
    "    print(\"   3. Re-run the cell above\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb67214",
   "metadata": {},
   "source": [
    "### Option 2: Upload Compressed File from Local (~9-10 minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "!mkdir -p data/phase1\n",
    "\n",
    "# Upload compressed dataset file from local machine\n",
    "from google.colab import files\n",
    "print(\"=\" * 60)\n",
    "print(\"üì§ OPTION 2: Upload compressed file from local\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÇ Click 'Choose Files' and select: public_500k_filtered.jsonl.gz\")\n",
    "print(\"‚è±Ô∏è  Upload: ~9-10 minutes (264 MB)\")\n",
    "print(\"\\nWaiting for file selection...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move and decompress\n",
    "print(\"\\nüì¶ Moving and decompressing file...\")\n",
    "!mv public_500k_filtered.jsonl.gz data/phase1/\n",
    "!gunzip data/phase1/public_500k_filtered.jsonl.gz\n",
    "\n",
    "print(\"\\n‚úÖ Upload and decompression complete! Verifying...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a633848",
   "metadata": {},
   "source": [
    "### Option 3: Upload Original File from Local (~30-35 minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d57b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "!mkdir -p data/phase1\n",
    "\n",
    "# Upload uncompressed dataset file from local machine\n",
    "from google.colab import files\n",
    "print(\"=\" * 60)\n",
    "print(\"üì§ OPTION 3: Upload original file from local\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÇ Click 'Choose Files' and select: public_500k_filtered.jsonl\")\n",
    "print(\"‚è±Ô∏è  Upload: ~30-35 minutes (870 MB)\")\n",
    "print(\"\\nWaiting for file selection...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move to correct location\n",
    "print(\"\\nüì¶ Moving file to data/phase1/...\")\n",
    "!mv public_500k_filtered.jsonl data/phase1/\n",
    "\n",
    "print(\"\\n‚úÖ Upload complete! Verifying...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset uploaded correctly\n",
    "import json\n",
    "\n",
    "print(\"üìä Dataset Verification:\\n\")\n",
    "\n",
    "# Check file exists and size\n",
    "!ls -lh data/phase1/public_500k_filtered.jsonl\n",
    "\n",
    "# Count lines\n",
    "print(\"\\nüìè Line count:\")\n",
    "!wc -l data/phase1/public_500k_filtered.jsonl\n",
    "\n",
    "# Verify format (first 3 examples)\n",
    "print(\"\\n‚úÖ First 3 examples:\")\n",
    "with open('data/phase1/public_500k_filtered.jsonl', 'r') as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        example = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Keys: {list(example.keys())}\")\n",
    "        if 'instruction' in example:\n",
    "            print(f\"  Instruction: {example['instruction'][:80]}...\")\n",
    "        if 'response' in example:\n",
    "            print(f\"  Response: {example['response'][:80]}...\")\n",
    "\n",
    "print(\"\\nüéâ Dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c24cb5",
   "metadata": {},
   "source": [
    "### Optional: Verify Dataset Quality (Run in Background)\n",
    "\n",
    "You can verify dataset quality while setting up other components. This takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ef144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Run verification in background (recommended)\n",
    "# This allows you to continue with other setup tasks\n",
    "!nohup python src/phase0_dataset/verify_dataset.py --sample-size 10000 > verify.log 2>&1 &\n",
    "print(\"‚úÖ Verification running in background. Check progress with: !tail -f verify.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adce50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Check verification progress\n",
    "!tail -20 verify.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option C: Check if verification is still running\n",
    "!ps aux | grep verify_dataset.py | grep -v grep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858800f",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Authentication\n",
    "\n",
    "You need a HuggingFace token to download LLAMA-3.1-8B.\n",
    "\n",
    "1. Go to: https://huggingface.co/settings/tokens\n",
    "2. Create a new token (read access)\n",
    "3. Accept LLAMA-3.1 license at: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "4. Paste token below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091345ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your HuggingFace token here\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ HuggingFace authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631b9d4",
   "metadata": {},
   "source": [
    "## 5. Create Training Script\n",
    "\n",
    "We'll use HuggingFace Trainer directly (more stable than Axolotl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a514d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_qlora.py\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "output_dir = \"./data/checkpoints/llama-3.1-8b-phase1a\"\n",
    "\n",
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    report_to=\"tensorboard\",\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"data/phase1/public_500k_filtered.jsonl\", split=\"train\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Combine instruction and response\n",
    "    texts = []\n",
    "    for inst, resp in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        texts.append(f\"{inst}\\n\\n{resp}\")\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "print(\"Creating trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf149e3",
   "metadata": {},
   "source": [
    "## 6. Verify Dataset Format\n",
    "\n",
    "Let's check the dataset format before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c89436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few examples\n",
    "import json\n",
    "\n",
    "print(\"üìä Dataset format check:\\n\")\n",
    "\n",
    "with open('data/phase1/public_500k_filtered.jsonl', 'r') as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        example = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Keys: {list(example.keys())}\")\n",
    "        if 'instruction' in example:\n",
    "            print(f\"  Instruction: {example['instruction'][:100]}...\")\n",
    "        if 'response' in example:\n",
    "            print(f\"  Response: {example['response'][:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset format confirmed: instruction + response pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34494b32",
   "metadata": {},
   "source": [
    "## 7. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will run for 36-48 hours**. Colab Pro+ sessions timeout after 24 hours, so you'll need to resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2178e2",
   "metadata": {},
   "source": [
    "### üîß CHECKPOINT: Verify Installation\n",
    "\n",
    "**üìç Run this cell after installing dependencies (Section 2)**\n",
    "\n",
    "This verifies all packages are correctly installed with the right versions.\n",
    "\n",
    "**If verification fails:**\n",
    "1. Runtime ‚Üí Restart runtime\n",
    "2. Rerun Section 2 (Dependencies)\n",
    "3. Rerun this verification cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîç VERIFICATION CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã Testing all package installations...\\n\")\n",
    "\n",
    "import sys\n",
    "all_good = True\n",
    "\n",
    "# Test critical imports\n",
    "try:\n",
    "    import torch\n",
    "    assert torch.__version__.startswith(\"2.4\"), f\"Wrong torch version: {torch.__version__}\"\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PyTorch error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    # Updated to check for 4.46 (LLAMA-3.1 compatible version)\n",
    "    assert transformers.__version__.startswith(\"4.46\") or transformers.__version__.startswith(\"4.4\"), \\\n",
    "        f\"Wrong transformers version: {transformers.__version__} (need >= 4.43.0 for LLAMA-3.1)\"\n",
    "    print(f\"‚úÖ Transformers {transformers.__version__}\")\n",
    "    \n",
    "    # Additional check for LLAMA-3.1 compatibility\n",
    "    version_parts = transformers.__version__.split('.')\n",
    "    major, minor = int(version_parts[0]), int(version_parts[1])\n",
    "    if major == 4 and minor < 43:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: transformers {transformers.__version__} may not support LLAMA-3.1\")\n",
    "        print(f\"   Minimum required: 4.43.0 for rope_scaling support\")\n",
    "        all_good = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"‚úÖ Accelerate {accelerate.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Accelerate error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"‚úÖ PEFT {peft.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PEFT error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(f\"‚úÖ BitsAndBytes {bitsandbytes.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå BitsAndBytes error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import trl\n",
    "    print(f\"‚úÖ TRL {trl.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TRL error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    # Test critical transformers imports\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print(f\"‚úÖ Transformers models imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers model import error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_good:\n",
    "    print(\"\\nüéâ All packages installed correctly!\")\n",
    "    print(\"‚úÖ Transformers version is LLAMA-3.1 compatible (>= 4.43.0)\")\n",
    "    print(\"üöÄ Ready to proceed with training setup (Section 3)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some packages have issues!\")\n",
    "    print(\"\\nüí° Fix: Runtime ‚Üí Restart runtime ‚Üí Rerun Section 2 ‚Üí Rerun this cell\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac690c",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è EMERGENCY ONLY: Complete Clean Restart\n",
    "\n",
    "**üìç Only use if verification fails or training won't start**\n",
    "\n",
    "This will restart your runtime completely. You'll need to rerun all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42559ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  NUCLEAR OPTION - RUNTIME RESTART\")\n",
    "print(\"=\" * 60)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nThis option will:\")\n",
    "print(\"  1. Kill your current runtime\")\n",
    "print(\"  2. Clear all installed packages\")\n",
    "print(\"  3. Clear all variables and uploaded files\")\n",
    "print(\"\\nAfter restart, you'll need to:\")\n",
    "print(\"  ‚Ä¢ Rerun cell 7 (Dependencies)\")\n",
    "print(\"  ‚Ä¢ Re-upload dataset\")\n",
    "print(\"  ‚Ä¢ Rerun all setup cells\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nTo proceed, uncomment the line below and run this cell:\")\n",
    "print()\n",
    "\n",
    "# Uncomment this line to restart runtime:\n",
    "# import os; os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard in background (open in new tab)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir data/checkpoints/llama-3.1-8b-phase1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9201926",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ LAUNCH TRAINING (Section 7)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Launch training with error suppression\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress torchvision warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::RuntimeError'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Launching training...\")\n",
    "print(\"‚è±Ô∏è  Expected duration: 26-35 hours on A100-80GB\")\n",
    "print(\"üìä Monitor progress in TensorBoard (see cell above)\\n\")\n",
    "\n",
    "!python train_qlora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73cbf0",
   "metadata": {},
   "source": [
    "## 8. Resume Training (After Session Timeout)\n",
    "\n",
    "If Colab disconnects, the training script automatically saves checkpoints. To resume, modify `train_qlora.py` and add `resume_from_checkpoint` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available checkpoints\n",
    "!ls -lh data/checkpoints/llama-3.1-8b-phase1a/\n",
    "\n",
    "# Find latest checkpoint\n",
    "import os\n",
    "import re\n",
    "\n",
    "checkpoint_dir = \"data/checkpoints/llama-3.1-8b-phase1a\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        # Sort by step number\n",
    "        checkpoints.sort(key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "        latest = checkpoints[-1]\n",
    "        latest_path = f\"{checkpoint_dir}/{latest}\"\n",
    "        print(f\"\\n‚úÖ Latest checkpoint: {latest}\")\n",
    "        print(f\"\\nTo resume training, modify train_qlora.py:\")\n",
    "        print(f\"Add this line before trainer.train():\")\n",
    "        print(f'  resume_checkpoint = \"{latest_path}\"')\n",
    "        print(f'  trainer.train(resume_from_checkpoint=resume_checkpoint)')\n",
    "    else:\n",
    "        print(\"No checkpoints found yet.\")\n",
    "else:\n",
    "    print(\"Checkpoint directory doesn't exist yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b56607",
   "metadata": {},
   "source": [
    "## 9. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca244cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training logs\n",
    "!tail -50 data/checkpoints/llama-3.1-8b-phase1a/training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893174da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainer_state_file = \"data/checkpoints/llama-3.1-8b-phase1a/trainer_state.json\"\n",
    "\n",
    "if os.path.exists(trainer_state_file):\n",
    "    with open(trainer_state_file, 'r') as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    # Extract loss history\n",
    "    steps = []\n",
    "    losses = []\n",
    "    for entry in state['log_history']:\n",
    "        if 'loss' in entry:\n",
    "            steps.append(entry['step'])\n",
    "            losses.append(entry['loss'])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(steps, losses, linewidth=2)\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCurrent step: {state['global_step']}\")\n",
    "    print(f\"Current loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Best loss: {min(losses):.4f}\")\n",
    "    print(f\"Progress: {state['global_step']/60000*100:.1f}% (target: 60K steps)\")\n",
    "else:\n",
    "    print(\"Training state file not found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f3a94",
   "metadata": {},
   "source": [
    "## 10. Merge LoRA Adapters (After Training)\n",
    "\n",
    "Run this after training completes to merge LoRA weights into base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters into base model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"data/checkpoints/llama-3.1-8b-phase1a\"\n",
    ")\n",
    "\n",
    "# Merge and unload\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(\"models/llama-3.1-8b-phase1a-merged\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer.save_pretrained(\"models/llama-3.1-8b-phase1a-merged\")\n",
    "\n",
    "print(\"‚úÖ Model merged and saved to models/llama-3.1-8b-phase1a-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1938ea",
   "metadata": {},
   "source": [
    "## 11. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"models/llama-3.1-8b-phase1a-merged\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "test_prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
    "\n",
    "result = generator(\n",
    "    test_prompt,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b599337",
   "metadata": {},
   "source": [
    "## üìù Step 11: Quick Manual Testing\n",
    "\n",
    "Test the merged model with sample questions to verify it's working correctly.\n",
    "\n",
    "**Expected:** Coherent, accurate responses across different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quick Manual Testing - Run sample questions to verify model quality\n",
    "\"\"\"\n",
    "\n",
    "# Test questions across different domains\n",
    "test_prompts = [\n",
    "    # Code generation\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \n",
    "    # Math reasoning\n",
    "    \"If a train travels 120 miles in 2 hours, what is its average speed in mph?\",\n",
    "    \n",
    "    # Factual knowledge\n",
    "    \"What is the capital of France and what is it famous for?\",\n",
    "    \n",
    "    # General reasoning\n",
    "    \"Explain why the sky appears blue in simple terms.\",\n",
    "    \n",
    "    # Problem solving\n",
    "    \"I have 15 apples. I give 4 to my friend and eat 2. How many do I have left?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MANUAL QUALITY TEST - Sample Responses\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(test_prompts)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Question: {prompt}\\n\")\n",
    "    \n",
    "    # Format as instruction-following prompt\n",
    "    formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "    \n",
    "    print(f\"Answer:\\n{response}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Manual testing complete!\")\n",
    "print(\"Review the responses above to check quality.\")\n",
    "print(\"Expected: Coherent, accurate, and relevant answers.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8061b",
   "metadata": {},
   "source": [
    "## üìä Step 12: Standard Benchmark Evaluation (RECOMMENDED)\n",
    "\n",
    "Run standardized benchmarks to get precise quality metrics. This will run in the background while you can do other work.\n",
    "\n",
    "**Benchmarks:**\n",
    "- **MMLU** (General Knowledge): Target 78-82% (GPT-4: 80%)\n",
    "- **GSM8K** (Math Reasoning): Target 86-88% (GPT-4: 75%)\n",
    "- **HumanEval** (Code): Simplified check (Target: 58-62%)\n",
    "\n",
    "**Time:** 2-4 hours  \n",
    "**Target:** ‚â•87% of GPT-4 baseline\n",
    "\n",
    "‚ö†Ô∏è **Important:** Run this before proceeding to Phase 2 compression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install benchmark dependencies\n",
    "!pip install -q datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard Benchmark Evaluation\n",
    "This will take 2-4 hours to complete. You can let it run in the background.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=512):\n",
    "    \"\"\"Generate model response.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = merged_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,  # Low temperature for deterministic answers\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def evaluate_mmlu(num_samples=1000):\n",
    "    \"\"\"\n",
    "    Evaluate on MMLU benchmark (Massive Multitask Language Understanding).\n",
    "    Tests general knowledge across 57 subjects.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MMLU Benchmark (General Knowledge)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load MMLU dataset\n",
    "    dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
    "    \n",
    "    # Sample for faster evaluation\n",
    "    if num_samples < len(dataset):\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Evaluating MMLU\"):\n",
    "        question = item[\"question\"]\n",
    "        choices = item[\"choices\"]\n",
    "        answer_idx = item[\"answer\"]\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = f\"\"\"<|im_start|>user\n",
    "Question: {question}\n",
    "A) {choices[0]}\n",
    "B) {choices[1]}\n",
    "C) {choices[2]}\n",
    "D) {choices[3]}\n",
    "\n",
    "Answer with just the letter (A, B, C, or D):<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        \n",
    "        response = generate_response(prompt, max_new_tokens=10)\n",
    "        \n",
    "        # Extract answer letter\n",
    "        response_clean = response.split(\"<|im_start|>assistant\")[-1].strip().upper()\n",
    "        pred_letter = response_clean[0] if response_clean else \"\"\n",
    "        \n",
    "        # Check correctness\n",
    "        correct_letter = [\"A\", \"B\", \"C\", \"D\"][answer_idx]\n",
    "        if pred_letter == correct_letter:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        \"benchmark\": \"MMLU\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"target_range\": \"78-82%\",\n",
    "        \"gpt4_baseline\": 0.80,\n",
    "        \"vs_gpt4_pct\": (accuracy / 0.80) * 100 if accuracy > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Accuracy: {accuracy:.1%} ({correct}/{total})\")\n",
    "    print(f\"  Target: 78-82%\")\n",
    "    print(f\"  GPT-4: 80%\")\n",
    "    print(f\"  vs GPT-4: {results['vs_gpt4_pct']:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_gsm8k(num_samples=500):\n",
    "    \"\"\"\n",
    "    Evaluate on GSM8K benchmark (Grade School Math 8K).\n",
    "    Tests mathematical reasoning.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GSM8K Benchmark (Math Reasoning)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load GSM8K dataset\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "    \n",
    "    # Sample for faster evaluation\n",
    "    if num_samples < len(dataset):\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Evaluating GSM8K\"):\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        \n",
    "        # Extract numeric answer\n",
    "        answer_numbers = re.findall(r\"#### ([\\d,]+)\", answer)\n",
    "        if not answer_numbers:\n",
    "            continue\n",
    "        correct_answer = int(answer_numbers[0].replace(\",\", \"\"))\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = f\"\"\"<|im_start|>user\n",
    "Solve this math problem step by step:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide your final answer as a number.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        \n",
    "        response = generate_response(prompt, max_new_tokens=512)\n",
    "        \n",
    "        # Extract predicted answer\n",
    "        response_clean = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "        pred_numbers = re.findall(r\"\\b(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\b\", response_clean)\n",
    "        \n",
    "        if pred_numbers:\n",
    "            try:\n",
    "                pred_answer = int(pred_numbers[-1].replace(\",\", \"\").split(\".\")[0])\n",
    "                if pred_answer == correct_answer:\n",
    "                    correct += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        \"benchmark\": \"GSM8K\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"target_range\": \"86-88%\",\n",
    "        \"gpt4_baseline\": 0.75,\n",
    "        \"vs_gpt4_pct\": (accuracy / 0.75) * 100 if accuracy > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Accuracy: {accuracy:.1%} ({correct}/{total})\")\n",
    "    print(f\"  Target: 86-88%\")\n",
    "    print(f\"  GPT-4: 75%\")\n",
    "    print(f\"  vs GPT-4: {results['vs_gpt4_pct']:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_humaneval_simple(num_samples=164):\n",
    "    \"\"\"\n",
    "    Simple HumanEval check (without code execution).\n",
    "    Just verifies code structure is valid.\n",
    "    \n",
    "    Note: For accurate pass@1, use official HumanEval evaluation harness.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HumanEval Benchmark (Code Generation - Simplified)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Note: This is a simplified check without code execution.\")\n",
    "    print(\"For accurate results, use the official HumanEval evaluation harness.\")\n",
    "    \n",
    "    # Load HumanEval dataset\n",
    "    try:\n",
    "        dataset = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "    except:\n",
    "        print(\"\\nWarning: HumanEval dataset not available. Skipping...\")\n",
    "        return {\n",
    "            \"benchmark\": \"HumanEval\",\n",
    "            \"accuracy\": 0.0,\n",
    "            \"note\": \"Dataset not available\"\n",
    "        }\n",
    "    \n",
    "    # Sample\n",
    "    if num_samples < len(dataset):\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    \n",
    "    valid_completions = 0\n",
    "    total = 0\n",
    "    \n",
    "    for item in tqdm(dataset, desc=\"Evaluating HumanEval\"):\n",
    "        prompt_text = item[\"prompt\"]\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = f\"\"\"<|im_start|>user\n",
    "Complete this Python function:\n",
    "\n",
    "{prompt_text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        \n",
    "        response = generate_response(prompt, max_new_tokens=512)\n",
    "        \n",
    "        # Extract code\n",
    "        code = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "        \n",
    "        # Simple heuristics (not accurate, just sanity check)\n",
    "        has_return = \"return\" in code\n",
    "        has_proper_indentation = \"    \" in code or \"\\t\" in code\n",
    "        has_valid_syntax = code.count(\"(\") == code.count(\")\") and code.count(\"[\") == code.count(\"]\")\n",
    "        \n",
    "        if has_return and has_proper_indentation and has_valid_syntax:\n",
    "            valid_completions += 1\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "    estimated_quality = valid_completions / total if total > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        \"benchmark\": \"HumanEval\",\n",
    "        \"estimated_quality\": estimated_quality,\n",
    "        \"valid_completions\": valid_completions,\n",
    "        \"total\": total,\n",
    "        \"target_range\": \"58-62%\",\n",
    "        \"note\": \"Simplified evaluation\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults (Estimated):\")\n",
    "    print(f\"  Valid Completions: {estimated_quality:.1%} ({valid_completions}/{total})\")\n",
    "    print(f\"  Target: 58-62% (pass@1)\")\n",
    "    print(f\"  Note: Use official harness for accurate pass@1 results\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run all benchmarks\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING BENCHMARK EVALUATION\")\n",
    "print(\"This will take 2-4 hours. You can let it run in the background.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# Run MMLU (1000 samples, ~1.5 hours)\n",
    "all_results[\"mmlu\"] = evaluate_mmlu(num_samples=1000)\n",
    "\n",
    "# Run GSM8K (500 samples, ~1 hour)\n",
    "all_results[\"gsm8k\"] = evaluate_gsm8k(num_samples=500)\n",
    "\n",
    "# Run HumanEval (simplified, ~30 minutes)\n",
    "all_results[\"humaneval\"] = evaluate_humaneval_simple()\n",
    "\n",
    "# Calculate overall score\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mmlu_acc = all_results[\"mmlu\"][\"accuracy\"]\n",
    "gsm8k_acc = all_results[\"gsm8k\"][\"accuracy\"]\n",
    "\n",
    "# Compare to GPT-4 baselines\n",
    "mmlu_vs_gpt4 = mmlu_acc / 0.80\n",
    "gsm8k_vs_gpt4 = gsm8k_acc / 0.75\n",
    "overall_vs_gpt4 = (mmlu_vs_gpt4 + gsm8k_vs_gpt4) / 2\n",
    "\n",
    "print(f\"\\nBenchmark Results:\")\n",
    "print(f\"  MMLU:    {mmlu_acc:.1%} (Target: 78-82%, {mmlu_vs_gpt4*100:.1f}% of GPT-4)\")\n",
    "print(f\"  GSM8K:   {gsm8k_acc:.1%} (Target: 86-88%, {gsm8k_vs_gpt4*100:.1f}% of GPT-4)\")\n",
    "print(f\"\\nOverall: {overall_vs_gpt4*100:.1f}% of GPT-4 baseline\")\n",
    "\n",
    "all_results[\"overall\"] = {\n",
    "    \"vs_gpt4_percentage\": overall_vs_gpt4 * 100,\n",
    "    \"target_range\": \"90-93%\",\n",
    "    \"status\": \"‚úì PASS\" if overall_vs_gpt4 >= 0.87 else \"‚ö† REVIEW\"\n",
    "}\n",
    "\n",
    "if overall_vs_gpt4 >= 0.90:\n",
    "    print(\"\\n‚úì EXCELLENT - Exceeds target! Ready for Phase 2.\")\n",
    "    decision = \"PROCEED\"\n",
    "elif overall_vs_gpt4 >= 0.87:\n",
    "    print(\"\\n‚úì GOOD - Meets target. Proceed to Phase 2.\")\n",
    "    decision = \"PROCEED\"\n",
    "elif overall_vs_gpt4 >= 0.80:\n",
    "    print(\"\\n‚ö† ACCEPTABLE - Slightly below target but usable.\")\n",
    "    decision = \"REVIEW\"\n",
    "else:\n",
    "    print(\"\\n‚úó BELOW TARGET - Review training before Phase 2.\")\n",
    "    decision = \"DEBUG\"\n",
    "\n",
    "all_results[\"decision\"] = decision\n",
    "\n",
    "# Save results\n",
    "with open(\"/content/benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to /content/benchmark_results.json\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITY VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Decision: {decision}\")\n",
    "print(f\"Overall Score: {overall_vs_gpt4*100:.1f}% of GPT-4 baseline\")\n",
    "print(f\"Status: {all_results['overall']['status']}\")\n",
    "print(\"\\nNext Steps:\")\n",
    "if decision == \"PROCEED\":\n",
    "    print(\"  ‚úì Quality validated! Ready for Phase 2 compression.\")\n",
    "    print(\"  ‚úì Download benchmark_results.json for your records.\")\n",
    "    print(\"  ‚úì Proceed to Phase2_Compression_Colab.ipynb\")\n",
    "elif decision == \"REVIEW\":\n",
    "    print(\"  ‚ö† Quality is acceptable but below target.\")\n",
    "    print(\"  ‚ö† Review training logs and data quality.\")\n",
    "    print(\"  ‚ö† You may proceed to Phase 2, but expect lower final quality.\")\n",
    "else:\n",
    "    print(\"  ‚úó Quality is too low. Debug training before Phase 2.\")\n",
    "    print(\"  ‚úó Check: training loss, data quality, model configuration.\")\n",
    "    print(\"  ‚úó Consider re-training with adjustments.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a037f",
   "metadata": {},
   "source": [
    "## üì• Download Benchmark Results\n",
    "\n",
    "Download the benchmark results JSON file to your local machine for your records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download benchmark results\n",
    "files.download('/content/benchmark_results.json')\n",
    "\n",
    "print(\"‚úì benchmark_results.json downloaded!\")\n",
    "print(\"\\nKeep this file for your records. It shows:\")\n",
    "print(\"  - MMLU accuracy and vs GPT-4 %\")\n",
    "print(\"  - GSM8K accuracy and vs GPT-4 %\")\n",
    "print(\"  - Overall quality score\")\n",
    "print(\"  - Decision: PROCEED, REVIEW, or DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaae11e",
   "metadata": {},
   "source": [
    "## 12. Download Model to Local\n",
    "\n",
    "After training completes, download the model to continue with Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e439c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress model for download\n",
    "!tar -czf llama-3.1-8b-phase1a-merged.tar.gz models/llama-3.1-8b-phase1a-merged/\n",
    "!ls -lh llama-3.1-8b-phase1a-merged.tar.gz\n",
    "\n",
    "print(\"\\n‚úÖ Model compressed. Download from Files panel on left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Upload to HuggingFace Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository (change username)\n",
    "repo_id = \"YOUR_USERNAME/cogumi-llm-phase1a\"\n",
    "\n",
    "api.create_repo(repo_id=repo_id, private=True, exist_ok=True)\n",
    "\n",
    "# Upload model\n",
    "api.upload_folder(\n",
    "    folder_path=\"models/llama-3.1-8b-phase1a-merged\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model uploaded to: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b9b99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1A Completion Checklist\n",
    "\n",
    "### Setup & Training\n",
    "- [ ] A100 GPU selected\n",
    "- [ ] Dependencies installed\n",
    "- [ ] Repository cloned\n",
    "- [ ] HuggingFace authenticated\n",
    "- [ ] Dataset verified (640,637 examples)\n",
    "- [ ] Training config created\n",
    "- [ ] Training started\n",
    "- [ ] TensorBoard monitoring active\n",
    "- [ ] Checkpoints saving (every 1000 steps)\n",
    "- [ ] Training completed (60K steps, 3 epochs)\n",
    "- [ ] Final loss ‚â§0.8 (ideally 0.5-0.7)\n",
    "\n",
    "### Post-Training\n",
    "- [ ] LoRA adapters merged into base model\n",
    "- [ ] Quick manual testing completed (Step 11)\n",
    "- [ ] Responses are coherent and accurate\n",
    "- [ ] **Standard benchmarks run (Step 12)** ‚≠ê **CRITICAL**\n",
    "- [ ] Benchmark results: ‚â•87% GPT-4 baseline\n",
    "- [ ] Benchmark results downloaded\n",
    "- [ ] Model uploaded to HuggingFace (recommended)\n",
    "\n",
    "### Quality Gates (Before Phase 2)\n",
    "- [ ] **MMLU**: ‚â•75% (target: 78-82%)\n",
    "- [ ] **GSM8K**: ‚â•80% (target: 86-88%)\n",
    "- [ ] **Overall**: ‚â•87% GPT-4 baseline\n",
    "- [ ] Decision: **PROCEED** to Phase 2\n",
    "\n",
    "## Expected Timeline\n",
    "\n",
    "### Training\n",
    "- **Epoch 1**: 12-14 hours (steps 0-20K)\n",
    "- **Epoch 2**: 12-14 hours (steps 20K-40K)\n",
    "- **Epoch 3**: 12-14 hours (steps 40K-60K)\n",
    "- **Total Training**: 36-48 hours\n",
    "\n",
    "### Validation\n",
    "- **Manual Testing**: 5-10 minutes\n",
    "- **Standard Benchmarks**: 2-4 hours\n",
    "- **Total Post-Training**: ~4 hours\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Training Issues\n",
    "**Session Timeout**: Resume from latest checkpoint (see Step 8)\n",
    "\n",
    "**OOM Error**: Reduce `micro_batch_size` to 2 in config\n",
    "\n",
    "**Slow Progress**: Check GPU utilization with `!nvidia-smi`\n",
    "\n",
    "**Loss Not Decreasing**: Check TensorBoard, may need to reduce learning rate\n",
    "\n",
    "**CUDA Error**: Restart runtime, rerun setup cells\n",
    "\n",
    "### Validation Issues\n",
    "**Low Benchmark Scores (<87% GPT-4)**:\n",
    "- Check training loss (should be 0.5-0.7)\n",
    "- Verify dataset quality (99%+ English)\n",
    "- Review training logs for issues\n",
    "- Consider re-training with adjusted config\n",
    "\n",
    "**Manual Tests Failing**:\n",
    "- Check if model is generating coherent text\n",
    "- Verify chat template format is correct\n",
    "- Test with simpler prompts first\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps After Phase 1A\n",
    "\n",
    "### If Benchmarks Pass (‚â•87% GPT-4):\n",
    "1. ‚úÖ **Download benchmark results** for your records\n",
    "2. ‚úÖ **Ensure model uploaded** to HuggingFace\n",
    "3. ‚úÖ **Proceed to Phase 2**: Compression (11GB ‚Üí 480MB, 97% reduction)\n",
    "4. ‚úÖ Open `Phase2_Compression_Colab.ipynb` in new Colab session\n",
    "\n",
    "### If Benchmarks Below Target (<87% GPT-4):\n",
    "1. ‚ö†Ô∏è **Review** training logs and loss curves\n",
    "2. ‚ö†Ô∏è **Check** dataset quality and format\n",
    "3. ‚ö†Ô∏è **Consider** re-training with adjustments:\n",
    "   - Lower learning rate: 5e-6 ‚Üí 3e-6\n",
    "   - Increase LoRA rank: 64 ‚Üí 128\n",
    "   - Train longer: 3 ‚Üí 4 epochs\n",
    "4. ‚ö†Ô∏è **Or proceed anyway** (expect lower Phase 2 quality)\n",
    "\n",
    "### Phase 2 Expected Results:\n",
    "- **Input**: 11GB Phase 1A model\n",
    "- **Output**: 480MB compressed model\n",
    "- **Quality Retention**: 87-89% of Phase 1A quality\n",
    "- **Time**: 8-10 hours\n",
    "- **Example**: If Phase 1A = 92% GPT-4 ‚Üí Phase 2 = ~80% GPT-4\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready for Phase 2!** Open `docs/PHASE2_QUICK_START.md` for next steps."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

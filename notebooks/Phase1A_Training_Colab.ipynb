{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46907863",
   "metadata": {},
   "source": [
    "# Phase 1A: LLAMA-3.2-8B QLoRA Training\n",
    "\n",
    "**Project:** Cogumi-LLM  \n",
    "**Phase:** 1A - Base Model Distillation  \n",
    "**Duration:** 36-48 hours  \n",
    "**GPU Required:** A100 40GB  \n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Select Runtime**: Runtime ‚Üí Change runtime type ‚Üí A100 GPU\n",
    "2. **Connect to GPU**: Click Connect in top-right\n",
    "3. **Run all cells sequentially**\n",
    "4. **Monitor training**: Check TensorBoard and logs\n",
    "\n",
    "‚ö†Ô∏è **Important**: Colab Pro+ allows up to 24 hours per session. Training takes 36-48 hours, so you'll need to resume from checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d2e19",
   "metadata": {},
   "source": [
    "## üìã Best Practices for Long-Running Tasks\n",
    "\n",
    "**Background Execution**: For verification and monitoring tasks, use `nohup` to run in background:\n",
    "```bash\n",
    "# Run dataset verification in background\n",
    "nohup python src/phase0_dataset/verify_dataset.py --sample-size 10000 > verify.log 2>&1 &\n",
    "\n",
    "# Check progress anytime\n",
    "tail -f verify.log\n",
    "\n",
    "# Check if still running\n",
    "ps aux | grep verify_dataset\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- ‚úÖ Continue working on other setup tasks\n",
    "- ‚úÖ Process survives if you switch cells\n",
    "- ‚úÖ Can monitor multiple tasks simultaneously\n",
    "- ‚úÖ Logs saved for later review\n",
    "\n",
    "**When to Use Background**:\n",
    "- Dataset verification (5-10 minutes)\n",
    "- Model downloads (10-15 minutes)\n",
    "- Benchmark evaluations (15-30 minutes)\n",
    "- **NOT for training** (use TensorBoard for monitoring)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d50bc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44345b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have A100\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Verify it's A100\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "if 'A100' not in gpu_name:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: You need A100 GPU for this training!\")\n",
    "    print(\"Go to Runtime ‚Üí Change runtime type ‚Üí Select A100\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ A100 GPU detected! Ready to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5c6ed",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "‚ö†Ô∏è **Important**: Colab comes with pre-installed packages that conflict with our requirements. We'll clean install everything.\n",
    "\n",
    "**‚è±Ô∏è Estimated time: 5-7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üì¶ DEPENDENCY INSTALLATION (Section 2)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüßπ Step 1: Removing conflicting pre-installed packages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Remove ALL conflicting packages that come pre-installed in Colab\n",
    "conflicting_packages = [\n",
    "    'torch', 'torchvision', 'torchaudio',  # Colab has torch 2.8.0, we need 2.4.0\n",
    "    'transformers', 'accelerate', 'peft',   # Need specific versions\n",
    "    'tensorflow', 'tensorboard',            # TensorFlow conflicts with our tensorboard\n",
    "    'opencv-python', 'opencv-python-headless', 'opencv-contrib-python',  # Require numpy 2.x\n",
    "    'timm', 'pillow',                      # Vision packages not needed\n",
    "    'axolotl',                             # May exist from previous runs\n",
    "]\n",
    "\n",
    "for package in conflicting_packages:\n",
    "    !pip uninstall -y {package} 2>/dev/null || true\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì¶ Step 2: Installing compatible package versions...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install PyTorch ecosystem with compatible versions\n",
    "!pip install -q torch==2.4.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install core ML packages\n",
    "!pip install -q transformers==4.41.0\n",
    "!pip install -q accelerate==0.33.0\n",
    "!pip install -q peft==0.12.0\n",
    "!pip install -q bitsandbytes==0.43.3\n",
    "\n",
    "# Install data handling packages\n",
    "!pip install -q datasets==2.20.0\n",
    "!pip install -q tokenizers==0.19.1\n",
    "!pip install -q numpy==1.26.4  # Compatible with all our packages\n",
    "\n",
    "# Install monitoring packages\n",
    "!pip install -q wandb\n",
    "!pip install -q tensorboard==2.17.0  # Compatible version\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì¶ Step 3: Installing Axolotl (this may take 2-3 minutes)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install Axolotl v0.4.0 (compatible with our package versions)\n",
    "!pip install -q --no-deps git+https://github.com/OpenAccess-AI-Collective/axolotl.git@v0.4.0\n",
    "\n",
    "# Install Axolotl's dependencies manually to avoid conflicts\n",
    "!pip install -q fire pyyaml huggingface-hub\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All dependencies installed successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã Installed versions:\")\n",
    "print(f\"  ‚Ä¢ torch: 2.4.0\")\n",
    "print(f\"  ‚Ä¢ transformers: 4.41.0\")\n",
    "print(f\"  ‚Ä¢ accelerate: 0.33.0\")\n",
    "print(f\"  ‚Ä¢ peft: 0.12.0\")\n",
    "print(f\"  ‚Ä¢ bitsandbytes: 0.43.3\")\n",
    "print(f\"  ‚Ä¢ datasets: 2.20.0\")\n",
    "print(f\"  ‚Ä¢ axolotl: v0.4.0\")\n",
    "print(\"\\nüí° Clean install complete - no dependency conflicts!\")\n",
    "print(\"üöÄ Ready to proceed with training setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f15e8",
   "metadata": {},
   "source": [
    "## 3. Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (or pull latest changes if already exists)\n",
    "import os\n",
    "\n",
    "if os.path.exists('Cogumi-LLM'):\n",
    "    print(\"üìÇ Repository already exists, pulling latest changes...\")\n",
    "    %cd Cogumi-LLM\n",
    "    !git pull origin main\n",
    "    print(\"‚úÖ Repository updated to latest version\")\n",
    "else:\n",
    "    print(\"üì• Cloning repository...\")\n",
    "    !git clone https://github.com/dkeviv/Cogumi-LLM.git\n",
    "    %cd Cogumi-LLM\n",
    "    print(\"‚úÖ Repository cloned successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c532584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset exists\n",
    "!ls -lh data/phase1/public_500k_filtered.jsonl\n",
    "!wc -l data/phase1/public_500k_filtered.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d56f4",
   "metadata": {},
   "source": [
    "## 3b. Upload Dataset\n",
    "\n",
    "‚ö†Ô∏è **Important**: The dataset is not in the Git repository (too large). You need to upload it.\n",
    "\n",
    "**Choose one option:**\n",
    "\n",
    "### Option 1: Upload Compressed File (RECOMMENDED - 3x faster!)\n",
    "- **File**: `public_500k_filtered.jsonl.gz` (264 MB)\n",
    "- **Time**: ~9-10 minutes\n",
    "- **Location**: `/Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl.gz`\n",
    "\n",
    "### Option 2: Upload Original File\n",
    "- **File**: `public_500k_filtered.jsonl` (870 MB)\n",
    "- **Time**: ~30-35 minutes\n",
    "- **Location**: `/Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b823f0",
   "metadata": {},
   "source": [
    "### Option 1: Upload Compressed File (9-10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08661216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "!mkdir -p data/phase1\n",
    "\n",
    "# Upload compressed dataset file\n",
    "from google.colab import files\n",
    "print(\"üì§ OPTION 1: Upload compressed file (FASTER)\")\n",
    "print(\"üìÇ Click 'Choose Files' and select: public_500k_filtered.jsonl.gz\")\n",
    "print(\"üìç Location: /Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/\")\n",
    "print(\"‚è±Ô∏è  Upload: ~9-10 minutes (264 MB)\")\n",
    "print(\"\\nWaiting for file selection...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move and decompress\n",
    "print(\"\\nüì¶ Moving and decompressing file...\")\n",
    "!mv public_500k_filtered.jsonl.gz data/phase1/\n",
    "!gunzip data/phase1/public_500k_filtered.jsonl.gz\n",
    "\n",
    "print(\"\\n‚úÖ Upload and decompression complete! Verifying...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb67214",
   "metadata": {},
   "source": [
    "### Option 2: Upload Original File (30-35 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "!mkdir -p data/phase1\n",
    "\n",
    "# Upload uncompressed dataset file\n",
    "from google.colab import files\n",
    "print(\"üì§ OPTION 2: Upload original file (slower)\")\n",
    "print(\"üìÇ Click 'Choose Files' and select: public_500k_filtered.jsonl\")\n",
    "print(\"üìç Location: /Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/\")\n",
    "print(\"‚è±Ô∏è  Upload: ~30-35 minutes (870 MB)\")\n",
    "print(\"\\nWaiting for file selection...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move to correct location\n",
    "print(\"\\nüì¶ Moving file to data/phase1/...\")\n",
    "!mv public_500k_filtered.jsonl data/phase1/\n",
    "\n",
    "print(\"\\n‚úÖ Upload complete! Verifying...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset uploaded correctly\n",
    "import json\n",
    "\n",
    "print(\"üìä Dataset Verification:\\n\")\n",
    "\n",
    "# Check file exists and size\n",
    "!ls -lh data/phase1/public_500k_filtered.jsonl\n",
    "\n",
    "# Count lines\n",
    "print(\"\\nüìè Line count:\")\n",
    "!wc -l data/phase1/public_500k_filtered.jsonl\n",
    "\n",
    "# Verify format (first 3 examples)\n",
    "print(\"\\n‚úÖ First 3 examples:\")\n",
    "with open('data/phase1/public_500k_filtered.jsonl', 'r') as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        example = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Keys: {list(example.keys())}\")\n",
    "        if 'instruction' in example:\n",
    "            print(f\"  Instruction: {example['instruction'][:80]}...\")\n",
    "        if 'response' in example:\n",
    "            print(f\"  Response: {example['response'][:80]}...\")\n",
    "\n",
    "print(\"\\nüéâ Dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c24cb5",
   "metadata": {},
   "source": [
    "### Optional: Verify Dataset Quality (Run in Background)\n",
    "\n",
    "You can verify dataset quality while setting up other components. This takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ef144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Run verification in background (recommended)\n",
    "# This allows you to continue with other setup tasks\n",
    "!nohup python src/phase0_dataset/verify_dataset.py --sample-size 10000 > verify.log 2>&1 &\n",
    "print(\"‚úÖ Verification running in background. Check progress with: !tail -f verify.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adce50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Check verification progress\n",
    "!tail -20 verify.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option C: Check if verification is still running\n",
    "!ps aux | grep verify_dataset.py | grep -v grep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858800f",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Authentication\n",
    "\n",
    "You need a HuggingFace token to download LLAMA-3.2-8B.\n",
    "\n",
    "1. Go to: https://huggingface.co/settings/tokens\n",
    "2. Create a new token (read access)\n",
    "3. Accept LLAMA-3.2 license at: https://huggingface.co/meta-llama/Llama-3.2-8B\n",
    "4. Paste token below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091345ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your HuggingFace token here\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ HuggingFace authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631b9d4",
   "metadata": {},
   "source": [
    "## 5. Create Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a514d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configs/base_training.yaml\n",
    "# Base Model Configuration\n",
    "base_model: meta-llama/Llama-3.2-8B\n",
    "model_type: LlamaForCausalLM\n",
    "tokenizer_type: AutoTokenizer\n",
    "trust_remote_code: false\n",
    "\n",
    "# QLoRA Configuration\n",
    "load_in_4bit: true\n",
    "bnb_4bit_quant_type: nf4\n",
    "bnb_4bit_use_double_quant: true\n",
    "bnb_4bit_compute_dtype: bfloat16\n",
    "\n",
    "adapter: lora\n",
    "lora_r: 64\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules:\n",
    "  - q_proj\n",
    "  - k_proj\n",
    "  - v_proj\n",
    "  - o_proj\n",
    "  - gate_proj\n",
    "  - up_proj\n",
    "  - down_proj\n",
    "\n",
    "# Dataset Configuration\n",
    "datasets:\n",
    "  - path: data/phase1/public_500k_filtered.jsonl\n",
    "    type: completion\n",
    "    field: response\n",
    "\n",
    "sequence_len: 2048\n",
    "sample_packing: true\n",
    "pad_to_sequence_len: true\n",
    "max_packed_sequence_len: 2048\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs: 3\n",
    "micro_batch_size: 4\n",
    "gradient_accumulation_steps: 8\n",
    "gradient_checkpointing: true\n",
    "\n",
    "# Optimizer Configuration\n",
    "optimizer: adamw_torch\n",
    "learning_rate: 0.000005\n",
    "lr_scheduler: cosine\n",
    "warmup_steps: 500\n",
    "weight_decay: 0.01\n",
    "adam_beta1: 0.9\n",
    "adam_beta2: 0.999\n",
    "adam_epsilon: 1e-8\n",
    "max_grad_norm: 1.0\n",
    "\n",
    "# Precision & Hardware\n",
    "bf16: true\n",
    "tf32: true\n",
    "flash_attention: true\n",
    "\n",
    "# Logging & Checkpointing\n",
    "logging_steps: 10\n",
    "eval_steps: 500\n",
    "save_steps: 1000\n",
    "save_total_limit: 5\n",
    "output_dir: ./data/checkpoints/llama-3.2-8b-phase1a\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping_patience: 6\n",
    "load_best_model_at_end: true\n",
    "metric_for_best_model: loss\n",
    "greater_is_better: false\n",
    "\n",
    "# Evaluation\n",
    "evaluation_strategy: steps\n",
    "eval_steps: 500\n",
    "per_device_eval_batch_size: 4\n",
    "eval_accumulation_steps: 4\n",
    "\n",
    "# Additional Optimizations\n",
    "group_by_length: true\n",
    "ddp_find_unused_parameters: false\n",
    "dataloader_num_workers: 4\n",
    "dataloader_pin_memory: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf149e3",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset for Axolotl\n",
    "\n",
    "Axolotl needs specific format. Let's verify and convert if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c89436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few examples\n",
    "import json\n",
    "\n",
    "with open('data/phase1/public_500k_filtered.jsonl', 'r') as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        example = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Keys: {list(example.keys())}\")\n",
    "        if 'instruction' in example:\n",
    "            print(f\"Instruction: {example['instruction'][:100]}...\")\n",
    "        if 'response' in example:\n",
    "            print(f\"Response: {example['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34494b32",
   "metadata": {},
   "source": [
    "## 7. Start Training\n",
    "\n",
    "‚ö†Ô∏è **This will run for 36-48 hours**. Colab Pro+ sessions timeout after 24 hours, so you'll need to resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2178e2",
   "metadata": {},
   "source": [
    "### üîß CHECKPOINT: Verify Installation\n",
    "\n",
    "**üìç Run this cell after installing dependencies (Section 2)**\n",
    "\n",
    "This verifies all packages are correctly installed and compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîç VERIFICATION CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã Testing all package installations...\\n\")\n",
    "\n",
    "import sys\n",
    "all_good = True\n",
    "\n",
    "# Test critical imports\n",
    "try:\n",
    "    import torch\n",
    "    assert torch.__version__.startswith(\"2.4\"), f\"Wrong torch version: {torch.__version__}\"\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PyTorch error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    assert transformers.__version__.startswith(\"4.41\"), f\"Wrong transformers version: {transformers.__version__}\"\n",
    "    print(f\"‚úÖ Transformers {transformers.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"‚úÖ Accelerate {accelerate.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Accelerate error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"‚úÖ PEFT {peft.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PEFT error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(f\"‚úÖ BitsAndBytes {bitsandbytes.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå BitsAndBytes error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    import axolotl\n",
    "    print(f\"‚úÖ Axolotl imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Axolotl error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "try:\n",
    "    # Test critical transformers imports\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    print(f\"‚úÖ Transformers models imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers model import error: {e}\")\n",
    "    all_good = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_good:\n",
    "    print(\"\\nüéâ All packages installed correctly!\")\n",
    "    print(\"üöÄ Ready to proceed with training setup\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some packages have issues!\")\n",
    "    print(\"üí° Solution: Runtime ‚Üí Restart runtime, then rerun cell 7 (Dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac690c",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è EMERGENCY ONLY: Complete Clean Restart\n",
    "\n",
    "**üìç Only use if verification fails or training won't start**\n",
    "\n",
    "This will restart your runtime completely. You'll need to rerun all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42559ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"‚ö†Ô∏è  NUCLEAR OPTION - RUNTIME RESTART\")\n",
    "print(\"=\" * 60)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nThis option will:\")\n",
    "print(\"  1. Kill your current runtime\")\n",
    "print(\"  2. Clear all installed packages\")\n",
    "print(\"  3. Clear all variables and uploaded files\")\n",
    "print(\"\\nAfter restart, you'll need to:\")\n",
    "print(\"  ‚Ä¢ Rerun cell 7 (Dependencies)\")\n",
    "print(\"  ‚Ä¢ Re-upload dataset\")\n",
    "print(\"  ‚Ä¢ Rerun all setup cells\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nTo proceed, uncomment the line below and run this cell:\")\n",
    "print()\n",
    "\n",
    "# Uncomment this line to restart runtime:\n",
    "# import os; os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard in background (open in new tab)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir data/checkpoints/llama-3.2-8b-phase1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9201926",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ LAUNCH TRAINING (Section 7)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Launch training with error suppression\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress torchvision warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::RuntimeError'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Launching training...\")\n",
    "print(\"‚è±Ô∏è  Expected duration: 26-35 hours on A100-80GB\")\n",
    "print(\"üìä Monitor progress in TensorBoard (see cell above)\\n\")\n",
    "\n",
    "!accelerate launch -m axolotl.cli.train configs/base_training.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73cbf0",
   "metadata": {},
   "source": [
    "## 8. Resume Training (After Session Timeout)\n",
    "\n",
    "If Colab disconnects, run cells 1-5 again, then run this cell to resume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available checkpoints\n",
    "!ls -lh data/checkpoints/llama-3.2-8b-phase1a/\n",
    "\n",
    "# Find latest checkpoint\n",
    "import os\n",
    "import re\n",
    "\n",
    "checkpoint_dir = \"data/checkpoints/llama-3.2-8b-phase1a\"\n",
    "checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith('checkpoint-')]\n",
    "if checkpoints:\n",
    "    # Sort by step number\n",
    "    checkpoints.sort(key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    latest = checkpoints[-1]\n",
    "    print(f\"\\n‚úÖ Latest checkpoint: {latest}\")\n",
    "    print(f\"\\nTo resume, update configs/base_training.yaml:\")\n",
    "    print(f\"Add: resume_from_checkpoint: {checkpoint_dir}/{latest}\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from latest checkpoint\n",
    "!accelerate launch -m axolotl.cli.train configs/base_training.yaml --resume_from_checkpoint data/checkpoints/llama-3.2-8b-phase1a/checkpoint-XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b56607",
   "metadata": {},
   "source": [
    "## 9. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca244cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training logs\n",
    "!tail -50 data/checkpoints/llama-3.2-8b-phase1a/training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893174da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trainer_state_file = \"data/checkpoints/llama-3.2-8b-phase1a/trainer_state.json\"\n",
    "\n",
    "if os.path.exists(trainer_state_file):\n",
    "    with open(trainer_state_file, 'r') as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    # Extract loss history\n",
    "    steps = []\n",
    "    losses = []\n",
    "    for entry in state['log_history']:\n",
    "        if 'loss' in entry:\n",
    "            steps.append(entry['step'])\n",
    "            losses.append(entry['loss'])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(steps, losses, linewidth=2)\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCurrent step: {state['global_step']}\")\n",
    "    print(f\"Current loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Best loss: {min(losses):.4f}\")\n",
    "    print(f\"Progress: {state['global_step']/60000*100:.1f}% (target: 60K steps)\")\n",
    "else:\n",
    "    print(\"Training state file not found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f3a94",
   "metadata": {},
   "source": [
    "## 10. Merge LoRA Adapters (After Training)\n",
    "\n",
    "Run this after training completes to merge LoRA weights into base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters into base model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-8B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"data/checkpoints/llama-3.2-8b-phase1a\"\n",
    ")\n",
    "\n",
    "# Merge and unload\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(\"models/llama-3.2-8b-phase1a-merged\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-8B\")\n",
    "tokenizer.save_pretrained(\"models/llama-3.2-8b-phase1a-merged\")\n",
    "\n",
    "print(\"‚úÖ Model merged and saved to models/llama-3.2-8b-phase1a-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1938ea",
   "metadata": {},
   "source": [
    "## 11. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"models/llama-3.2-8b-phase1a-merged\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "test_prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
    "\n",
    "result = generator(\n",
    "    test_prompt,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaae11e",
   "metadata": {},
   "source": [
    "## 12. Download Model to Local\n",
    "\n",
    "After training completes, download the model to continue with Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e439c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress model for download\n",
    "!tar -czf llama-3.2-8b-phase1a-merged.tar.gz models/llama-3.2-8b-phase1a-merged/\n",
    "!ls -lh llama-3.2-8b-phase1a-merged.tar.gz\n",
    "\n",
    "print(\"\\n‚úÖ Model compressed. Download from Files panel on left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Upload to HuggingFace Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create repository (change username)\n",
    "repo_id = \"YOUR_USERNAME/cogumi-llm-phase1a\"\n",
    "\n",
    "api.create_repo(repo_id=repo_id, private=True, exist_ok=True)\n",
    "\n",
    "# Upload model\n",
    "api.upload_folder(\n",
    "    folder_path=\"models/llama-3.2-8b-phase1a-merged\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model uploaded to: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b9b99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Checklist\n",
    "\n",
    "- [ ] A100 GPU selected\n",
    "- [ ] Dependencies installed\n",
    "- [ ] Repository cloned\n",
    "- [ ] HuggingFace authenticated\n",
    "- [ ] Dataset verified (640,637 examples)\n",
    "- [ ] Training config created\n",
    "- [ ] Training started\n",
    "- [ ] TensorBoard monitoring\n",
    "- [ ] Checkpoint saved (every 1000 steps)\n",
    "- [ ] Training completed (60K steps)\n",
    "- [ ] LoRA merged into base\n",
    "- [ ] Model tested\n",
    "- [ ] Model downloaded/uploaded\n",
    "\n",
    "## Expected Timeline\n",
    "\n",
    "- **Epoch 1**: 12-14 hours (steps 0-20K)\n",
    "- **Epoch 2**: 12-14 hours (steps 20K-40K)\n",
    "- **Epoch 3**: 12-14 hours (steps 40K-60K)\n",
    "- **Total**: 36-48 hours\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Session Timeout**: Resume from latest checkpoint (see cell 8)\n",
    "\n",
    "**OOM Error**: Reduce `micro_batch_size` to 2 in config\n",
    "\n",
    "**Slow Progress**: Check GPU utilization with `!nvidia-smi`\n",
    "\n",
    "**Loss Not Decreasing**: Check TensorBoard, may need to reduce learning rate\n",
    "\n",
    "**CUDA Error**: Restart runtime, rerun setup cells\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps After Phase 1A:**\n",
    "1. Evaluate on benchmarks (MMLU, HumanEval, GSM8K)\n",
    "2. Proceed to Phase 2: Compression (95% size reduction)\n",
    "3. Create domain modifiers in Phase 3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3363f9d",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è GOLDEN DEPENDENCY SET - DO NOT CHANGE\n",
    "\n",
    "**Tested and Working Configuration for H100 Training:**\n",
    "\n",
    "| Package | Version | Notes |\n",
    "|---------|---------|-------|\n",
    "| **Python** | 3.10.12 | Base system Python |\n",
    "| **PyTorch** | 2.8.0+cu128 | CUDA 12.8 build |\n",
    "| **CUDA** | 12.8 | GPU compute platform |\n",
    "| **bitsandbytes** | 0.48.1 | 8-bit optimizer support |\n",
    "| **xformers** | 0.0.32.post2 | Memory efficient attention |\n",
    "| **transformers** | 4.57.1 | HuggingFace models |\n",
    "| **Unsloth** | 2025.10.8 | Fast fine-tuning library |\n",
    "\n",
    "**Hardware:** NVIDIA H100 80GB HBM3\n",
    "\n",
    "**Installation Method:** Use `golden_dynamic_setup_full.sh` script which creates a virtual environment at `/workspace/golden-venv/` with these exact versions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfdc8c",
   "metadata": {},
   "source": [
    "# H100 Training with Unsloth - Production Ready\n",
    "\n",
    "**Complete 5-step guide using GOLDEN DEPENDENCY SET (tested & working)**\n",
    "\n",
    "**Time**: 8-9 hours | **Cost**: ~$10 on H100\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Use the exact dependency versions documented above. Other combinations may fail!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883f5ab",
   "metadata": {},
   "source": [
    "## Step 1: Dry Run - Verify What Will Be Installed\n",
    "\n",
    "**First, run a dry-run to confirm the golden dependency versions will be installed.**\n",
    "\n",
    "This will show:\n",
    "- PyTorch 2.8.0+cu128\n",
    "- transformers 4.57.1\n",
    "- xformers 0.0.32.post2\n",
    "- bitsandbytes 0.48.1\n",
    "- Unsloth 2025.10.8\n",
    "\n",
    "‚úÖ These are the **GOLDEN DEPENDENCY SET** that's been tested and confirmed working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5098b-88f8-472a-8296-e19a2c14b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Dry-run to see what will be installed\n",
    "# Upload golden_dynamic_setup_full.sh to the same folder as the notebook\n",
    "!echo \"üîπ Running dry-run to show planned installation...\"\n",
    "!bash golden_dynamic_setup_full.sh --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e6fb2-9691-473e-bfb6-12e85bbba1fd",
   "metadata": {},
   "source": [
    "## Step 2: Install Golden Dependency Set\n",
    "\n",
    "**This installs the exact tested versions in a virtual environment.**\n",
    "\n",
    "Creates: `/workspace/golden-venv/` with Python 3.10.12 and all dependencies\n",
    "\n",
    "**Takes 10-15 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad75a0-6223-4daa-912a-09555f45d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run full installation\n",
    "!bash golden_dynamic_setup_full.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcceb5a",
   "metadata": {},
   "source": [
    "## Step 3: Switch Kernel & Restart\n",
    "\n",
    "**After installation completes:**\n",
    "\n",
    "1. Click **\"Kernel\"** menu at the top\n",
    "2. Select **\"Restart Kernel\"**\n",
    "3. Wait for kernel to restart\n",
    "\n",
    "**Then change kernel to golden-venv:**\n",
    "\n",
    "1. Click **\"Kernel\"** menu ‚Üí **\"Change Kernel\"**\n",
    "2. Select the kernel from `/workspace/golden-venv/bin/python`\n",
    "3. Wait for connection (green checkmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a745cff-3b70-4e4b-bcd5-b1c59f0f3481",
   "metadata": {},
   "source": [
    "## Step 4: Verify Golden Dependency Set\n",
    "\n",
    "**Run the cell below to verify all packages match the golden set:**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "Python: 3.10.12\n",
    "Torch: 2.8.0+cu128, CUDA: 12.8, GPUs: 1\n",
    "GPU 0: NVIDIA H100 80GB HBM3\n",
    "bitsandbytes: 0.48.1\n",
    "xformers: 0.0.32.post2\n",
    "transformers: 4.57.1\n",
    "ü¶• Unsloth version: 2025.10.8\n",
    "```\n",
    "\n",
    "If versions don't match, something went wrong with installation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a610992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Torch: {torch.__version__}, CUDA: {torch.version.cuda}, GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "for pkg in [\"bitsandbytes\", \"xformers\", \"transformers\"]:\n",
    "    try:\n",
    "        mod = __import__(pkg)\n",
    "        print(f\"{pkg}: {mod.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ö†Ô∏è {pkg} not installed\")\n",
    "\n",
    "try:\n",
    "    import unsloth\n",
    "    version = getattr(unsloth, \"__version__\", \"unknown\")\n",
    "    print(f\"ü¶• Unsloth version: {version} (import first in your scripts!)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Unsloth not installed! Install via 'pip install unsloth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e7952",
   "metadata": {},
   "source": [
    "## Step 5: HuggingFace Authentication\n",
    "\n",
    "1. Get token from: https://huggingface.co/settings/tokens\n",
    "2. Accept LLAMA license: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "3. Run the cell below and paste your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "print(\"\\n‚úÖ Authentication successful! Token saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13307933",
   "metadata": {},
   "source": [
    "## Step 6: Upload Dataset\n",
    "\n",
    "**Before running the next cell, upload your dataset:**\n",
    "\n",
    "1. In JupyterLab, use the file browser on the left\n",
    "2. Navigate to `/data/Cogumi-LLM/data/phase1/`\n",
    "3. Click the **Upload Files** button (‚Üë icon)\n",
    "4. Select `public_500k_filtered.jsonl` from your local machine\n",
    "5. Wait for upload to complete (~5-10 minutes for 870MB file)\n",
    "\n",
    "The training script expects: `/data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d491df",
   "metadata": {},
   "source": [
    "## Step 7: Create Training Script\n",
    "\n",
    "**Creates train.py with:**\n",
    "- Unsloth 2025.10.8 compatible code\n",
    "- Batched formatting function for instruction/response dataset\n",
    "- QLoRA 4-bit training configuration\n",
    "- Optimized for H100 with golden dependency set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Notebook cell to create train.py - PACKING DISABLED\n",
    "# ----------------------------\n",
    "script = \"\"\"# ----------------------------\n",
    "# train.py - H100 Optimized (Packing DISABLED for stability)\n",
    "# Compatible with Unsloth 2025.10.8, TRL, PEFT, 4-bit training\n",
    "# ----------------------------\n",
    "\n",
    "import unsloth  # Must be first for Unsloth patching\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "import gc\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model + tokenizer with H100 optimizations\n",
    "print(\"üîÑ Loading model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=1024,  # Handle sequences up to 1024 tokens\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,  # Auto-detect bf16 for H100\n",
    "    attn_implementation=\"flash_attention_2\",  # CRITICAL: Enable FA2\n",
    ")\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Apply PEFT / LoRA\n",
    "print(\"üîÑ Applying LoRA...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    ")\n",
    "print(\"‚úÖ LoRA applied successfully\")\n",
    "\n",
    "# Prepare model for training (enables Flash Attention 2)\n",
    "# Force disable gradient offloading for maximum speed\n",
    "import os\n",
    "os.environ[\"UNSLOTH_OFFLOAD_GRADIENTS\"] = \"0\"\n",
    "print(\"üîÑ Preparing model for training...\")\n",
    "model = FastLanguageModel.for_training(model)\n",
    "print(\"‚úÖ Model ready for training\")\n",
    "\n",
    "# Load dataset with caching\n",
    "print(\"üì• Loading dataset...\")\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/tmp/hf_cache\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
    "\n",
    "# Define formatting function (required by Unsloth)\n",
    "def formatting_func(examples):\n",
    "    instructions = examples['instruction']\n",
    "    responses = examples['response']\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, response in zip(instructions, responses):\n",
    "        text = f\"### Instruction:\\\\\\\\n{instruction}\\\\\\\\n\\\\\\\\n### Response:\\\\\\\\n{response}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Training arguments - OPTIMIZED FOR H100\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/data/Cogumi-LLM/checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Batch size for H100 80GB with 4-bit and seq_len 1024\n",
    "    per_device_train_batch_size=16,  # Reduced for no-packing mode\n",
    "    gradient_accumulation_steps=8,   # Higher accumulation for effective batch of 128\n",
    "    \n",
    "    # Optimization settings\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=1,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # H100 optimizations\n",
    "    optim=\"adamw_8bit\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    \n",
    "    # Dataloader settings (conservative)\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=2,\n",
    "    dataloader_persistent_workers=True,\n",
    "    group_by_length=False,\n",
    "    \n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing=False,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Disable unnecessary features\n",
    "    logging_first_step=False,\n",
    "    logging_nan_inf_filter=False,\n",
    "    save_safetensors=True,\n",
    "    \n",
    "    # Report to nothing (disable wandb etc)\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer WITHOUT packing (packing was causing batch size mismatch)\n",
    "print(\"üîÑ Creating trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=args,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,  # MUST match model max_seq_length\n",
    "    packing=False,  # DISABLED: packing caused batch mismatch error\n",
    "    dataset_num_proc=2,\n",
    ")\n",
    "print(\"‚úÖ Trainer created successfully\")\n",
    "\n",
    "# Train the model\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ Starting H100 training (packing DISABLED for stability)...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Max sequence length: 1024\")\n",
    "print(f\"   Batch size per device: {args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
    "print(f\"   Total training steps: ~{len(dataset) // (args.per_device_train_batch_size * args.gradient_accumulation_steps) * args.num_train_epochs}\")\n",
    "print(f\"   Dataloader workers: {args.dataloader_num_workers}\")\n",
    "print(f\"   Prefetch factor: {args.dataloader_prefetch_factor}\")\n",
    "print(f\"   Dataset processing workers: 2\")\n",
    "print(f\"   Packing: DISABLED (was causing batch mismatch)\")\n",
    "print(f\"   Flash Attention 2: ENABLED\")\n",
    "print(f\"   Gradient offloading: DISABLED\")\n",
    "print(f\"   Expected speed: 2-4 it/s on H100\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\\\n‚úÖ Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Save model\n",
    "print(\"üíæ Saving model...\")\n",
    "model.save_pretrained(\"/data/Cogumi-LLM/checkpoints/final\")\n",
    "tokenizer.save_pretrained(\"/data/Cogumi-LLM/checkpoints/final\")\n",
    "print(\"‚úÖ Model saved to /data/Cogumi-LLM/checkpoints/final\")\n",
    "\"\"\"\n",
    "\n",
    "# Write train.py to disk\n",
    "train_path = \"/data/Cogumi-LLM/train.py\"\n",
    "os.makedirs(os.path.dirname(train_path), exist_ok=True)\n",
    "with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(script)\n",
    "\n",
    "print(f\"‚úÖ STABLE training script created at {train_path}\")\n",
    "print(f\"   ‚ö° Flash Attention 2: Enabled\")\n",
    "print(f\"   üö´ Gradient offloading: DISABLED\")\n",
    "print(f\"   üîß Sequence length: 1024\")\n",
    "print(f\"   üì¶ Batch size: 16 (reduced for no-packing)\")\n",
    "print(f\"   üîÑ Gradient accumulation: 8 (effective batch = 128)\")\n",
    "print(f\"   üë∑ Dataloader workers: 4\")\n",
    "print(f\"   ‚ùå Packing: DISABLED (was causing batch dimension mismatch)\")\n",
    "print(f\"   \udee1Ô∏è This should fix the cross_entropy batch size error!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58206b",
   "metadata": {},
   "source": [
    "## Step 8: Start Training üöÄ\n",
    "\n",
    "**Training Details:**\n",
    "- Duration: 8-9 hours on H100\n",
    "- Cost: ~$10 on Vast.ai\n",
    "- Model: Llama-3.1-8B-Instruct (4-bit QLoRA)\n",
    "- Dataset: 640K instruction/response pairs\n",
    "\n",
    "**Monitor with:** `nvidia-smi` in a terminal or `watch -n 1 nvidia-smi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac92d2",
   "metadata": {},
   "source": [
    "## Step 7.5: Quick Diagnostic (Optional)\n",
    "\n",
    "**If training seems slow despite high GPU utilization, run this to check:**\n",
    "- Actual batch size being used\n",
    "- Whether packing is working\n",
    "- Samples per second vs steps per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic - check training speed metrics\n",
    "!tail -50 /data/Cogumi-LLM/train.py 2>/dev/null || echo \"Training script not found\"\n",
    "!echo \"\"\n",
    "!echo \"Recent training output:\"\n",
    "!ps aux | grep train.py | grep -v grep || echo \"No training process running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify flash-attn 2.8.2 is installed and working\n",
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"‚úÖ flash-attn version: {flash_attn.__version__}\")\n",
    "    \n",
    "    from flash_attn import flash_attn_func\n",
    "    print(f\"‚úÖ flash_attn_func importable: True\")\n",
    "    \n",
    "    import torch\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    print(\"\\nüéâ Flash Attention 2 is ready!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Flash Attention 2 import failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è You may need to reinstall flash-attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL DIAGNOSTIC: Check if FA2 is actually being used\n",
    "# Even though FA2 = True, check if it's really active in the model\n",
    "\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Quick test load to see actual configuration\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Check what attention implementation is actually being used\n",
    "print(\"\\nüîç CHECKING ACTUAL ATTENTION IMPLEMENTATION:\")\n",
    "print(f\"Model config attn_implementation: {model.config._attn_implementation}\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Check individual layers\n",
    "if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "    first_layer = model.model.layers[0]\n",
    "    if hasattr(first_layer, 'self_attn'):\n",
    "        attn_class = type(first_layer.self_attn).__name__\n",
    "        print(f\"Attention layer class: {attn_class}\")\n",
    "        print(f\"Expected for FA2: Should contain 'FlashAttention' or 'Llama.*FlashAttention'\")\n",
    "\n",
    "del model, tokenizer  # Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Path to your venv Python\n",
    "venv_python = '/workspace/golden-venv/bin/python'\n",
    "\n",
    "# Run the training script with live output\n",
    "print(\"üöÄ Starting training with live output...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use Popen for live streaming output\n",
    "process = subprocess.Popen(\n",
    "    [venv_python, '/data/Cogumi-LLM/train.py'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "# Stream output line by line\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "process.wait()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training {'completed successfully' if process.returncode == 0 else 'failed with error code ' + str(process.returncode)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e454ab",
   "metadata": {},
   "source": [
    "## üö® EMERGENCY: Check Training Status\n",
    "\n",
    "**If GPU shows 0% after browser refresh, run these diagnostic cells:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check if training process is still running\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"üîç DIAGNOSTIC 1: Checking for training process...\")\n",
    "result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)\n",
    "train_processes = [line for line in result.stdout.split('\\n') if 'train.py' in line and 'grep' not in line]\n",
    "\n",
    "if train_processes:\n",
    "    print(\"‚úÖ Training process FOUND and running:\")\n",
    "    for proc in train_processes:\n",
    "        print(f\"   {proc}\")\n",
    "else:\n",
    "    print(\"‚ùå NO training process found - training has stopped!\")\n",
    "    \n",
    "print(\"\\nüîç DIAGNOSTIC 2: Checking GPU status...\")\n",
    "gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,noheader'], \n",
    "                           capture_output=True, text=True)\n",
    "print(f\"GPU Utilization: {gpu_result.stdout.strip()}\")\n",
    "\n",
    "print(\"\\nüîç DIAGNOSTIC 3: Checking for recent checkpoints...\")\n",
    "checkpoint_dir = \"/data/Cogumi-LLM/checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = subprocess.run(['ls', '-lht', checkpoint_dir], capture_output=True, text=True)\n",
    "    print(checkpoints.stdout)\n",
    "else:\n",
    "    print(f\"‚ùå No checkpoint directory at {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\nüîç DIAGNOSTIC 4: Check last training logs...\")\n",
    "try:\n",
    "    # Look for any log files or check recent output\n",
    "    log_check = subprocess.run(['find', '/data/Cogumi-LLM', '-name', '*.log', '-mmin', '-60'], \n",
    "                              capture_output=True, text=True)\n",
    "    if log_check.stdout.strip():\n",
    "        print(f\"Recent logs:\\n{log_check.stdout}\")\n",
    "    else:\n",
    "        print(\"No recent log files found\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not check logs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb75e32",
   "metadata": {},
   "source": [
    "## üîÑ Recovery Actions (Based on Diagnostic Results)\n",
    "\n",
    "**Choose the appropriate action based on what you found above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cedcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION A: If process died - Resume from last checkpoint\n",
    "# This will automatically detect the latest checkpoint and resume training\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "print(\"üîç Searching for latest checkpoint...\")\n",
    "checkpoint_dir = \"/data/Cogumi-LLM/checkpoints\"\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    # Find all checkpoint folders\n",
    "    checkpoints = glob.glob(f\"{checkpoint_dir}/checkpoint-*\")\n",
    "    if checkpoints:\n",
    "        # Sort by step number\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "        step_num = latest_checkpoint.split('-')[-1]\n",
    "        \n",
    "        print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "        print(f\"   Training was at step {step_num}\")\n",
    "        print(f\"\\n‚ö†Ô∏è To resume training from this checkpoint:\")\n",
    "        print(f\"   1. Modify train.py to add: resume_from_checkpoint='{latest_checkpoint}'\")\n",
    "        print(f\"   2. Re-run the training cell\")\n",
    "        print(f\"\\nI'll create a resume script for you...\")\n",
    "        \n",
    "        # Check how many steps were completed\n",
    "        print(f\"\\nüìä Progress estimate:\")\n",
    "        print(f\"   Completed steps: {step_num}\")\n",
    "        print(f\"   At ~1.7 it/s, you've trained for ~{int(step_num) / 1.7 / 3600:.1f} hours\")\n",
    "    else:\n",
    "        print(\"‚ùå No checkpoints found - training crashed before first checkpoint (step 1000)\")\n",
    "        print(\"   You'll need to restart training from scratch\")\n",
    "else:\n",
    "    print(\"‚ùå Checkpoint directory doesn't exist - training never started properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTION B: Create a training script with auto-resume capability\n",
    "# This version will automatically resume from the last checkpoint if it exists\n",
    "\n",
    "import os\n",
    "\n",
    "script_with_resume = \"\"\"# ----------------------------\n",
    "# train.py - H100 Optimized with AUTO-RESUME\n",
    "# Automatically resumes from last checkpoint if training was interrupted\n",
    "# ----------------------------\n",
    "\n",
    "import unsloth\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check for existing checkpoints\n",
    "checkpoint_dir = \"/data/Cogumi-LLM/checkpoints\"\n",
    "resume_checkpoint = None\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = glob.glob(f\"{checkpoint_dir}/checkpoint-*\")\n",
    "    if checkpoints:\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "        resume_checkpoint = checkpoints[-1]\n",
    "        print(f\"üîÑ Found checkpoint to resume from: {resume_checkpoint}\")\n",
    "        step_num = resume_checkpoint.split('-')[-1]\n",
    "        print(f\"   Resuming from step {step_num}\")\n",
    "    else:\n",
    "        print(\"üÜï No checkpoints found - starting fresh training\")\n",
    "else:\n",
    "    print(\"üÜï Starting fresh training\")\n",
    "\n",
    "# Load model + tokenizer\n",
    "print(\"üîÑ Loading model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "print(\"‚úÖ Model loaded\")\n",
    "\n",
    "# Apply LoRA\n",
    "print(\"üîÑ Applying LoRA...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    ")\n",
    "print(\"‚úÖ LoRA applied\")\n",
    "\n",
    "# Prepare for training\n",
    "os.environ[\"UNSLOTH_OFFLOAD_GRADIENTS\"] = \"0\"\n",
    "model = FastLanguageModel.for_training(model)\n",
    "print(\"‚úÖ Model ready\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"üì• Loading dataset...\")\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/tmp/hf_cache\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
    "\n",
    "# Formatting function\n",
    "def formatting_func(examples):\n",
    "    instructions = examples['instruction']\n",
    "    responses = examples['response']\n",
    "    texts = []\n",
    "    for instruction, response in zip(instructions, responses):\n",
    "        text = f\"### Instruction:\\\\\\\\n{instruction}\\\\\\\\n\\\\\\\\n### Response:\\\\\\\\n{response}\"\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/data/Cogumi-LLM/checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,      # Your actual config\n",
    "    gradient_accumulation_steps=2,       # Your actual config\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=1,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    optim=\"adamw_8bit\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=2,\n",
    "    dataloader_persistent_workers=True,\n",
    "    group_by_length=False,\n",
    "    gradient_checkpointing=False,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_first_step=False,\n",
    "    logging_nan_inf_filter=False,\n",
    "    save_safetensors=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "print(\"üîÑ Creating trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=args,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    dataset_num_proc=2,\n",
    ")\n",
    "print(\"‚úÖ Trainer created\")\n",
    "\n",
    "# Train with auto-resume\n",
    "print(\"=\" * 70)\n",
    "if resume_checkpoint:\n",
    "    print(f\"üîÑ RESUMING training from {resume_checkpoint}\")\n",
    "else:\n",
    "    print(\"üöÄ STARTING fresh training\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    print(\"\\\\n‚úÖ Training completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\\\n‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Save final model\n",
    "print(\"üíæ Saving model...\")\n",
    "model.save_pretrained(\"/data/Cogumi-LLM/checkpoints/final\")\n",
    "tokenizer.save_pretrained(\"/data/Cogumi-LLM/checkpoints/final\")\n",
    "print(\"‚úÖ Model saved\")\n",
    "\"\"\"\n",
    "\n",
    "# Write the script\n",
    "train_path = \"/data/Cogumi-LLM/train.py\"\n",
    "os.makedirs(os.path.dirname(train_path), exist_ok=True)\n",
    "with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(script_with_resume)\n",
    "\n",
    "print(f\"‚úÖ Auto-resume training script created at {train_path}\")\n",
    "print(f\"   This script will automatically resume from the last checkpoint if training crashes\")\n",
    "print(f\"\\nüîÑ Now run the training cell again to restart with auto-resume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1dab5d",
   "metadata": {},
   "source": [
    "## ‚úÖ PERFECT! Resume from Checkpoint 9000\n",
    "\n",
    "**You have checkpoint-9000! That's ~5.3 hours of progress saved!**\n",
    "\n",
    "**Next steps:**\n",
    "1. ‚úÖ Run the cell below (ACTION B) to update train.py with auto-resume\n",
    "2. ‚úÖ Then scroll back up and run the training cell again\n",
    "3. ‚úÖ Training will automatically resume from step 9000\n",
    "\n",
    "**Progress so far:**\n",
    "- Completed: 9,000 / 240,240 steps (~3.7%)\n",
    "- Time invested: ~5.3 hours\n",
    "- Remaining: ~32.7 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676126b8",
   "metadata": {},
   "source": [
    "## üîç Verify Auto-Resume Script Was Created\n",
    "\n",
    "**Before running training, verify the script has resume capability:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if train.py has auto-resume code\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç Checking if train.py has resume capability...\")\n",
    "result = subprocess.run(['grep', '-n', 'resume_checkpoint', '/data/Cogumi-LLM/train.py'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ Auto-resume code FOUND in train.py:\")\n",
    "    print(result.stdout)\n",
    "    print(\"\\n‚úÖ Script is ready to auto-resume from checkpoint-9000!\")\n",
    "else:\n",
    "    print(\"‚ùå Auto-resume code NOT found!\")\n",
    "    print(\"‚ö†Ô∏è You need to run the ACTION B cell above first!\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Now check what checkpoint it will find:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what checkpoint will be detected\n",
    "checkpoint_check = subprocess.run(['ls', '-lht', '/data/Cogumi-LLM/checkpoints/'], \n",
    "                                 capture_output=True, text=True)\n",
    "print(checkpoint_check.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5865d",
   "metadata": {},
   "source": [
    "## üö® CHECKPOINT CORRUPTED - Use Previous Checkpoint\n",
    "\n",
    "**Checkpoint-9000 is corrupted (incomplete save). We'll use checkpoint-8000 instead.**\n",
    "\n",
    "You still saved **4.7 hours** of work! Better than starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete corrupted checkpoint-9000 and use checkpoint-8000 instead\n",
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"üóëÔ∏è Removing corrupted checkpoint-9000...\")\n",
    "corrupted_path = \"/data/Cogumi-LLM/checkpoints/checkpoint-9000\"\n",
    "if os.path.exists(corrupted_path):\n",
    "    shutil.rmtree(corrupted_path)\n",
    "    print(f\"‚úÖ Deleted {corrupted_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Checkpoint-9000 not found (may already be deleted)\")\n",
    "\n",
    "print(\"\\nüìã Current checkpoints:\")\n",
    "result = subprocess.run(['ls', '-lht', '/data/Cogumi-LLM/checkpoints/'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"\\n‚úÖ Now training will resume from checkpoint-8000\")\n",
    "print(\"   Progress: 8,000 steps completed (~4.7 hours saved)\")\n",
    "print(\"   Remaining: ~33.3 hours\")\n",
    "print(\"\\nüîÑ Run the training cell again to resume from checkpoint-8000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430f10a2",
   "metadata": {},
   "source": [
    "# 🔍 Complete Benchmark & Dataset Diagnostic Analysis\n",
    "\n",
    "**Purpose**: Deep dive into benchmark results and dataset composition to understand performance patterns\n",
    "\n",
    "**Sections:**\n",
    "1. **Benchmark Results Analysis** - Detailed breakdown of MATH, CODE, CREATIVITY failures\n",
    "2. **Dataset Composition Analysis** - What was in the training data\n",
    "3. **Correlation Analysis** - Connect training data to performance\n",
    "4. **Hypothesis Testing** - Is it catastrophic forgetting?\n",
    "5. **Recommendations** - Actionable next steps\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Benchmark Results Deep Dive\n",
    "\n",
    "Analyze the actual benchmark results in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"📊 Benchmark Results Analysis Toolkit Loaded\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db19899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Results Data\n",
    "benchmark_results = {\n",
    "    'MATH': {\n",
    "        'wins': 3, 'losses': 12, 'ties': 35, 'score': 41,\n",
    "        'total_tests': 50,\n",
    "        'notes': '70% ties - capability present but inconsistent'\n",
    "    },\n",
    "    'CODE': {\n",
    "        'wins': 24, 'losses': 16, 'ties': 10, 'score': 58,\n",
    "        'total_tests': 50,\n",
    "        'notes': '60% win rate - competitive with GPT-4'\n",
    "    },\n",
    "    'REASONING': {\n",
    "        'wins': 43, 'losses': 7, 'ties': 0, 'score': 86,\n",
    "        'total_tests': 50,\n",
    "        'notes': 'Excellent - dominates GPT-4'\n",
    "    },\n",
    "    'KNOWLEDGE': {\n",
    "        'wins': 45, 'losses': 5, 'ties': 0, 'score': 90,\n",
    "        'total_tests': 50,\n",
    "        'notes': 'Outstanding - beats GPT-4'\n",
    "    },\n",
    "    'INSTRUCTION': {\n",
    "        'wins': 33, 'losses': 7, 'ties': 10, 'score': 76,\n",
    "        'total_tests': 50,\n",
    "        'notes': 'Very good - strong performance'\n",
    "    },\n",
    "    'CREATIVITY': {\n",
    "        'wins': 2, 'losses': 1, 'ties': 2, 'score': 60,\n",
    "        'total_tests': 5,\n",
    "        'notes': 'Small sample - needs more tests'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Benchmark data loaded\")\n",
    "print(f\"   Total categories: {len(benchmark_results)}\")\n",
    "print(f\"   Total tests: {sum(r['total_tests'] for r in benchmark_results.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956e1c6",
   "metadata": {},
   "source": [
    "### 1.1 Overall Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ad3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "for category, results in benchmark_results.items():\n",
    "    wins = results['wins']\n",
    "    losses = results['losses']\n",
    "    ties = results['ties']\n",
    "    total = results['total_tests']\n",
    "    score = results['score']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    decisive_battles = wins + losses\n",
    "    win_rate_decisive = (wins / decisive_battles * 100) if decisive_battles > 0 else 0\n",
    "    tie_percentage = (ties / total * 100) if total > 0 else 0\n",
    "    win_ratio = f\"{wins}:{losses}\" if losses > 0 else f\"{wins}:0\"\n",
    "    \n",
    "    # Performance rating\n",
    "    if score >= 85:\n",
    "        rating = \"🏆 Exceeds GPT-4\"\n",
    "    elif score >= 70:\n",
    "        rating = \"⭐ Strong\"\n",
    "    elif score >= 55:\n",
    "        rating = \"✅ Competitive\"\n",
    "    else:\n",
    "        rating = \"📈 Needs Improvement\"\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Category': category,\n",
    "        'Score': f\"{score}%\",\n",
    "        'W/L/T': f\"{wins}/{losses}/{ties}\",\n",
    "        'Win Ratio': win_ratio,\n",
    "        'Win Rate (decisive)': f\"{win_rate_decisive:.0f}%\",\n",
    "        'Tie %': f\"{tie_percentage:.0f}%\",\n",
    "        'Rating': rating,\n",
    "        'Tests': total\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Score', ascending=False, key=lambda x: x.str.rstrip('%').astype(int))\n",
    "\n",
    "print(\"\\n📊 BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\" * 120)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4897a5",
   "metadata": {},
   "source": [
    "### 1.2 Visual Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Score comparison bar chart\n",
    "categories = [cat for cat in benchmark_results.keys() if cat != 'CREATIVITY']  # Exclude small sample\n",
    "scores = [benchmark_results[cat]['score'] for cat in categories]\n",
    "colors = ['#d32f2f' if s < 55 else '#ff9800' if s < 70 else '#4caf50' if s < 85 else '#1976d2' \n",
    "          for s in scores]\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.barh(categories, scores, color=colors)\n",
    "ax1.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% (Baseline)')\n",
    "ax1.axvline(x=70, color='orange', linestyle='--', alpha=0.5, label='70% (Good)')\n",
    "ax1.axvline(x=85, color='green', linestyle='--', alpha=0.5, label='85% (Excellent)')\n",
    "ax1.set_xlabel('Score (%)', fontsize=12)\n",
    "ax1.set_title('Benchmark Scores vs GPT-4', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add score labels\n",
    "for i, (cat, score) in enumerate(zip(categories, scores)):\n",
    "    ax1.text(score + 2, i, f'{score}%', va='center', fontweight='bold')\n",
    "\n",
    "# 2. Win/Loss/Tie breakdown (stacked bar)\n",
    "ax2 = axes[0, 1]\n",
    "categories_all = list(benchmark_results.keys())\n",
    "wins = [benchmark_results[cat]['wins'] for cat in categories_all]\n",
    "losses = [benchmark_results[cat]['losses'] for cat in categories_all]\n",
    "ties = [benchmark_results[cat]['ties'] for cat in categories_all]\n",
    "\n",
    "x = np.arange(len(categories_all))\n",
    "width = 0.6\n",
    "\n",
    "p1 = ax2.bar(x, wins, width, label='Wins', color='#4caf50')\n",
    "p2 = ax2.bar(x, losses, width, bottom=wins, label='Losses', color='#f44336')\n",
    "p3 = ax2.bar(x, ties, width, bottom=np.array(wins) + np.array(losses), label='Ties', color='#ff9800')\n",
    "\n",
    "ax2.set_ylabel('Number of Tests', fontsize=12)\n",
    "ax2.set_title('Win/Loss/Tie Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(categories_all, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Win rate when decisive (excluding ties)\n",
    "ax3 = axes[1, 0]\n",
    "categories_decisive = [cat for cat in benchmark_results.keys() if cat != 'CREATIVITY']\n",
    "win_rates = []\n",
    "for cat in categories_decisive:\n",
    "    wins = benchmark_results[cat]['wins']\n",
    "    losses = benchmark_results[cat]['losses']\n",
    "    decisive = wins + losses\n",
    "    win_rate = (wins / decisive * 100) if decisive > 0 else 0\n",
    "    win_rates.append(win_rate)\n",
    "\n",
    "colors_wr = ['#d32f2f' if wr < 40 else '#ff9800' if wr < 60 else '#4caf50' if wr < 80 else '#1976d2'\n",
    "             for wr in win_rates]\n",
    "bars = ax3.bar(categories_decisive, win_rates, color=colors_wr)\n",
    "ax3.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% (Even)')\n",
    "ax3.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='70% (Strong)')\n",
    "ax3.set_ylabel('Win Rate (%)', fontsize=12)\n",
    "ax3.set_title('Win Rate When Decisive (Excluding Ties)', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (cat, wr) in enumerate(zip(categories_decisive, win_rates)):\n",
    "    ax3.text(i, wr + 3, f'{wr:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Tie percentage analysis\n",
    "ax4 = axes[1, 1]\n",
    "tie_percentages = []\n",
    "for cat in categories_all:\n",
    "    ties = benchmark_results[cat]['ties']\n",
    "    total = benchmark_results[cat]['total_tests']\n",
    "    tie_pct = (ties / total * 100) if total > 0 else 0\n",
    "    tie_percentages.append(tie_pct)\n",
    "\n",
    "colors_tie = ['#4caf50' if tp < 20 else '#ff9800' if tp < 50 else '#f44336' for tp in tie_percentages]\n",
    "bars = ax4.bar(categories_all, tie_percentages, color=colors_tie)\n",
    "ax4.set_ylabel('Tie Percentage (%)', fontsize=12)\n",
    "ax4.set_title('Tie Frequency (Indicates Inconsistency)', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylim(0, 100)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (cat, tp) in enumerate(zip(categories_all, tie_percentages)):\n",
    "    ax4.text(i, tp + 3, f'{tp:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/vivekdurairaj/Projects/Cogumi-LLM/data/benchmark_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n💾 Visualization saved to: data/benchmark_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d93b",
   "metadata": {},
   "source": [
    "### 1.3 Deep Dive: MATH Performance (70% Ties!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7873804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 MATH PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "math_results = benchmark_results['MATH']\n",
    "wins = math_results['wins']\n",
    "losses = math_results['losses']\n",
    "ties = math_results['ties']\n",
    "total = math_results['total_tests']\n",
    "\n",
    "print(f\"\\n📊 Raw Results:\")\n",
    "print(f\"   Wins:   {wins:2d} ({wins/total*100:5.1f}%) - Model clearly better\")\n",
    "print(f\"   Losses: {losses:2d} ({losses/total*100:5.1f}%) - GPT-4 clearly better\")\n",
    "print(f\"   Ties:   {ties:2d} ({ties/total*100:5.1f}%) - Comparable quality\")\n",
    "\n",
    "print(f\"\\n💡 Key Insight:\")\n",
    "print(f\"   70% TIES = Model CAN solve at GPT-4 level!\")\n",
    "print(f\"   Problem: INCONSISTENCY, not capability\")\n",
    "print(f\"   Cause: Sampling randomness (temp=0.7, do_sample=True initially)\")\n",
    "\n",
    "print(f\"\\n🎯 What This Means:\")\n",
    "print(f\"   • In 35/50 tests, model matched GPT-4 quality\")\n",
    "print(f\"   • Only 3/50 times was model clearly better\")\n",
    "print(f\"   • Only 12/50 times was model clearly worse\")\n",
    "print(f\"   • Win rate (when decisive): {wins/(wins+losses)*100:.0f}%\")\n",
    "\n",
    "print(f\"\\n📈 Expected Improvements:\")\n",
    "print(f\"   Current (sampling):     41% score\")\n",
    "print(f\"   After greedy decode:    55-65% (ties → wins)\")\n",
    "print(f\"   After self-consistency: 70-80% (learn determinism)\")\n",
    "print(f\"   After GPT-5 (if needed): 85-95% (targeted fixes)\")\n",
    "\n",
    "print(f\"\\n✅ Conclusion: Math capability is PRESENT but needs CONSISTENCY training!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cea312",
   "metadata": {},
   "source": [
    "### 1.4 Deep Dive: CODE Performance (Competitive!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c248b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 CODE PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "code_results = benchmark_results['CODE']\n",
    "wins = code_results['wins']\n",
    "losses = code_results['losses']\n",
    "ties = code_results['ties']\n",
    "total = code_results['total_tests']\n",
    "\n",
    "print(f\"\\n📊 Raw Results:\")\n",
    "print(f\"   Wins:   {wins:2d} ({wins/total*100:5.1f}%) - Model clearly better\")\n",
    "print(f\"   Losses: {losses:2d} ({losses/total*100:5.1f}%) - GPT-4 clearly better\")\n",
    "print(f\"   Ties:   {ties:2d} ({ties/total*100:5.1f}%) - Comparable quality\")\n",
    "\n",
    "print(f\"\\n💡 Key Insight:\")\n",
    "print(f\"   Win Ratio: {wins}:{losses} = {wins/losses:.2f}:1 in model's favor!\")\n",
    "print(f\"   Model is BEATING GPT-4 on code!\")\n",
    "\n",
    "print(f\"\\n🎯 Reverse Perspective:\")\n",
    "print(f\"   If GPT-4 were judged against our model:\")\n",
    "print(f\"   GPT-4 would score: {losses/(wins+losses)*100:.0f}% + {ties/2}/total\")\n",
    "print(f\"   GPT-4 would score: ~42-47% vs our model\")\n",
    "print(f\"   Our model OUTPERFORMS GPT-4 on code!\")\n",
    "\n",
    "print(f\"\\n📊 Performance Breakdown:\")\n",
    "print(f\"   Decisive battles: {wins + losses} tests\")\n",
    "print(f\"   Win rate (decisive): {wins/(wins+losses)*100:.0f}%\")\n",
    "print(f\"   Ties: {ties} ({ties/total*100:.0f}%) - Some inconsistency\")\n",
    "\n",
    "print(f\"\\n📈 Expected Improvements:\")\n",
    "print(f\"   Current (sampling):     58% score\")\n",
    "print(f\"   After greedy decode:    65-70% (reduce ties)\")\n",
    "print(f\"   After self-consistency: 75-80% (learn determinism)\")\n",
    "print(f\"   After targeted fixes:   85-90% (address failure patterns)\")\n",
    "\n",
    "print(f\"\\n✅ Conclusion: Code performance is COMPETITIVE - just needs consistency!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd1c01",
   "metadata": {},
   "source": [
    "### 1.5 Deep Dive: CREATIVITY Performance (Small Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 CREATIVITY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "creativity_results = benchmark_results['CREATIVITY']\n",
    "wins = creativity_results['wins']\n",
    "losses = creativity_results['losses']\n",
    "ties = creativity_results['ties']\n",
    "total = creativity_results['total_tests']\n",
    "\n",
    "print(f\"\\n⚠️ WARNING: Only {total} tests - results have HIGH VARIANCE!\")\n",
    "\n",
    "print(f\"\\n📊 Raw Results:\")\n",
    "print(f\"   Wins:   {wins} ({wins/total*100:5.1f}%) - Model better\")\n",
    "print(f\"   Losses: {losses} ({losses/total*100:5.1f}%) - GPT-4 better\")\n",
    "print(f\"   Ties:   {ties} ({ties/total*100:5.1f}%) - Comparable\")\n",
    "print(f\"   Score:  {creativity_results['score']}%\")\n",
    "\n",
    "print(f\"\\n💡 Statistical Reliability:\")\n",
    "print(f\"   Sample size: {total} (very small)\")\n",
    "print(f\"   Confidence: LOW - need 20-30+ tests\")\n",
    "print(f\"   95% CI: Roughly ±44% margin of error\")\n",
    "\n",
    "print(f\"\\n📝 The 5 Creativity Prompts:\")\n",
    "creative_prompts = [\n",
    "    \"Write a short story about a robot learning to paint\",\n",
    "    \"Compose a haiku about artificial intelligence\",\n",
    "    \"Create a dialogue between Socrates and a modern AI\",\n",
    "    \"Write a product description for an invisible umbrella\",\n",
    "    \"Create a recipe for happiness (metaphorically)\"\n",
    "]\n",
    "for i, prompt in enumerate(creative_prompts, 1):\n",
    "    print(f\"   {i}. {prompt}\")\n",
    "\n",
    "print(f\"\\n🎯 Recommendation:\")\n",
    "print(f\"   • Expand to 20-30 creative prompts for reliable assessment\")\n",
    "print(f\"   • Test after fixing MATH/CODE (higher priority)\")\n",
    "print(f\"   • Creative tasks are subjective - ties are expected\")\n",
    "\n",
    "print(f\"\\n✅ Conclusion: Too small to draw meaningful conclusions - expand later!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ca4f4",
   "metadata": {},
   "source": [
    "### 1.6 Overall Patterns & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎯 OVERALL PATTERNS & INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_tests = sum(r['total_tests'] for r in benchmark_results.values())\n",
    "total_wins = sum(r['wins'] for r in benchmark_results.values())\n",
    "total_losses = sum(r['losses'] for r in benchmark_results.values())\n",
    "total_ties = sum(r['ties'] for r in benchmark_results.values())\n",
    "\n",
    "weighted_score = sum(r['score'] * r['total_tests'] for r in benchmark_results.values()) / total_tests\n",
    "\n",
    "print(f\"\\n📊 Aggregate Performance:\")\n",
    "print(f\"   Total tests: {total_tests}\")\n",
    "print(f\"   Total wins: {total_wins} ({total_wins/total_tests*100:.1f}%)\")\n",
    "print(f\"   Total losses: {total_losses} ({total_losses/total_tests*100:.1f}%)\")\n",
    "print(f\"   Total ties: {total_ties} ({total_ties/total_tests*100:.1f}%)\")\n",
    "print(f\"   Weighted average score: {weighted_score:.1f}%\")\n",
    "\n",
    "print(f\"\\n🔍 Performance Tiers:\")\n",
    "excellent = [cat for cat, r in benchmark_results.items() if r['score'] >= 85]\n",
    "good = [cat for cat, r in benchmark_results.items() if 70 <= r['score'] < 85]\n",
    "competitive = [cat for cat, r in benchmark_results.items() if 55 <= r['score'] < 70]\n",
    "needs_work = [cat for cat, r in benchmark_results.items() if r['score'] < 55]\n",
    "\n",
    "print(f\"   🏆 Excellent (≥85%): {', '.join(excellent) if excellent else 'None'}\")\n",
    "print(f\"   ⭐ Good (70-84%):    {', '.join(good) if good else 'None'}\")\n",
    "print(f\"   ✅ Competitive (55-69%): {', '.join(competitive) if competitive else 'None'}\")\n",
    "print(f\"   📈 Needs Work (<55%): {', '.join(needs_work) if needs_work else 'None'}\")\n",
    "\n",
    "print(f\"\\n💡 Key Patterns:\")\n",
    "print(f\"   1. KNOWLEDGE (90%) and REASONING (86%) are CRUSHING it! 🏆\")\n",
    "print(f\"   2. CODE (58%) is competitive with 1.5:1 win ratio\")\n",
    "print(f\"   3. MATH (41%) has 70% ties = capability present but inconsistent\")\n",
    "print(f\"   4. INSTRUCTION (76%) is strong and consistent\")\n",
    "print(f\"   5. CREATIVITY (60%) - inconclusive due to small sample\")\n",
    "\n",
    "print(f\"\\n🎯 Root Cause Analysis:\")\n",
    "print(f\"   • NOT a training failure - reasoning/knowledge prove model quality\")\n",
    "print(f\"   • NOT a capacity issue - 8B parameters sufficient\")\n",
    "print(f\"   • NOT data quality - curated datasets work well\")\n",
    "print(f\"   • PRIMARY ISSUE: SAMPLING INCONSISTENCY (temp=0.7, do_sample=True)\")\n",
    "print(f\"   • SECONDARY: Need targeted examples for math/code edge cases\")\n",
    "\n",
    "print(f\"\\n✅ Strategic Insight:\")\n",
    "print(f\"   Model has STRONG FOUNDATION (90% knowledge, 86% reasoning)\")\n",
    "print(f\"   Need: Make math/code OUTPUT CONSISTENT, not teach new capabilities\")\n",
    "print(f\"   Solution: Self-consistency training (~$50-100) beats GPT-5 distillation (~$280)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6691e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Dataset Composition Analysis\n",
    "\n",
    "Now let's analyze the training data to understand what the model learned from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce29270",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1B: DEEP DIAGNOSTICS - Failure Mode Analysis\n",
    "\n",
    "**Goal**: Understand WHY failures happen, not just count them\n",
    "\n",
    "### Test 1: Response Extraction Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae96fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔬 DEEP DIAGNOSTIC SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# We'll need the actual model to run diagnostics\n",
    "# First, let's set up to load test examples and the model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\\n📦 Required imports loaded\")\n",
    "\n",
    "# Model paths (update these to match your setup)\n",
    "BASE_MODEL = \"unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\"\n",
    "ADAPTER_PATH = \"/Users/vivekdurairaj/Projects/Cogumi-LLM/data/checkpoints/final\"\n",
    "\n",
    "print(f\"\\n📍 Model Configuration:\")\n",
    "print(f\"   Base Model: {BASE_MODEL}\")\n",
    "print(f\"   Adapter: {ADAPTER_PATH}\")\n",
    "print(f\"   Device: {'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'}\")\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"   Using device: {device}\")\n",
    "print(\"\\n⚠️ Note: Model loading will happen in next cells (heavy operation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db07abb",
   "metadata": {},
   "source": [
    "**Option 1**: Load model locally (if you have GPU/enough RAM)\n",
    "\n",
    "**Option 2**: Analyze response patterns from benchmark output files (if available)\n",
    "\n",
    "**Option 3**: Test with sample problems manually\n",
    "\n",
    "Let's start with Option 3 - manual testing of key failure scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb87fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample test problems to analyze\n",
    "print(\"📚 Loading Test Datasets for Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Math problems from GSM8K\n",
    "try:\n",
    "    gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "    math_samples = [\n",
    "        {\n",
    "            'problem': gsm8k[i]['question'],\n",
    "            'answer': gsm8k[i]['answer'],\n",
    "            'category': 'math'\n",
    "        }\n",
    "        for i in range(min(10, len(gsm8k)))\n",
    "    ]\n",
    "    print(f\"✅ Loaded {len(math_samples)} MATH samples from GSM8K\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load GSM8K: {e}\")\n",
    "    math_samples = []\n",
    "\n",
    "# Code problems from HumanEval\n",
    "try:\n",
    "    humaneval = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "    code_samples = [\n",
    "        {\n",
    "            'problem': humaneval[i]['prompt'],\n",
    "            'answer': humaneval[i]['canonical_solution'],\n",
    "            'test_cases': humaneval[i].get('test', ''),\n",
    "            'category': 'code'\n",
    "        }\n",
    "        for i in range(min(10, len(humaneval)))\n",
    "    ]\n",
    "    print(f\"✅ Loaded {len(code_samples)} CODE samples from HumanEval\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load HumanEval: {e}\")\n",
    "    code_samples = []\n",
    "\n",
    "# Reasoning problems from ARC\n",
    "try:\n",
    "    arc = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "    reasoning_samples = [\n",
    "        {\n",
    "            'problem': arc[i]['question'],\n",
    "            'choices': arc[i]['choices']['text'],\n",
    "            'answer': arc[i]['answerKey'],\n",
    "            'category': 'reasoning'\n",
    "        }\n",
    "        for i in range(min(10, len(arc)))\n",
    "    ]\n",
    "    print(f\"✅ Loaded {len(reasoning_samples)} REASONING samples from ARC-Challenge\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load ARC: {e}\")\n",
    "    reasoning_samples = []\n",
    "\n",
    "print(f\"\\n📊 Total diagnostic samples: {len(math_samples) + len(code_samples) + len(reasoning_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0abfe4",
   "metadata": {},
   "source": [
    "### Test 2: Answer Extraction Pattern Analysis\n",
    "\n",
    "Check if the model is generating correct answers but they're not being extracted properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43525698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define answer extraction functions used in benchmark\n",
    "def extract_math_answer(response: str) -> str:\n",
    "    \"\"\"Extract answer from math response - multiple patterns.\"\"\"\n",
    "    patterns = [\n",
    "        r'####\\s*([^\\n]+)',  # GSM8K format: #### 42\n",
    "        r'\\\\boxed\\{([^}]+)\\}',  # LaTeX boxed: \\boxed{42}\n",
    "        r'(?:answer|Answer|ANSWER)(?:\\s+is)?[:\\s]+([^\\n\\.]+)',  # \"Answer: 42\" or \"Answer is 42\"\n",
    "        r'(?:final answer|Final answer|FINAL ANSWER)[:\\s]+([^\\n\\.]+)',  # \"Final answer: 42\"\n",
    "        r'\\$([0-9,]+(?:\\.[0-9]+)?)\\$',  # $42$ or $42.50$\n",
    "        r'([0-9,]+(?:\\.[0-9]+)?)\\s*$',  # Just a number at end: \"42\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: find last number\n",
    "    numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', response)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    \n",
    "    return response.strip()[-50:]  # Last 50 chars as fallback\n",
    "\n",
    "def extract_code_answer(response: str) -> str:\n",
    "    \"\"\"Extract code from response.\"\"\"\n",
    "    patterns = [\n",
    "        r'```python\\s*(.*?)\\s*```',  # Python code block\n",
    "        r'```\\s*(.*?)\\s*```',  # Generic code block\n",
    "        r'def\\s+\\w+\\([^)]*\\):.*?(?=\\n(?:def|\\Z))',  # Function definition\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: return first code-like block\n",
    "    lines = response.split('\\n')\n",
    "    code_lines = [l for l in lines if l.strip() and (l.startswith(' ') or 'def ' in l or 'return ' in l)]\n",
    "    if code_lines:\n",
    "        return '\\n'.join(code_lines)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def extract_multiple_choice(response: str) -> str:\n",
    "    \"\"\"Extract A/B/C/D answer.\"\"\"\n",
    "    patterns = [\n",
    "        r'(?:answer|Answer|ANSWER)(?:\\s+is)?[:\\s]+([A-D])',\n",
    "        r'^([A-D])[\\.\\):]',  # \"A.\" or \"A)\" or \"A:\"\n",
    "        r'\\b([A-D])\\b(?=\\s*$)',  # Just \"A\" at end\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.MULTILINE | re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # Fallback: find first A-D\n",
    "    match = re.search(r'\\b([A-D])\\b', response)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    return response.strip()[:10]\n",
    "\n",
    "# Test extraction on sample responses\n",
    "print(\"🧪 Testing Answer Extraction Patterns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Math extraction tests\n",
    "math_test_responses = [\n",
    "    \"Let me solve this step by step. First, 10 + 5 = 15. Then 15 * 2 = 30. #### 30\",\n",
    "    \"The calculation gives us \\\\boxed{42} as the final answer.\",\n",
    "    \"After working through the problem, the answer is: 156\",\n",
    "    \"We get 25 apples in total.\",\n",
    "    \"Final answer: $127.50$\",\n",
    "]\n",
    "\n",
    "print(\"\\n📊 MATH Extraction Tests:\")\n",
    "for i, resp in enumerate(math_test_responses, 1):\n",
    "    extracted = extract_math_answer(resp)\n",
    "    print(f\"   {i}. Input:     {resp[:60]}{'...' if len(resp) > 60 else ''}\")\n",
    "    print(f\"      Extracted: '{extracted}'\")\n",
    "    print()\n",
    "\n",
    "# Code extraction tests\n",
    "code_test_responses = [\n",
    "    \"Here's the solution:\\n```python\\ndef add(a, b):\\n    return a + b\\n```\\nThis function adds two numbers.\",\n",
    "    \"def multiply(x, y):\\n    return x * y\",\n",
    "    \"The implementation:\\n\\n    def solve(arr):\\n        return max(arr)\\n\\nThis returns the maximum.\",\n",
    "]\n",
    "\n",
    "print(\"\\n💻 CODE Extraction Tests:\")\n",
    "for i, resp in enumerate(code_test_responses, 1):\n",
    "    extracted = extract_code_answer(resp)\n",
    "    print(f\"   {i}. Input:     {resp[:60]}{'...' if len(resp) > 60 else ''}\")\n",
    "    print(f\"      Extracted: '{extracted[:80]}{'...' if len(extracted) > 80 else ''}'\")\n",
    "    print()\n",
    "\n",
    "# Multiple choice extraction tests\n",
    "mc_test_responses = [\n",
    "    \"Looking at the choices, the answer is B.\",\n",
    "    \"Answer: C. The speed of light is constant.\",\n",
    "    \"A) is incorrect. B) is also wrong. The correct answer is D\",\n",
    "    \"D\",\n",
    "]\n",
    "\n",
    "print(\"\\n🔤 MULTIPLE CHOICE Extraction Tests:\")\n",
    "for i, resp in enumerate(mc_test_responses, 1):\n",
    "    extracted = extract_multiple_choice(resp)\n",
    "    print(f\"   {i}. Input:     {resp[:60]}{'...' if len(resp) > 60 else ''}\")\n",
    "    print(f\"      Extracted: '{extracted}'\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ Extraction functions defined and tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417e54d",
   "metadata": {},
   "source": [
    "### Test 3: Consistency Analysis (if model is available locally)\n",
    "\n",
    "Generate same prompt multiple times to measure variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency test framework\n",
    "# This will be executed if model is loaded, otherwise skipped\n",
    "\n",
    "def test_consistency(model, tokenizer, prompt, num_runs=10, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Test response consistency for a single prompt.\"\"\"\n",
    "    responses = []\n",
    "    answers = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        # Format prompt\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # Generate\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if response.startswith(formatted_prompt):\n",
    "            response = response[len(formatted_prompt):].strip()\n",
    "        \n",
    "        responses.append(response)\n",
    "        \n",
    "        # Extract answer\n",
    "        if \"math\" in prompt.lower() or any(x in prompt for x in ['calculate', 'how many', 'total']):\n",
    "            answer = extract_math_answer(response)\n",
    "        elif \"def \" in prompt or \"function\" in prompt.lower():\n",
    "            answer = extract_code_answer(response)\n",
    "        else:\n",
    "            answer = extract_multiple_choice(response)\n",
    "        \n",
    "        answers.append(answer)\n",
    "    \n",
    "    # Analyze consistency\n",
    "    unique_answers = set(answers)\n",
    "    answer_counts = Counter(answers)\n",
    "    most_common = answer_counts.most_common(1)[0]\n",
    "    consistency_rate = most_common[1] / num_runs\n",
    "    \n",
    "    return {\n",
    "        'responses': responses,\n",
    "        'answers': answers,\n",
    "        'unique_answers': len(unique_answers),\n",
    "        'most_common_answer': most_common[0],\n",
    "        'most_common_count': most_common[1],\n",
    "        'consistency_rate': consistency_rate,\n",
    "        'answer_distribution': dict(answer_counts)\n",
    "    }\n",
    "\n",
    "print(\"✅ Consistency testing framework defined\")\n",
    "print(\"\\n⚠️ Note: This requires the model to be loaded\")\n",
    "print(\"   To run consistency tests:\")\n",
    "print(\"   1. Load model and tokenizer\")\n",
    "print(\"   2. Call test_consistency(model, tokenizer, prompt)\")\n",
    "print(\"   3. Analyze the returned metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fc813",
   "metadata": {},
   "source": [
    "### Test 4: Benchmark Output File Analysis (if available)\n",
    "\n",
    "Analyze detailed benchmark results from Vast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for benchmark result files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "benchmark_dir = Path(\"/Users/vivekdurairaj/Projects/Cogumi-LLM/benchmark_results\")\n",
    "\n",
    "print(\"🔍 Searching for Benchmark Result Files\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if benchmark_dir.exists():\n",
    "    print(f\"✅ Found benchmark directory: {benchmark_dir}\")\n",
    "    \n",
    "    # List all files\n",
    "    all_files = list(benchmark_dir.glob(\"*\"))\n",
    "    print(f\"\\n📁 Files in benchmark_results/:\")\n",
    "    for f in sorted(all_files):\n",
    "        size = f.stat().st_size if f.is_file() else 0\n",
    "        ftype = \"DIR\" if f.is_dir() else \"FILE\"\n",
    "        print(f\"   [{ftype}] {f.name:50s} ({size:>10,} bytes)\")\n",
    "    \n",
    "    # Look for JSON report files\n",
    "    json_files = list(benchmark_dir.glob(\"*.json\"))\n",
    "    if json_files:\n",
    "        print(f\"\\n✅ Found {len(json_files)} JSON report file(s)\")\n",
    "        for jf in json_files:\n",
    "            print(f\"   📄 {jf.name}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ No JSON report files found\")\n",
    "    \n",
    "    # Look for category-specific result files\n",
    "    category_files = list(benchmark_dir.glob(\"*_results.*\"))\n",
    "    if category_files:\n",
    "        print(f\"\\n✅ Found {len(category_files)} category result file(s)\")\n",
    "        for cf in category_files:\n",
    "            print(f\"   📄 {cf.name}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ No category-specific result files found\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Benchmark directory not found: {benchmark_dir}\")\n",
    "    print(f\"\\n💡 To analyze benchmark results:\")\n",
    "    print(f\"   1. Download benchmark_results/ folder from Vast.ai\")\n",
    "    print(f\"   2. Place it in: {benchmark_dir}\")\n",
    "    print(f\"   3. Re-run this cell\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39209020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If benchmark JSON exists, load and analyze it\n",
    "def analyze_benchmark_json(json_path):\n",
    "    \"\"\"Deep analysis of benchmark JSON file.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n🔬 DEEP DIVE: Benchmark Results Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract test cases\n",
    "    by_category = data.get('by_category', {})\n",
    "    \n",
    "    for category, results in by_category.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 Category: {category.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get individual test results if available\n",
    "        test_results = results.get('test_results', results.get('tests', []))\n",
    "        \n",
    "        if not test_results:\n",
    "            print(\"⚠️ No detailed test results available\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze patterns\n",
    "        wins = [t for t in test_results if t.get('outcome') == 'win']\n",
    "        losses = [t for t in test_results if t.get('outcome') == 'loss']\n",
    "        ties = [t for t in test_results if t.get('outcome') == 'tie']\n",
    "        \n",
    "        print(f\"\\n📈 Outcome Distribution:\")\n",
    "        print(f\"   Wins:   {len(wins):3d} ({len(wins)/len(test_results)*100:5.1f}%)\")\n",
    "        print(f\"   Losses: {len(losses):3d} ({len(losses)/len(test_results)*100:5.1f}%)\")\n",
    "        print(f\"   Ties:   {len(ties):3d} ({len(ties)/len(test_results)*100:5.1f}%)\")\n",
    "        \n",
    "        # Analyze TIE patterns (most interesting for math!)\n",
    "        if ties and category.lower() == 'math':\n",
    "            print(f\"\\n🔍 MATH TIE ANALYSIS (Why 70% ties?):\")\n",
    "            print(f\"   Looking at {len(ties)} tie cases...\")\n",
    "            \n",
    "            # Sample some ties to analyze\n",
    "            sample_size = min(5, len(ties))\n",
    "            print(f\"\\n   📋 Sample of {sample_size} TIE cases:\")\n",
    "            for i, tie_case in enumerate(ties[:sample_size], 1):\n",
    "                prompt = tie_case.get('prompt', 'N/A')[:80]\n",
    "                local_resp = tie_case.get('local_response', 'N/A')[:100]\n",
    "                gpt4_resp = tie_case.get('gpt4_response', 'N/A')[:100]\n",
    "                \n",
    "                print(f\"\\n   Tie #{i}:\")\n",
    "                print(f\"      Prompt: {prompt}...\")\n",
    "                print(f\"      Local:  {local_resp}...\")\n",
    "                print(f\"      GPT-4:  {gpt4_resp}...\")\n",
    "                print(f\"      Judge reasoning: {tie_case.get('judge_reasoning', 'N/A')[:100]}...\")\n",
    "        \n",
    "        # Analyze LOSS patterns\n",
    "        if losses:\n",
    "            print(f\"\\n🔍 LOSS ANALYSIS (Where model fails):\")\n",
    "            sample_size = min(3, len(losses))\n",
    "            print(f\"   Looking at {sample_size} loss cases...\")\n",
    "            \n",
    "            for i, loss_case in enumerate(losses[:sample_size], 1):\n",
    "                prompt = loss_case.get('prompt', 'N/A')[:80]\n",
    "                local_resp = loss_case.get('local_response', 'N/A')[:100]\n",
    "                \n",
    "                print(f\"\\n   Loss #{i}:\")\n",
    "                print(f\"      Prompt: {prompt}...\")\n",
    "                print(f\"      Local (wrong):  {local_resp}...\")\n",
    "                print(f\"      Why failed: {loss_case.get('judge_reasoning', 'N/A')[:100]}...\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Try to load if file exists\n",
    "json_files = list(benchmark_dir.glob(\"*.json\")) if benchmark_dir.exists() else []\n",
    "if json_files:\n",
    "    print(f\"📄 Analyzing: {json_files[0].name}\")\n",
    "    benchmark_data = analyze_benchmark_json(json_files[0])\n",
    "else:\n",
    "    print(\"⏭️ Skipping - no JSON files available yet\")\n",
    "    print(\"   Download from Vast.ai to enable detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254089f",
   "metadata": {},
   "source": [
    "### Test 5: Manual Diagnostic Tests\n",
    "\n",
    "Create a script to run on Vast.ai for detailed failure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a diagnostic script to run on Vast.ai\n",
    "diagnostic_script = \"\"\"#!/usr/bin/env python3\n",
    "'''\n",
    "Deep Diagnostic Script for Model Failure Analysis\n",
    "Run this on Vast.ai to understand why MATH has 70% ties and CODE needs improvement\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\"\n",
    "ADAPTER_PATH = \"/workspace/data/Cogumi-LLM/checkpoints/final\"\n",
    "OUTPUT_FILE = \"/workspace/diagnostic_results.json\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Test 1: Consistency on same prompt\n",
    "def test_consistency(prompt, num_runs=10, temp=0.7):\n",
    "    answers = []\n",
    "    for i in range(num_runs):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=temp,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if response.startswith(formatted):\n",
    "            response = response[len(formatted):].strip()\n",
    "        \n",
    "        # Extract answer\n",
    "        numbers = re.findall(r'\\\\b\\\\d+(?:\\\\.\\\\d+)?\\\\b', response)\n",
    "        answer = numbers[-1] if numbers else response[-50:]\n",
    "        answers.append(answer)\n",
    "    \n",
    "    unique = len(set(answers))\n",
    "    most_common = Counter(answers).most_common(1)[0]\n",
    "    \n",
    "    return {\n",
    "        'answers': answers,\n",
    "        'unique_count': unique,\n",
    "        'consistency_rate': most_common[1] / num_runs,\n",
    "        'most_common_answer': most_common[0]\n",
    "    }\n",
    "\n",
    "# Test 2: Math problem analysis\n",
    "print(\"Testing MATH consistency...\")\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test[:10]\")\n",
    "math_results = []\n",
    "\n",
    "for i, example in enumerate(gsm8k):\n",
    "    prompt = example['question']\n",
    "    correct_answer = example['answer'].split('####')[-1].strip()\n",
    "    \n",
    "    print(f\"  Math {i+1}/10...\")\n",
    "    result = test_consistency(prompt, num_runs=10, temp=0.7)\n",
    "    result['prompt'] = prompt\n",
    "    result['correct_answer'] = correct_answer\n",
    "    math_results.append(result)\n",
    "\n",
    "# Test 3: Code problem analysis\n",
    "print(\"Testing CODE consistency...\")\n",
    "humaneval = load_dataset(\"openai_humaneval\", split=\"test[:10]\")\n",
    "code_results = []\n",
    "\n",
    "for i, example in enumerate(humaneval):\n",
    "    prompt = f\"Complete this Python function:\\\\n\\\\n{example['prompt']}\"\n",
    "    \n",
    "    print(f\"  Code {i+1}/10...\")\n",
    "    result = test_consistency(prompt, num_runs=10, temp=0.7)\n",
    "    result['prompt'] = prompt[:100]\n",
    "    result['canonical_solution'] = example['canonical_solution']\n",
    "    code_results.append(result)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'math_consistency': math_results,\n",
    "    'code_consistency': code_results\n",
    "}\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\\\n✅ Diagnostic complete! Results saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_math_consistency = sum(r['consistency_rate'] for r in math_results) / len(math_results)\n",
    "avg_code_consistency = sum(r['consistency_rate'] for r in code_results) / len(code_results)\n",
    "\n",
    "print(f\"\\\\nMATH Consistency: {avg_math_consistency*100:.1f}%\")\n",
    "print(f\"   Average unique answers per problem: {sum(r['unique_count'] for r in math_results) / len(math_results):.1f}\")\n",
    "print(f\"\\\\nCODE Consistency: {avg_code_consistency*100:.1f}%\")\n",
    "print(f\"   Average unique answers per problem: {sum(r['unique_count'] for r in code_results) / len(code_results):.1f}\")\n",
    "\n",
    "if avg_math_consistency < 0.6:\n",
    "    print(f\"\\\\n⚠️ MATH consistency is LOW ({avg_math_consistency*100:.0f}%) - explains 70% ties!\")\n",
    "if avg_code_consistency < 0.6:\n",
    "    print(f\"\\\\n⚠️ CODE consistency is LOW ({avg_code_consistency*100:.0f}%) - explains lower performance!\")\n",
    "\"\"\"\n",
    "\n",
    "# Save the diagnostic script\n",
    "script_path = Path(\"/Users/vivekdurairaj/Projects/Cogumi-LLM/scripts/deep_diagnostic.py\")\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(diagnostic_script)\n",
    "\n",
    "print(f\"✅ Generated Deep Diagnostic Script\")\n",
    "print(f\"   Location: {script_path}\")\n",
    "print(f\"\\n📋 To run on Vast.ai:\")\n",
    "print(f\"   1. Upload to: /workspace/scripts/deep_diagnostic.py\")\n",
    "print(f\"   2. Run: python /workspace/scripts/deep_diagnostic.py\")\n",
    "print(f\"   3. Wait ~30-60 minutes for completion\")\n",
    "print(f\"   4. Download: /workspace/diagnostic_results.json\")\n",
    "print(f\"   5. Load results in this notebook for analysis\")\n",
    "print(f\"\\n💡 This will test:\")\n",
    "print(f\"   • 10 math problems × 10 runs each = 100 generations\")\n",
    "print(f\"   • 10 code problems × 10 runs each = 100 generations\")\n",
    "print(f\"   • Measure consistency rate (do answers vary?)\")\n",
    "print(f\"   • Identify if inconsistency explains ties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b0b3f",
   "metadata": {},
   "source": [
    "### Test 6: Analyze Downloaded Diagnostic Results\n",
    "\n",
    "If you've run the diagnostic script and downloaded results, load them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17beec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze diagnostic results\n",
    "diagnostic_results_path = Path(\"/Users/vivekdurairaj/Projects/Cogumi-LLM/diagnostic_results.json\")\n",
    "\n",
    "if diagnostic_results_path.exists():\n",
    "    print(\"✅ Found diagnostic results file!\")\n",
    "    \n",
    "    with open(diagnostic_results_path, 'r') as f:\n",
    "        diagnostic_data = json.load(f)\n",
    "    \n",
    "    print(\"\\n🔬 DEEP DIAGNOSTIC ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Analyze MATH consistency\n",
    "    math_results = diagnostic_data.get('math_consistency', [])\n",
    "    if math_results:\n",
    "        print(f\"\\n📊 MATH Consistency Analysis ({len(math_results)} problems):\")\n",
    "        \n",
    "        for i, result in enumerate(math_results, 1):\n",
    "            prompt = result['prompt'][:60]\n",
    "            correct = result.get('correct_answer', 'N/A')\n",
    "            unique = result['unique_count']\n",
    "            consistency = result['consistency_rate']\n",
    "            most_common = result['most_common_answer']\n",
    "            \n",
    "            print(f\"\\n   Problem {i}: {prompt}...\")\n",
    "            print(f\"      Correct answer: {correct}\")\n",
    "            print(f\"      Unique answers generated: {unique}/10\")\n",
    "            print(f\"      Most common: '{most_common}' (appeared {int(consistency*10)}/10 times)\")\n",
    "            print(f\"      Consistency: {consistency*100:.0f}%\")\n",
    "            \n",
    "            if unique > 5:\n",
    "                print(f\"      ⚠️ HIGH VARIANCE - generated {unique} different answers!\")\n",
    "            elif unique > 2:\n",
    "                print(f\"      ⚠️ MODERATE VARIANCE - some inconsistency\")\n",
    "            else:\n",
    "                print(f\"      ✅ CONSISTENT\")\n",
    "        \n",
    "        avg_consistency = sum(r['consistency_rate'] for r in math_results) / len(math_results)\n",
    "        avg_unique = sum(r['unique_count'] for r in math_results) / len(math_results)\n",
    "        \n",
    "        print(f\"\\n   📈 Overall MATH Stats:\")\n",
    "        print(f\"      Average consistency: {avg_consistency*100:.1f}%\")\n",
    "        print(f\"      Average unique answers: {avg_unique:.1f}/10\")\n",
    "        print(f\"\\n   💡 Interpretation:\")\n",
    "        if avg_consistency < 0.5:\n",
    "            print(f\"      CRITICAL: Model is highly inconsistent!\")\n",
    "            print(f\"      This EXPLAINS the 70% ties - answers vary each time!\")\n",
    "        elif avg_consistency < 0.7:\n",
    "            print(f\"      MODERATE: Some inconsistency present\")\n",
    "        else:\n",
    "            print(f\"      GOOD: Model is fairly consistent\")\n",
    "    \n",
    "    # Analyze CODE consistency\n",
    "    code_results = diagnostic_data.get('code_consistency', [])\n",
    "    if code_results:\n",
    "        print(f\"\\n\\n💻 CODE Consistency Analysis ({len(code_results)} problems):\")\n",
    "        \n",
    "        for i, result in enumerate(code_results, 1):\n",
    "            prompt = result['prompt'][:60]\n",
    "            unique = result['unique_count']\n",
    "            consistency = result['consistency_rate']\n",
    "            \n",
    "            print(f\"\\n   Problem {i}: {prompt}...\")\n",
    "            print(f\"      Unique solutions generated: {unique}/10\")\n",
    "            print(f\"      Consistency: {consistency*100:.0f}%\")\n",
    "            \n",
    "            if unique > 5:\n",
    "                print(f\"      ⚠️ HIGH VARIANCE - code differs significantly!\")\n",
    "            elif unique > 2:\n",
    "                print(f\"      ⚠️ MODERATE VARIANCE\")\n",
    "            else:\n",
    "                print(f\"      ✅ CONSISTENT\")\n",
    "        \n",
    "        avg_consistency = sum(r['consistency_rate'] for r in code_results) / len(code_results)\n",
    "        avg_unique = sum(r['unique_count'] for r in code_results) / len(code_results)\n",
    "        \n",
    "        print(f\"\\n   📈 Overall CODE Stats:\")\n",
    "        print(f\"      Average consistency: {avg_consistency*100:.1f}%\")\n",
    "        print(f\"      Average unique solutions: {avg_unique:.1f}/10\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 DIAGNOSTIC CONCLUSIONS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if math_results:\n",
    "        math_consistency = sum(r['consistency_rate'] for r in math_results) / len(math_results)\n",
    "        print(f\"\\n1. MATH (70% ties explained):\")\n",
    "        if math_consistency < 0.6:\n",
    "            print(f\"   ✅ CONFIRMED: Low consistency ({math_consistency*100:.0f}%) causes ties!\")\n",
    "            print(f\"   → Model CAN solve problems but answers vary\")\n",
    "            print(f\"   → GPT-4 judge sees \\\"comparable but different\\\" → TIE\")\n",
    "            print(f\"   → Solution: Greedy decoding + self-consistency training\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ UNEXPECTED: Consistency is good ({math_consistency*100:.0f}%)\")\n",
    "            print(f\"   → Ties may be due to formatting differences, not inconsistency\")\n",
    "    \n",
    "    if code_results:\n",
    "        code_consistency = sum(r['consistency_rate'] for r in code_results) / len(code_results)\n",
    "        print(f\"\\n2. CODE (58% score analysis):\")\n",
    "        if code_consistency < 0.6:\n",
    "            print(f\"   ✅ CONFIRMED: Low consistency ({code_consistency*100:.0f}%) impacts performance\")\n",
    "            print(f\"   → Code structure varies between runs\")\n",
    "            print(f\"   → Solution: Self-consistency training on correct solutions\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Consistency is acceptable ({code_consistency*100:.0f}%)\")\n",
    "            print(f\"   → May need more training examples for edge cases\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️ Diagnostic results not yet available\")\n",
    "    print(f\"   Expected location: {diagnostic_results_path}\")\n",
    "    print(f\"\\n📋 Steps to generate:\")\n",
    "    print(f\"   1. Run scripts/deep_diagnostic.py on Vast.ai\")\n",
    "    print(f\"   2. Download diagnostic_results.json\")\n",
    "    print(f\"   3. Place in project root\")\n",
    "    print(f\"   4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df16226",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Complete Diagnostic Workflow\n",
    "\n",
    "**This notebook provides:**\n",
    "\n",
    "1. **Benchmark Results Analysis** - High-level performance metrics\n",
    "2. **Deep Diagnostics** - Answer extraction, consistency testing, failure pattern analysis\n",
    "3. **Dataset Composition** - What was in the training data\n",
    "4. **Correlation Analysis** - Connect training data to performance\n",
    "5. **Actionable Scripts** - deep_diagnostic.py to run on Vast.ai\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Run cells 1-10 for immediate benchmark analysis (no GPU needed)\n",
    "2. Upload `scripts/deep_diagnostic.py` to Vast.ai\n",
    "3. Run diagnostic script (~30-60 min)\n",
    "4. Download `diagnostic_results.json`\n",
    "5. Run remaining cells to analyze consistency patterns\n",
    "6. Use insights to guide self-consistency training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b346efa",
   "metadata": {},
   "source": [
    "# 🔍 Benchmark Diagnostic Analysis\n",
    "\n",
    "**Purpose**: Analyze benchmark results to understand failure patterns in MATH, CODE, and CREATIVITY\n",
    "\n",
    "**Results Summary:**\n",
    "- MATH: 41% (3W-12L-35T) - 70% ties suggest capability but inconsistency\n",
    "- CODE: 58% (24W-16L-10T) - 60% win rate, competitive performance\n",
    "- CREATIVITY: 60% (2W-1L-2T) - Small sample (5 tests)\n",
    "- REASONING: 86% (43W-7L-0T) - Excellent! 🏆\n",
    "- KNOWLEDGE: 90% (45W-5L-0T) - Outstanding! 🏆\n",
    "- INSTRUCTION: 76% (33W-7L-10T) - Very good ⭐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea47e4e",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Composition Analysis\n",
    "\n",
    "Check how many examples of each type are in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to training dataset\n",
    "dataset_path = Path(\"/Users/vivekdurairaj/Projects/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl\")\n",
    "\n",
    "print(f\"📂 Loading dataset from: {dataset_path}\")\n",
    "print(f\"   File exists: {dataset_path.exists()}\")\n",
    "print(f\"   File size: {dataset_path.stat().st_size / 1024 / 1024:.1f} MB\" if dataset_path.exists() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze dataset\n",
    "def analyze_dataset(file_path, sample_size=None):\n",
    "    \"\"\"Analyze dataset composition and order.\"\"\"\n",
    "    \n",
    "    sources = []\n",
    "    categories = []\n",
    "    \n",
    "    print(\"📖 Reading dataset...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if sample_size and i >= sample_size:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                example = json.loads(line)\n",
    "                \n",
    "                # Track source\n",
    "                source = example.get('source', 'unknown')\n",
    "                sources.append(source)\n",
    "                \n",
    "                # Infer category from source or content\n",
    "                if 'math' in source.lower() or 'metamath' in source.lower():\n",
    "                    category = 'math'\n",
    "                elif 'code' in source.lower() or 'alpaca' in source.lower():\n",
    "                    category = 'code'\n",
    "                elif 'orca' in source.lower():\n",
    "                    category = 'reasoning'\n",
    "                elif 'dolly' in source.lower():\n",
    "                    category = 'knowledge'\n",
    "                elif 'anthropic' in source.lower():\n",
    "                    category = 'instruction'\n",
    "                else:\n",
    "                    category = 'other'\n",
    "                \n",
    "                categories.append(category)\n",
    "                \n",
    "                if i % 100000 == 0 and i > 0:\n",
    "                    print(f\"   Processed {i:,} examples...\")\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    total = len(sources)\n",
    "    print(f\"\\n✅ Loaded {total:,} examples\")\n",
    "    \n",
    "    return sources, categories\n",
    "\n",
    "# Analyze full dataset (or sample if too large)\n",
    "sources, categories = analyze_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze composition\n",
    "source_counts = Counter(sources)\n",
    "category_counts = Counter(categories)\n",
    "\n",
    "print(\"\\n📊 Dataset Composition by Source:\")\n",
    "print(\"=\" * 60)\n",
    "for source, count in source_counts.most_common():\n",
    "    percentage = count / len(sources) * 100\n",
    "    print(f\"   {source:30s}: {count:7,} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n📊 Dataset Composition by Inferred Category:\")\n",
    "print(\"=\" * 60)\n",
    "for category, count in category_counts.most_common():\n",
    "    percentage = count / len(categories) * 100\n",
    "    print(f\"   {category:15s}: {count:7,} ({percentage:5.1f}%)\")\n",
    "    \n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {'Category': cat, 'Count': count, 'Percentage': f\"{count/len(categories)*100:.1f}%\"}\n",
    "    for cat, count in category_counts.most_common()\n",
    "])\n",
    "\n",
    "print(\"\\n📋 Summary Table:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training order - check first/middle/last portions\n",
    "def analyze_order(categories, window_size=10000):\n",
    "    \"\"\"Analyze category distribution at different points in training.\"\"\"\n",
    "    \n",
    "    total = len(categories)\n",
    "    \n",
    "    # First 10K, middle 10K, last 10K\n",
    "    first = Counter(categories[:window_size])\n",
    "    middle_start = (total // 2) - (window_size // 2)\n",
    "    middle = Counter(categories[middle_start:middle_start + window_size])\n",
    "    last = Counter(categories[-window_size:])\n",
    "    \n",
    "    print(\"\\n🔄 Training Order Analysis (First/Middle/Last 10K examples):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_categories = set(first.keys()) | set(middle.keys()) | set(last.keys())\n",
    "    \n",
    "    for category in sorted(all_categories):\n",
    "        f = first.get(category, 0)\n",
    "        m = middle.get(category, 0)\n",
    "        l = last.get(category, 0)\n",
    "        \n",
    "        print(f\"   {category:15s}: First {f:5,} ({f/window_size*100:4.1f}%) | \"\n",
    "              f\"Middle {m:5,} ({m/window_size*100:4.1f}%) | \"\n",
    "              f\"Last {l:5,} ({l/window_size*100:4.1f}%)\")\n",
    "    \n",
    "    return first, middle, last\n",
    "\n",
    "first_dist, middle_dist, last_dist = analyze_order(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, dist, title in zip(axes, [first_dist, middle_dist, last_dist], \n",
    "                            ['First 10K', 'Middle 10K', 'Last 10K']):\n",
    "    categories_list = list(dist.keys())\n",
    "    counts = list(dist.values())\n",
    "    ax.bar(categories_list, counts)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Category')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/vivekdurairaj/Projects/Cogumi-LLM/data/dataset_order_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n💾 Saved visualization to: data/dataset_order_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dad348",
   "metadata": {},
   "source": [
    "## Step 2: Correlate Dataset Distribution with Benchmark Results\n",
    "\n",
    "Compare training data composition with performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark results\n",
    "benchmark_results = {\n",
    "    'math': {'score': 41, 'wins': 3, 'losses': 12, 'ties': 35},\n",
    "    'code': {'score': 58, 'wins': 24, 'losses': 16, 'ties': 10},\n",
    "    'reasoning': {'score': 86, 'wins': 43, 'losses': 7, 'ties': 0},\n",
    "    'knowledge': {'score': 90, 'wins': 45, 'losses': 5, 'ties': 0},\n",
    "    'instruction': {'score': 76, 'wins': 33, 'losses': 7, 'ties': 10},\n",
    "}\n",
    "\n",
    "# Create correlation analysis\n",
    "category_counts_dict = dict(category_counts)\n",
    "total_examples = sum(category_counts_dict.values())\n",
    "\n",
    "correlation_data = []\n",
    "for category in ['math', 'code', 'reasoning', 'knowledge', 'instruction']:\n",
    "    training_count = category_counts_dict.get(category, 0)\n",
    "    training_pct = training_count / total_examples * 100\n",
    "    \n",
    "    bench = benchmark_results.get(category, {})\n",
    "    score = bench.get('score', 0)\n",
    "    wins = bench.get('wins', 0)\n",
    "    losses = bench.get('losses', 0)\n",
    "    ties = bench.get('ties', 0)\n",
    "    total_tests = wins + losses + ties\n",
    "    win_rate = (wins / (wins + losses) * 100) if (wins + losses) > 0 else 0\n",
    "    \n",
    "    # Check position in training (early/late)\n",
    "    first_pct = first_dist.get(category, 0) / 10000 * 100\n",
    "    last_pct = last_dist.get(category, 0) / 10000 * 100\n",
    "    position = \"Early\" if first_pct > last_pct else \"Late\" if last_pct > first_pct else \"Uniform\"\n",
    "    \n",
    "    correlation_data.append({\n",
    "        'Category': category.upper(),\n",
    "        'Training %': f\"{training_pct:.1f}%\",\n",
    "        'Training Count': f\"{training_count:,}\",\n",
    "        'Position': position,\n",
    "        'Score': f\"{score}%\",\n",
    "        'Win Rate': f\"{win_rate:.0f}%\",\n",
    "        'Ties %': f\"{ties/total_tests*100:.0f}%\" if total_tests > 0 else \"N/A\"\n",
    "    })\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_data)\n",
    "\n",
    "print(\"\\n🔗 Training Data vs Benchmark Performance Correlation:\")\n",
    "print(\"=\" * 100)\n",
    "print(correlation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab629257",
   "metadata": {},
   "source": [
    "## Step 3: Hypothesis Testing\n",
    "\n",
    "Test the catastrophic forgetting hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 HYPOTHESIS: Catastrophic Forgetting Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nExpectation: If catastrophic forgetting is happening, we should see:\")\n",
    "print(\"   1. Early training data (First 10K%) → Lower benchmark performance\")\n",
    "print(\"   2. Late training data (Last 10K%) → Higher benchmark performance\")\n",
    "print(\"   3. Math appears early → Should perform worse than late categories\")\n",
    "\n",
    "print(\"\\n📊 Observations:\")\n",
    "for idx, row in correlation_df.iterrows():\n",
    "    cat = row['Category']\n",
    "    pos = row['Position']\n",
    "    score = row['Score']\n",
    "    \n",
    "    if pos == \"Early\":\n",
    "        expectation = \"LOW performance (forgetting)\"\n",
    "    elif pos == \"Late\":\n",
    "        expectation = \"HIGH performance (recent)\"\n",
    "    else:\n",
    "        expectation = \"MEDIUM performance (uniform)\"\n",
    "    \n",
    "    print(f\"   {cat:12s}: Position={pos:8s} → Expected: {expectation:30s} | Actual: {score}\")\n",
    "\n",
    "print(\"\\n🎯 Key Insights:\")\n",
    "print(\"   • MATH (41%) with 70% ties → Capability present but INCONSISTENT (sampling issue)\")\n",
    "print(\"   • CODE (58%) with 60% win rate → Competitive but needs refinement\")\n",
    "print(\"   • REASONING (86%) and KNOWLEDGE (90%) → EXCELLENT retention!\")\n",
    "print(\"\\n💡 Conclusion:\")\n",
    "print(\"   Issue is likely NOT catastrophic forgetting but SAMPLING INCONSISTENCY.\")\n",
    "print(\"   Math has 70% ties = model CAN solve at GPT-4 level but unreliably.\")\n",
    "print(\"   Solution: Self-consistency training to 'bake in' deterministic behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee2e43",
   "metadata": {},
   "source": [
    "## Step 4: Recommendations\n",
    "\n",
    "Based on the analysis, determine next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎯 RECOMMENDED ACTIONS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1️⃣ IMMEDIATE: Self-Consistency Training\")\n",
    "print(\"   Target: MATH (41% → 70-80%) and CODE (58% → 75-80%)\")\n",
    "print(\"   Method: Generate with greedy decoding (do_sample=False)\")\n",
    "print(\"   Script: scripts/self_consistency_distillation.py\")\n",
    "print(\"   Duration: 2-4 hours generation + 6-12 hours training\")\n",
    "print(\"   Cost: ~$50-100\")\n",
    "\n",
    "print(\"\\n2️⃣ MONITOR: Reasoning & Knowledge\")\n",
    "print(\"   Current: REASONING 86%, KNOWLEDGE 90% - EXCELLENT!\")\n",
    "print(\"   Action: Ensure self-consistency training doesn't degrade these\")\n",
    "print(\"   Method: Use conservative learning rate (1e-6) and mix old data\")\n",
    "\n",
    "print(\"\\n3️⃣ OPTIONAL: Creativity Expansion\")\n",
    "print(\"   Current: 60% on 5 tests (too small for conclusions)\")\n",
    "print(\"   Action: Expand to 20-30 creative prompts after fixing math/code\")\n",
    "\n",
    "print(\"\\n4️⃣ FINAL: GPT-5 Hybrid (if needed)\")\n",
    "print(\"   If post-training: MATH/CODE still <70%\")\n",
    "print(\"   Then: Use GPT-5 for failure cases only\")\n",
    "print(\"   Cost: Additional $50-150 (targeted distillation)\")\n",
    "\n",
    "print(\"\\n✅ Expected Final Performance:\")\n",
    "print(\"   MATH:        41% → 70-80% (self-consistency) → 85-95% (GPT-5 if needed)\")\n",
    "print(\"   CODE:        58% → 75-80% (self-consistency) → 85-95% (GPT-5 if needed)\")\n",
    "print(\"   REASONING:   86% → 90-95% (maintain/improve)\")\n",
    "print(\"   KNOWLEDGE:   90% → 92-98% (maintain/improve)\")\n",
    "print(\"   INSTRUCTION: 76% → 80-85% (maintain/improve)\")\n",
    "print(\"   CREATIVITY:  60% → 70-80% (expand tests + training)\")\n",
    "print(\"\\n🎯 Overall: 75-85% average → Target: 88-100% GPT-4 baseline ACHIEVABLE! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2185dd7d",
   "metadata": {},
   "source": [
    "# Phase 1B: Automated GPT-4 Benchmarking\n",
    "\n",
    "**Run this after Phase 1A training completes**\n",
    "\n",
    "This notebook:\n",
    "1. Loads your trained model\n",
    "2. Compares it against GPT-4 on 6 categories\n",
    "3. Identifies failure patterns\n",
    "4. Generates targeted distillation dataset\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Phase 1A training\n",
    "- OpenAI API key\n",
    "- ~$10-20 in API credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e24ee",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai datasets transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a653de8",
   "metadata": {},
   "source": [
    "## Step 2: Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your OpenAI API key\n",
    "OPENAI_API_KEY = getpass(\"Enter OpenAI API key: \")\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "print(\"‚úÖ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc46aed",
   "metadata": {},
   "source": [
    "## Step 3: Run Quick Benchmark (50 samples/category)\n",
    "\n",
    "**This will take ~30-60 minutes and cost ~$5-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aa2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/Cogumi-LLM')\n",
    "\n",
    "from scripts.automated_gpt4_benchmark import BenchmarkSuite\n",
    "\n",
    "# Initialize benchmark\n",
    "suite = BenchmarkSuite(\n",
    "    model_path=\"/data/Cogumi-LLM/checkpoints/final\",\n",
    "    openai_key=OPENAI_API_KEY,\n",
    "    output_dir=\"./benchmark_results\"\n",
    ")\n",
    "\n",
    "# Run on key categories\n",
    "suite.run_full_benchmark(\n",
    "    categories=[\"math\", \"code\", \"reasoning\", \"knowledge\", \"instruction\"],\n",
    "    samples_per_category=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ef4d5",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load latest report\n",
    "reports = sorted(Path(\"./benchmark_results\").glob(\"benchmark_report_*.json\"))\n",
    "if not reports:\n",
    "    print(\"‚ùå No benchmark reports found\")\n",
    "else:\n",
    "    with open(reports[-1]) as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    # Display overall score\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Overall Score vs GPT-4: {report['overall']['score_vs_gpt4']:.1f}%\")\n",
    "    print(f\"Rating: {report['overall']['performance_rating']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(report['by_category']).T\n",
    "    df = df.sort_values('score', ascending=False)\n",
    "    \n",
    "    print(\"\\nResults by Category:\")\n",
    "    print(df[['score', 'wins', 'losses', 'ties']])\n",
    "    \n",
    "    # Identify weak areas (< 85%)\n",
    "    weak_categories = df[df['score'] < 85].index.tolist()\n",
    "    \n",
    "    if weak_categories:\n",
    "        print(f\"\\n‚ö†Ô∏è Weak areas (need GPT-5 distillation):\")\n",
    "        for cat in weak_categories:\n",
    "            print(f\"   - {cat}: {df.loc[cat, 'score']:.1f}%\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All categories above 85% - excellent performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d9e2e",
   "metadata": {},
   "source": [
    "## Step 5: Identify Failure Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed results\n",
    "failure_examples = []\n",
    "\n",
    "for category in suite.results.keys():\n",
    "    for result in suite.results[category]['details']:\n",
    "        if result['judgment']['winner'] == 'B':  # GPT-4 won\n",
    "            failure_examples.append({\n",
    "                'category': category,\n",
    "                'prompt': result['prompt'],\n",
    "                'local_response': result['local_response'],\n",
    "                'gpt4_response': result['gpt4_response'],\n",
    "                'reasoning': result['judgment'].get('reasoning', '')\n",
    "            })\n",
    "\n",
    "print(f\"\\nüìä Found {len(failure_examples)} failures\")\n",
    "print(f\"\\nBreakdown by category:\")\n",
    "\n",
    "from collections import Counter\n",
    "category_counts = Counter(ex['category'] for ex in failure_examples)\n",
    "for cat, count in category_counts.most_common():\n",
    "    print(f\"   {cat}: {count} failures\")\n",
    "\n",
    "# Save failures for Phase 1C\n",
    "with open('./benchmark_results/failure_examples.json', 'w') as f:\n",
    "    json.dump(failure_examples, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Failure examples saved to: ./benchmark_results/failure_examples.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838ad66",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on results:\n",
    "\n",
    "### If Score ‚â• 90%:\n",
    "‚úÖ **Proceed to Phase 2: Compression**\n",
    "- Your model is ready for pruning and quantization\n",
    "\n",
    "### If Score 85-90%:\n",
    "üìä **Optional: Phase 1C Distillation**\n",
    "- Generate 10K targeted examples for weak areas\n",
    "- Quick fine-tune to push above 90%\n",
    "\n",
    "### If Score < 85%:\n",
    "‚ö†Ô∏è **Required: Phase 1C Distillation**\n",
    "- Generate 40K targeted examples\n",
    "- Focus on weak categories identified above\n",
    "- Target: 88-100% GPT-4 baseline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# ============================================================================
# Phase 1A 2.0 - Full Precision Training Dependencies
# ============================================================================
# STABLE VERSIONS WITH PRE-COMPILED WHEELS FOR H100
# All versions verified to work together with pre-compiled binaries
# Last verified: October 2025
# CUDA Version: 12.4
# PyTorch Version: 2.6.0+cu124 (latest stable for CUDA 12.4)
# ============================================================================

# ============================================================================
# ⚠️  INSTALLATION WARNING - 4-STAGE INSTALLATION REQUIRED
# ============================================================================
# Stage 1: Install PyTorch first
# Stage 2: Install psutil (required by Flash Attention's setup.py)
# Stage 3: Install Flash Attention with --no-build-isolation flag
# Stage 4: Install everything else from this file
#
# Why 4 stages?
# 1. Flash Attention's setup.py IMPORTS torch AND psutil during build metadata generation
# 2. pip's build isolation creates a clean environment without installed packages
# 3. --no-build-isolation disables isolation, allowing Flash Attention to import dependencies
# 4. This flag cannot be specified in requirements.txt, must be done manually
#
# INSTALLATION SEQUENCE FOR CUDA 12.4:
#
#   # Stage 1: PyTorch first (CUDA 12.4) - CLEAN INSTALL
#   pip uninstall torch torchvision torchaudio -y
#   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
#   python -c "import torch; print(f'✅ PyTorch {torch.__version__} installed')"
#
#   # Stage 2: psutil (required by Flash Attention)
#   pip install psutil==5.9.8
#   python -c "import psutil; print('✅ psutil installed')"
#
#   # Stage 3: Flash Attention with --no-build-isolation
#   pip install flash-attn --no-build-isolation
#   python -c "import flash_attn; print('✅ Flash Attention installed')"
#
#   # Stage 4: Everything else
#   pip install -r requirements-stable-precompiled.txt
#
# ============================================================================

# ============================================================================
# CORE PRINCIPLE: Use STABLE versions with pre-compiled wheels
# - PyTorch 2.6.0+cu124 (latest stable for CUDA 12.4)
# - Flash Attention 2.x (auto-selected for CUDA 12.4)
# - xformers (auto-selected to match PyTorch version)
# - Transformers 4.43.3 (stable, Llama 3.1 support)
# - Unsloth July-2024 (2-3× training speedup)
# ============================================================================

# ============================================================================
# INSTALLATION ORDER CRITICAL:
# 1. PyTorch first (required by Flash Attention build system)
# 2. Flash Attention (pre-compiled, needs PyTorch installed)
# 3. Everything else (transformers, bitsandbytes, etc.)
# ============================================================================

# ----------------------------------------------------------------------------
# 1. PYTORCH STACK - CUDA 12.4 (H100 Compatible) - INSTALL FIRST
# ----------------------------------------------------------------------------
# Install from official PyTorch wheel repository
# CUDA 12.4 is the latest stable version for H100
# Latest versions: PyTorch 2.6.0, torchvision, torchaudio (auto-matched)
# NOTE: These should be installed in Stage 1 (see installation sequence above)
# Command: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# ----------------------------------------------------------------------------
# 2. FLASH ATTENTION 2 - MUST BE INSTALLED SEPARATELY
# ----------------------------------------------------------------------------
# ⚠️  FLASH ATTENTION NOT IN THIS FILE - INSTALL MANUALLY
# Flash Attention's build system imports torch during setup.py execution
# Must use --no-build-isolation flag (cannot be specified in requirements.txt)
# 
# MANUAL INSTALLATION REQUIRED (Stage 3):
#   pip install flash-attn --no-build-isolation
#
# See installation guide for correct sequence

# ----------------------------------------------------------------------------
# 3. BITSANDBYTES - PRE-COMPILED WHEELS
# ----------------------------------------------------------------------------
# Standard PyPI version works with CUDA 12.1
# NO compilation needed
bitsandbytes==0.43.1

# ----------------------------------------------------------------------------
# 4. TRANSFORMERS ECOSYSTEM - STABLE VERSIONS
# ----------------------------------------------------------------------------
# Transformers 4.43.3: Stable, Llama 3.1 support, compatible with above
transformers==4.43.3
tokenizers==0.19.1

# PEFT for LoRA/QLoRA support
peft==0.11.1

# Accelerate for distributed training
accelerate==0.30.1

# TRL for training utilities
trl==0.9.6

# Datasets for data loading
datasets==2.19.1

# ----------------------------------------------------------------------------
# 5. XFORMERS - REQUIRED BY UNSLOTH
# ----------------------------------------------------------------------------
# xformers provides memory-efficient attention operations
# Required by Unsloth for 2-3× speedup
# Version auto-selected by pip to match PyTorch version
# For PyTorch 2.6.0+cu124: xformers will auto-select compatible version
# Note: Use flexible version constraint to avoid torch version conflicts
xformers>=0.0.26

# ----------------------------------------------------------------------------
# 6. UNSLOTH - OPTIONAL BUT RECOMMENDED (2-3x speedup)
# ----------------------------------------------------------------------------
# Use git version for compatibility (July-2024 tag is stable)
# This provides Unsloth library WITHOUT pre-quantized base
# IMPORTANT: Use with full precision meta-llama/Meta-Llama-3.1-8B-Instruct
# REQUIRES: xformers (installed above)
unsloth @ git+https://github.com/unslothai/unsloth.git@July-2024

# ----------------------------------------------------------------------------
# 7. SUPPORTING LIBRARIES
# ----------------------------------------------------------------------------
# NumPy 1.26.4 (CRITICAL - NOT 2.0+, breaks scipy/sklearn)
numpy==1.26.4

# Scientific computing
scipy==1.13.0
scikit-learn==1.4.2

# HuggingFace Hub (>=0.23.2 required by transformers 4.43.3)
huggingface-hub==0.23.4

# Logging and monitoring
tensorboard==2.16.2
wandb==0.17.0

# Utilities
tqdm==4.66.4
rich==13.7.1

# JSONL handling
jsonlines==4.0.0

# Ninja for faster compilation (if any compilation needed)
ninja==1.11.1

# Packaging for version checks
packaging==24.0

# ----------------------------------------------------------------------------
# 8. JUPYTER SUPPORT (Optional)
# ----------------------------------------------------------------------------
ipykernel==6.29.4
jupyter==1.0.0

# ============================================================================
# INSTALLATION INSTRUCTIONS
# ============================================================================
# 
# 1. Create virtual environment:
#    python3.10 -m venv venv_phase1a_2_0
#    source venv_phase1a_2_0/bin/activate
#
# 2. Upgrade pip:
#    pip install --upgrade pip setuptools wheel
#
# 3. Install all dependencies:
#    pip install -r requirements-stable-precompiled.txt
#
# 4. Verify installation:
#    python scripts/verify_environment.py
#
# EXPECTED INSTALL TIME: 5-10 minutes (all pre-compiled)
# ============================================================================

# ============================================================================
# VERSION RATIONALE
# ============================================================================
# PyTorch 2.3.1: Stable, well-tested, H100 support, CUDA 12.1 compatible
# Flash Attention 2.5.8: Latest with pre-compiled wheels for PyTorch 2.3.x
# Transformers 4.43.3: Stable, Llama 3.1 support, no breaking changes
# Unsloth 2024.7: Stable release, NOT 2024.8+ (has dependency conflicts)
# NumPy 1.26.4: Last 1.x version, 2.0+ breaks many ML packages
# BitsAndBytes 0.43.1: Stable, CUDA 12.1 support, pre-compiled wheels
# PEFT 0.11.1: Compatible with Transformers 4.43.3
# Accelerate 0.30.1: Stable, compatible with above versions
# TRL 0.9.6: Stable, compatible with Transformers 4.43.3
# ============================================================================

# ============================================================================
# KNOWN COMPATIBLE CONFIGURATIONS
# ============================================================================
# H100 80GB: ✅ Tested and verified
# A100 40GB/80GB: ✅ Should work (CUDA 12.1 compatible)
# RTX 4090: ✅ Should work (CUDA 12.1 compatible)
# RTX 4080: ✅ Should work (CUDA 12.1 compatible)
# ============================================================================

# ============================================================================
# TROUBLESHOOTING
# ============================================================================
# If Flash Attention install fails:
#   - Verify CUDA 12.1 is installed: nvcc --version
#   - Verify PyTorch 2.3.1 is installed: python -c "import torch; print(torch.__version__)"
#   - Check GPU: nvidia-smi
#   - Try manual install: pip install flash-attn==2.5.8 --no-build-isolation \
#       --extra-index-url https://flashattn.github.io/whl/cu121/torch2.3/
#
# If Unsloth install fails:
#   - Try without Unsloth (optional): Comment out unsloth line
#   - Training will still work, just slower (no 2-3x speedup)
#
# If NumPy conflicts:
#   - Force reinstall: pip install numpy==1.26.4 --force-reinstall
#   - Verify: python -c "import numpy; print(numpy.__version__)"
# ============================================================================

# ============================================================================
# Phase 1A 2.0 - Full Precision Training Dependencies
# ============================================================================
# ðŸ† GOLDEN CONFIGURATION - VERIFIED WORKING ON H100 80GB ðŸ†
# All versions tested and confirmed compatible
# Last verified: October 2025
# CUDA Version: 12.4
# PyTorch Version: 2.6.0+cu124
# ============================================================================

# ============================================================================
# âš ï¸  GOLDEN CONFIGURATION - INSTALLATION SEQUENCE
# ============================================================================
# This exact sequence has been tested and verified to work on H100 80GB
# DO NOT change version numbers or installation order
#
# INSTALLATION SEQUENCE FOR CUDA 12.4:
#
#   # Stage 1: PyTorch 2.6.0 + CUDA 12.4
#   pip install torch==2.6.0+cu124 torchvision==0.21.0 torchaudio==2.6.0 \
#       --index-url https://download.pytorch.org/whl/cu124
#
#   # Stage 2: FlashAttention 2.7.4 (pre-built for torch 2.6.0 + CUDA 12.4)
#   pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4+cu124torch2.6-cp310-cp310-linux_x86_64.whl
#
#   # Stage 3: psutil
#   pip install psutil==7.1.2
#
#   # Stage 4: bitsandbytes
#   pip install bitsandbytes==0.43.1
#
#   # Stage 5: Transformers ecosystem
#   pip install transformers==4.43.3 tokenizers==0.19.1
#   pip install peft==0.11.1 accelerate==0.30.1 trl==0.9.6 datasets==2.19.1
#
#   # Stage 6: xformers (with --no-deps to avoid version conflicts)
#   pip install xformers==0.0.28.post2 --no-deps
#
#   # Stage 7: Unsloth (with --no-deps to avoid dependency conflicts)
#   pip install "unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git@July-2024" --no-deps
#
#   # Stage 8: Other dependencies
#   pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.4.2 \
#       huggingface-hub==0.23.4 tensorboard==2.16.2 wandb==0.17.0 \
#       tqdm==4.66.4 rich==13.7.1 jsonlines==4.0.0 ninja==1.11.1 packaging==24.0
#
# ============================================================================
# ============================================================================

# ============================================================================
# CORE PRINCIPLE: Use STABLE versions with pre-compiled wheels
# - PyTorch 2.6.0+cu124 (PINNED for compatibility)
# - Flash Attention 2.8.2 (pre-built for torch 2.6.0 + CUDA 12.4)
# - xformers 0.0.28.post2 (PINNED for torch 2.6.0 compatibility)
# - Transformers 4.43.3 (stable, Llama 3.1 support)
# - Unsloth July-2024 (2-3Ã— training speedup)
#
# CRITICAL: xformers version MUST match PyTorch version
# - torch 2.6.0 â†’ xformers 0.0.28.post2 or 0.0.29
# - torch 2.8.0 â†’ xformers 0.0.32+ (DO NOT USE with torch 2.6.0)
#
# Flash Attention pre-built wheels: https://github.com/mjun0812/flash-attention-prebuild-wheels
# ============================================================================

# ============================================================================
# INSTALLATION ORDER CRITICAL:
# 1. PyTorch first (required by Flash Attention build system)
# 2. Flash Attention (pre-compiled, needs PyTorch installed)
# 3. Everything else (transformers, bitsandbytes, etc.)
# ============================================================================

# ----------------------------------------------------------------------------
# 1. PYTORCH STACK - CUDA 12.4 (H100 Compatible) - INSTALL FIRST
# ----------------------------------------------------------------------------
# Install from official PyTorch wheel repository
# CUDA 12.4 is the latest stable version for H100
# Latest versions: PyTorch 2.6.0, torchvision, torchaudio (auto-matched)
# NOTE: These should be installed in Stage 1 (see installation sequence above)
# Command: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# ----------------------------------------------------------------------------
# 2. FLASH ATTENTION 2 - MUST BE INSTALLED SEPARATELY
# ----------------------------------------------------------------------------
# âš ï¸  FLASH ATTENTION NOT IN THIS FILE - INSTALL MANUALLY
# Using pre-built wheel for torch 2.6.0 + CUDA 12.4 from mjun0812's repository
# 
# MANUAL INSTALLATION REQUIRED (Stage 3):
#   pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v2.8.2/flash_attn-2.8.2+cu124torch2.6-cp310-cp310-linux_x86_64.whl
#
# Pre-built wheels repository: https://github.com/mjun0812/flash-attention-prebuild-wheels/releases

# ----------------------------------------------------------------------------
# 3. BITSANDBYTES - PRE-COMPILED WHEELS
# ----------------------------------------------------------------------------
# Standard PyPI version works with CUDA 12.1
# NO compilation needed
bitsandbytes==0.43.1

# ----------------------------------------------------------------------------
# 4. TRANSFORMERS ECOSYSTEM - STABLE VERSIONS
# ----------------------------------------------------------------------------
# Transformers 4.43.3: Stable, Llama 3.1 support, compatible with above
transformers==4.43.3
tokenizers==0.19.1

# PEFT for LoRA/QLoRA support
peft==0.11.1

# Accelerate for distributed training
accelerate==0.30.1

# TRL for training utilities
trl==0.9.6

# Datasets for data loading
datasets==2.19.1

# ----------------------------------------------------------------------------
# 5. XFORMERS - REQUIRED BY UNSLOTH
# ----------------------------------------------------------------------------
# xformers provides memory-efficient attention operations
# Required by Unsloth for 2-3Ã— speedup
# Version MUST match PyTorch version
# For PyTorch 2.6.0+cu124: Use xformers 0.0.28.post2 or 0.0.29
# DO NOT use xformers 0.0.32+ (requires torch 2.8.0)
xformers==0.0.28.post2

# ----------------------------------------------------------------------------
# 6. UNSLOTH - OPTIONAL BUT RECOMMENDED (2-3x speedup)
# ----------------------------------------------------------------------------
# Use git version for compatibility (July-2024 tag is stable)
# This provides Unsloth library WITHOUT pre-quantized base
# IMPORTANT: Use with full precision meta-llama/Meta-Llama-3.1-8B-Instruct
# REQUIRES: xformers (installed above)
unsloth @ git+https://github.com/unslothai/unsloth.git@July-2024

# ----------------------------------------------------------------------------
# 7. SUPPORTING LIBRARIES
# ----------------------------------------------------------------------------
# NumPy 1.26.4 (CRITICAL - NOT 2.0+, breaks scipy/sklearn)
numpy==1.26.4

# Scientific computing
scipy==1.13.0
scikit-learn==1.4.2

# HuggingFace Hub (>=0.23.2 required by transformers 4.43.3)
huggingface-hub==0.23.4

# Logging and monitoring
tensorboard==2.16.2
wandb==0.17.0

# Utilities
tqdm==4.66.4
rich==13.7.1

# JSONL handling
jsonlines==4.0.0

# Ninja for faster compilation (if any compilation needed)
ninja==1.11.1

# Packaging for version checks
packaging==24.0

# ----------------------------------------------------------------------------
# 8. JUPYTER SUPPORT (Optional)
# ----------------------------------------------------------------------------
ipykernel==6.29.4
jupyter==1.0.0

# ============================================================================
# INSTALLATION INSTRUCTIONS
# ============================================================================
# 
# 1. Create virtual environment:
#    python3.10 -m venv venv_phase1a_2_0
#    source venv_phase1a_2_0/bin/activate
#
# 2. Upgrade pip:
#    pip install --upgrade pip setuptools wheel
#
# 3. Install all dependencies:
#    pip install -r requirements-stable-precompiled.txt
#
# 4. Verify installation:
#    python scripts/verify_environment.py
#
# EXPECTED INSTALL TIME: 5-10 minutes (all pre-compiled)
# ============================================================================

# ============================================================================
# VERSION RATIONALE
# ============================================================================
# PyTorch 2.3.1: Stable, well-tested, H100 support, CUDA 12.1 compatible
# Flash Attention 2.5.8: Latest with pre-compiled wheels for PyTorch 2.3.x
# Transformers 4.43.3: Stable, Llama 3.1 support, no breaking changes
# Unsloth 2024.7: Stable release, NOT 2024.8+ (has dependency conflicts)
# NumPy 1.26.4: Last 1.x version, 2.0+ breaks many ML packages
# BitsAndBytes 0.43.1: Stable, CUDA 12.1 support, pre-compiled wheels
# PEFT 0.11.1: Compatible with Transformers 4.43.3
# Accelerate 0.30.1: Stable, compatible with above versions
# TRL 0.9.6: Stable, compatible with Transformers 4.43.3
# ============================================================================

# ============================================================================
# KNOWN COMPATIBLE CONFIGURATIONS
# ============================================================================
# H100 80GB: âœ… Tested and verified
# A100 40GB/80GB: âœ… Should work (CUDA 12.1 compatible)
# RTX 4090: âœ… Should work (CUDA 12.1 compatible)
# RTX 4080: âœ… Should work (CUDA 12.1 compatible)
# ============================================================================

# ============================================================================
# TROUBLESHOOTING
# ============================================================================
# If Flash Attention install fails:
#   - Verify CUDA 12.1 is installed: nvcc --version
#   - Verify PyTorch 2.3.1 is installed: python -c "import torch; print(torch.__version__)"
#   - Check GPU: nvidia-smi
#   - Try manual install: pip install flash-attn==2.5.8 --no-build-isolation \
#       --extra-index-url https://flashattn.github.io/whl/cu121/torch2.3/
#
# If Unsloth install fails:
#   - Try without Unsloth (optional): Comment out unsloth line
#   - Training will still work, just slower (no 2-3x speedup)
#
# If NumPy conflicts:
#   - Force reinstall: pip install numpy==1.26.4 --force-reinstall
#   - Verify: python -c "import numpy; print(numpy.__version__)"
# ============================================================================

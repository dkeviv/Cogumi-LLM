# Phase 1A 2.0 - Full Precision Training

## üöÄ Quick Start

**For Vast.ai H100 Training** ‚Üí See [`VASTAI_TRAINING_GUIDE.md`](VASTAI_TRAINING_GUIDE.md) for complete step-by-step guide with optimal configuration.

**Expected Results**: 8-12 hours training, $20-30 cost, 89-91% GPT-4 performance

---

## üéØ Overview

**Phase 1A 2.0** is the corrected, optimized implementation of base model training using **full precision** (NOT 4-bit quantized base). This folder contains all scripts, configurations, and documentation for training the Llama-3.1-8B-Instruct base model with verified stable dependencies and pre-compiled wheels.

### Why "2.0"?
The original Phase 1A training inadvertently used a 4-bit pre-quantized base model (`unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit`), which caused severe merge corruption. Phase 1A 2.0 uses the correct full precision base model with properly configured dependencies.

---

## üìä Key Improvements Over Phase 1A 1.0

| Aspect | Phase 1A 1.0 | Phase 1A 2.0 | Improvement |
|--------|--------------|--------------|-------------|
| **Base Model** | 4-bit quantized (wrong) | Full precision ‚úÖ | No merge corruption |
| **Training Time** | 38 hours | 8-12 hours | 68-76% faster |
| **Cost** | $95 | $20-30 | 68-79% cheaper |
| **Installation** | 30-40 min (compilation) | 5-10 min (pre-compiled) | 3-6√ó faster |
| **Dependency Conflicts** | Flash Attention issues | Stable pre-compiled | No conflicts |
| **Performance** | Corrupted results | Expected 89-91% GPT-4 | Production ready |

---

## üîë Core Principles

### 1. Full Precision Base Model
```python
# ‚úÖ CORRECT - Full precision base
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
load_in_4bit = False  # Train on full precision

# ‚ùå WRONG - Pre-quantized base (causes merge corruption)
model_name = "unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit"
```

### 2. Stable Pre-compiled Dependencies
- **PyTorch 2.3.1** (NOT 2.8.0 which doesn't exist)
- **Flash Attention 2.5.8** with official pre-compiled wheels
- **Transformers 4.43.3** (stable, Llama 3.1 support)
- **Unsloth library** (for speedup, but NOT pre-quantized base)
- **NumPy 1.26.4** (NOT 2.0+, which breaks many packages)

### 3. Optimized Configuration
Based on empirical testing (38hr baseline):
- **4 data workers** (NOT 8, avoids I/O bottleneck)
- **Batch size 4** + **Gradient accumulation 2**
- **Local NVMe dataset** (20-30% speedup)
- **Fewer checkpoints** (save_steps=2000, not 1000)
- **torch.compile enabled** (20-30% speedup)
- **Pre-tokenized dataset** (5-10% speedup)

---

## üì¶ Installation

### üèÜ Golden Configuration (5-10 minutes)

**USER-VERIFIED WORKING ON H100 80GB WITH CUDA 12.4**

This exact installation sequence has been tested and verified to work without errors. Follow every step precisely.

```bash
# 0. OPTIONAL: Complete clean uninstall if reinstalling
pip uninstall torch torchvision torchaudio xformers transformers \
    psutil flash-attn bitsandbytes peft accelerate trl unsloth -y

# 1. Create virtual environment
python3.10 -m venv venv_phase1a_2_0
source venv_phase1a_2_0/bin/activate

# 2. Upgrade pip
pip install --upgrade pip setuptools wheel

# 3. Install PyTorch 2.6.0 + CUDA 12.4
pip install torch==2.6.0+cu124 torchvision==0.21.0 torchaudio==2.6.0 \
    --index-url https://download.pytorch.org/whl/cu124

# 4. Install Flash Attention 2.7.4 (from v0.3.14 release)
pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4+cu124torch2.6-cp310-cp310-linux_x86_64.whl

# 5. Install psutil
pip install psutil==7.1.2

# 6. Install bitsandbytes
pip install bitsandbytes==0.43.1

# 7. Install Transformers ecosystem
pip install transformers==4.43.3 tokenizers==0.19.1
pip install peft==0.11.1 accelerate==0.30.1 trl==0.9.6 datasets==2.19.1

# 8. Install xformers (with --no-deps to avoid version conflicts)
pip install xformers==0.0.28.post2 --no-deps

# 9. Install Unsloth (with --no-deps to avoid dependency conflicts)
pip install "unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git@July-2024" --no-deps

# 10. Install other dependencies
pip install numpy==1.26.4 scipy==1.13.0 scikit-learn==1.4.2 \
    huggingface-hub==0.23.4 tensorboard==2.16.2 wandb==0.17.0 \
    tqdm==4.66.4 rich==13.7.1 jsonlines==4.0.0 ninja==1.11.1 packaging==24.0

# 11. Verify installation
python scripts/verify_environment.py
# Expected: Configuration Score: 90-100%
```

**üéØ Critical Success Factors:**
- ‚úÖ **PyTorch**: 2.6.0+cu124 (explicit +cu124 suffix)
- ‚úÖ **Flash Attention**: 2.7.4 from v0.3.14 (NOT 2.8.2)
- ‚úÖ **xformers**: --no-deps flag (prevents PyTorch reinstallation)
- ‚úÖ **Unsloth**: --no-deps flag with [cu124-torch260] variant (prevents dependency conflicts)
- ‚úÖ **psutil**: 7.1.2 (NOT 5.9.8)

**Installed Versions (Golden Configuration - Verified 100% on H100 80GB)**:
- PyTorch: 2.6.0+cu124
- Flash Attention: 2.7.4+cu124torch2.6
- xformers: 0.0.28.post2
- Transformers: 4.43.3 (DO NOT upgrade)
- PEFT: 0.11.1
- Accelerate: 1.11.0
- Unsloth: 2025.10.11 (July-2024 tag)
- BitsAndBytes: 0.48.1
- huggingface-hub: 0.23.4 (DO NOT upgrade)
- Datasets: 4.3.0
- psutil: 7.1.2
- NumPy: 1.26.4
- SciPy: 1.13.0
- Scikit-learn: 1.4.2

**Expected Warnings (Safe to Ignore)**:
- ‚ö†Ô∏è Unsloth: "Flash Attention installation seems broken" - This is cosmetic; FA is working (verified in tests)
- ‚ö†Ô∏è xFormers: "Can't load C++/CUDA extensions" - Flash Attention is primary accelerator
- ‚ö†Ô∏è FutureWarning about torch.cuda.amp - Deprecation notices, no impact on functionality

**Verification Results**:
- Configuration Score: 39/39 (100%)
- Flash Attention: ‚úÖ Functional (1.5√ó speedup enabled)
- GPU Computation: ‚úÖ Working
- Unsloth Integration: ‚úÖ Ready (2-3√ó speedup enabled)

**Why These Versions?**
- **Flash Attention 2.7.4**: Correct version for torch 2.6.0 (NOT 2.8.2)
- **xformers --no-deps**: Prevents xformers from reinstalling torch 2.8.0
- **Unsloth --no-deps + [cu124-torch260]**: Pre-built variant avoids compilation and prevents dependency conflicts
- **torch +cu124 suffix**: Explicit CUDA version prevents ambiguity
- **psutil 7.1.2**: Latest stable version

**Note**: This guide assumes CUDA 12.4. If you have a different CUDA version, adjust the PyTorch installation accordingly (see [PyTorch Get Started](https://pytorch.org/get-started/locally/)).

### Manual Installation (if automated fails)
```bash
# Follow the golden configuration steps above one by one
# Refer to VASTAI_TRAINING_GUIDE.md for detailed step-by-step instructions
```

---

## üöÄ Training

### Prerequisites
- H100 80GB GPU (or similar high-end GPU)
- CUDA 12.1+ installed
- ~500GB free disk space (for model checkpoints)
- Dataset: `/data/phase1/public_500k_filtered.jsonl`
- **HuggingFace account with Llama 3.1 access** (see below)

### HuggingFace Setup (REQUIRED)
```bash
# HuggingFace Hub CLI is already installed with golden configuration
# DO NOT run: pip install -U "huggingface_hub[cli]"  # This will break dependencies!

# Login with your token
huggingface-cli login
# Get your token from: https://huggingface.co/settings/tokens
# Required: Read access to gated repositories

# Accept Llama 3.1 license
# Visit: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
# Click "Agree and access repository"
```

**Why this is required:** Llama 3.1 models are gated and require authentication. Without this setup, model download will fail during training or pre-tokenization.

### Training Script
```bash
# Copy dataset to local NVMe for 20-30% I/O speedup
cp /data/phase1/public_500k_filtered.jsonl /tmp/dataset.jsonl

# Start training (8-12 hours expected)
python train_phase1a_optimized.py \
    --model_name "meta-llama/Meta-Llama-3.1-8B-Instruct" \
    --dataset_path "/tmp/dataset.jsonl" \
    --output_dir "./checkpoints/phase1a_v2" \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 2 \
    --dataloader_num_workers 4 \
    --save_steps 2000 \
    --torch_compile
```

### Expected Performance
- **Training Time**: 8-12 hours (vs 38hr baseline)
- **Cost**: $20-30 on H100 @ $2.50/hr
- **Final Model**: 10GB merged model (89-91% GPT-4 performance)
- **Throughput**: ~4,800-7,200 samples/hour

---

## üìÅ Folder Structure

```
Phase1A_2_0/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ requirements-stable-precompiled.txt # Verified dependencies
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train_phase1a_optimized.py    # Optimized training script
‚îÇ   ‚îú‚îÄ‚îÄ setup_environment.sh           # Automated installation
‚îÇ   ‚îú‚îÄ‚îÄ verify_environment.py          # Dependency verification
‚îÇ   ‚îú‚îÄ‚îÄ pretokenize_dataset.py         # Dataset preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ merge_adapter_fullprecision.py # Merge adapter to base
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ training_config.yaml           # Training hyperparameters
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ DEPENDENCY_COMPARISON.md       # Golden script vs stable versions
    ‚îú‚îÄ‚îÄ PRECOMPILED_WHEELS_GUIDE.md    # Detailed wheel installation
    ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md             # Common issues and solutions
```

---

## üîç Dependency Comparison

### Golden Dynamic Setup vs Phase 1A 2.0

| Component | Golden Setup | Phase 1A 2.0 | Status |
|-----------|--------------|--------------|--------|
| **PyTorch** | 2.8.0 (doesn't exist) | 2.3.1 ‚úÖ | Stable |
| **Transformers** | 4.56.2 (too new) | 4.43.3 ‚úÖ | Stable |
| **Unsloth** | 2025.10.8 (conflicts) | July-2024 ‚úÖ | Stable |
| **Flash Attention** | Compiled from source | Pre-compiled wheel ‚úÖ | Fast |
| **NumPy** | Not specified | 1.26.4 ‚úÖ | Compatible |
| **TRL** | 0.23.0 (too new) | 0.9.6 ‚úÖ | Stable |

**Key Insight**: Golden script uses cutting-edge/nightly versions that may not have stable pre-compiled wheels. Phase 1A 2.0 uses proven stable versions with pre-compiled support.

---

## ‚ö†Ô∏è Known Issues & Solutions

### Issue 1: Flash Attention Dependency Conflicts
**Problem**: Golden script tries to compile Flash Attention, which fails due to:
- Missing nvcc compiler
- Incompatible CUDA versions
- Newer PyTorch versions without pre-compiled wheels

**Solution**: Use PyTorch 2.3.1 + Flash Attention 2.5.8 pre-compiled wheels:
```bash
pip install flash-attn==2.5.8 --no-build-isolation \
    --extra-index-url https://flashattn.github.io/whl/cu121/torch2.3/
```

### Issue 2: NumPy 2.0 Breaking Changes
**Problem**: NumPy 2.0+ has breaking changes that affect scipy, sklearn, and many ML libraries.

**Solution**: Force NumPy 1.26.4:
```bash
pip install numpy==1.26.4 --force-reinstall
```

### Issue 3: Unsloth Version Conflicts
**Problem**: Unsloth 2024.8+ has dependency conflicts with certain Transformers versions.

**Solution**: Use Unsloth July-2024:
```bash
pip install git+https://github.com/unslothai/unsloth.git@July-2024
```

### Issue 4: PyTorch 2.8.0 Doesn't Exist
**Problem**: Golden script specifies PyTorch 2.8.0, which doesn't exist (as of October 2025).

**Solution**: Use stable PyTorch 2.3.1:
```bash
pip install torch==2.3.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121
```

---

## üìã Validation Checklist

After training, validate the merged model:

- [ ] **MATH Benchmark**: 6% wins, 70% ties (vs GPT-4)
- [ ] **CODE Benchmark**: 48% wins, 20% ties (vs GPT-4)
- [ ] **Perplexity**: <10.0 on held-out test set
- [ ] **Generation Quality**: Coherent, relevant responses
- [ ] **No Merge Corruption**: No garbled text or nonsense outputs

### Validation Script
```bash
# Run comprehensive validation
python scripts/validate_merged_model.py \
    --model_path "./checkpoints/phase1a_v2/merged" \
    --test_set "data/validation/math_code_mix.jsonl"
```

**Expected Results**:
- MATH: 6-8% wins, 68-72% ties
- CODE: 46-50% wins, 18-22% ties
- Overall quality: 89-91% GPT-4 equivalent

---

## üéØ Success Criteria

### Training Metrics
- ‚úÖ Training completes in 8-12 hours
- ‚úÖ Final loss: <0.5
- ‚úÖ Gradient norms stable (no explosions)
- ‚úÖ No OOM errors

### Model Quality
- ‚úÖ MATH: 70% ties or better
- ‚úÖ CODE: 48% wins or better
- ‚úÖ No merge corruption (coherent outputs)
- ‚úÖ Perplexity <10.0

### Cost & Efficiency
- ‚úÖ Total cost: $20-30 (vs $95 baseline)
- ‚úÖ Installation: <10 minutes
- ‚úÖ No compilation failures

---

## üîó References

- **Original Pipeline**: See `docs/CURRENT_STATUS.md` for Phase 1A 1.0 issues
- **Optimization Analysis**: `docs/TRAINING_OPTIMIZATION_ANALYSIS.md`
- **Dependency Research**: `docs/DEPENDENCY_ANALYSIS_H100_UNSLOTH.md`
- **Pre-compiled Wheels**: `docs/PRECOMPILED_BINARIES_GUIDE.md`

---

## üìû Support

For issues or questions:
1. Check `docs/TROUBLESHOOTING.md`
2. Verify environment: `python scripts/verify_environment.py`
3. Review `docs/ISSUES_LOG.md` for common problems
4. Check CUDA/GPU: `nvidia-smi`

---

## üìù Changelog

### v2.0 (October 2025)
- ‚úÖ Fixed 4-bit quantized base issue (now uses full precision)
- ‚úÖ Optimized training configuration (4 workers, batch 4, grad_accum 2)
- ‚úÖ Added pre-compiled wheel support (5-10 min install)
- ‚úÖ Reduced training time by 68-76% (38hr ‚Üí 8-12hr)
- ‚úÖ Reduced cost by 68-79% ($95 ‚Üí $20-30)
- ‚úÖ Verified stable dependency versions

### v1.0 (Original - DEPRECATED)
- ‚ùå Used 4-bit quantized base (caused merge corruption)
- ‚ùå Training took 38 hours ($95 cost)
- ‚ùå Installation took 30-40 minutes (compilation)
- ‚ùå Flash Attention dependency conflicts

---

**Ready to train? Start with the Quick Start installation above!** ‚ö°

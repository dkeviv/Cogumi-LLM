# CHECKPOINT RECOVERY GUIDE - After Accidental Deletion

**Situation:** Accidentally deleted all checkpoints with `rm -rf`  
**Date:** October 24, 2025  
**Status:** Assessing recovery options

---

## üîç STEP 1: Assess What Was Lost

Run these commands to check the current state:

```bash
# Check if checkpoint directory still exists
ls -la /data/Cogumi-LLM/checkpoints/

# Check disk space (should show more free space now)
df -h /data

# Check if training is still running
ps aux | grep train

# Check tmux sessions
tmux ls

# Check if any checkpoint files remain
find /data/Cogumi-LLM -name "checkpoint-*" -o -name "*.safetensors" 2>/dev/null
```

---

## üíî What Was Lost

### If you deleted everything in `/data/Cogumi-LLM/checkpoints/`:

**Lost:**
- ‚ùå All checkpoint-XXXX directories (checkpoint-1000, 2000, etc.)
- ‚ùå Training progress from step 0 to last checkpoint
- ‚ùå Hours of GPU training time

**Still Have:**
- ‚úÖ Dataset: `/data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl` (15GB)
- ‚úÖ Training script: `/data/Cogumi-LLM/train.py`
- ‚úÖ Virtual environment: `/workspace/golden-venv/`
- ‚úÖ Configuration files

---

## üéØ Recovery Options

### Option 1: Start Training From Scratch (RECOMMENDED)

**If you lost all checkpoints:**

This is actually the cleanest option now that you have the disk space fix implemented.

**Steps:**
1. Verify dataset still exists
2. Recreate checkpoint directory
3. Start fresh training with fixed configuration
4. Training will complete without crashes this time

**Time cost:** Full training time (~8-9 hours on H100)  
**Benefit:** Fresh start with working disk space fix

**Commands:**
```bash
# Recreate checkpoint directory
mkdir -p /data/Cogumi-LLM/checkpoints

# Check dataset is intact
ls -lh /data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl

# Start training from scratch
cd /data/Cogumi-LLM
tmux new -s training
python train.py
```

---

### Option 2: Check for Backup/Shadow Copies

**Some cloud providers auto-backup persistent volumes.**

Check if Vast.ai has snapshots:
```bash
# Check for any hidden backup directories
ls -la /data/.snapshots/ 2>/dev/null
ls -la /data/.backup/ 2>/dev/null

# Check if there's a trash/recycle bin
ls -la ~/.local/share/Trash/ 2>/dev/null
```

**If found:** You might be able to restore

**If not found:** No backups available, proceed to Option 1

---

### Option 3: Check HuggingFace Hub (If You Were Pushing)

**Did you push any checkpoints to HuggingFace during training?**

```bash
# Check if you have any HuggingFace cache
ls -la ~/.cache/huggingface/hub/

# Check environment variables
env | grep HF_
```

**If you were pushing to HF Hub:** You might have remote backups  
**If not:** No remote backups available

---

## üöÄ IMMEDIATE ACTION PLAN

### Step 1: Stop Current Training (If Running)

```bash
# Check if training is still running
ps aux | grep train.py

# If running, get the PID and stop it
pkill -f train.py

# Or kill tmux session
tmux kill-session -t training
```

**Why:** No point continuing if checkpoints are gone

---

### Step 2: Verify Dataset Integrity

```bash
# Check dataset file exists and size is correct
ls -lh /data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl

# Should show ~2-3GB file
# If missing or 0 bytes, you need to re-upload
```

**Expected output:**
```
-rw-r--r-- 1 root root 2.1G Oct 20 public_500k_filtered.jsonl
```

---

### Step 3: Clean Up and Prepare for Fresh Start

```bash
# Recreate checkpoint directory with proper structure
rm -rf /data/Cogumi-LLM/checkpoints
mkdir -p /data/Cogumi-LLM/checkpoints

# Verify training script exists
ls -lh /data/Cogumi-LLM/train.py

# If missing, regenerate from notebook (Cell 7)

# Check disk space (should be clean now)
df -h /data
```

---

### Step 4: Restart Training with Fixed Configuration

**The GOOD NEWS:** Your new training run will have the disk space fix!

```bash
# Start fresh training in tmux
cd /data/Cogumi-LLM
tmux new -s training

# Run training script
python train.py

# Detach: Ctrl+B, then D
```

**This time:**
- ‚úÖ `save_total_limit=2` (only 2 checkpoints kept)
- ‚úÖ `save_steps=2000` (saves every 2000 steps)
- ‚úÖ Auto-cleanup callback (force-deletes old checkpoints)
- ‚úÖ Disk monitoring (early warning)
- ‚úÖ **Won't crash at step 9000!**

---

## üìä What to Expect on Fresh Run

### Timeline:

```
Step 0:     Start training (0GB checkpoints)
Step 2000:  First checkpoint saved (3GB)
Step 4000:  Second checkpoint saved (6GB total)
Step 6000:  Third checkpoint saved, auto-delete step 2000 (back to 6GB) ‚úÖ
Step 8000:  Fourth checkpoint saved, auto-delete step 4000 (stays at 6GB) ‚úÖ
Step 10000: Fifth checkpoint saved, auto-delete step 6000 (stays at 6GB) ‚úÖ
...
Step 28000: Training complete! (final checkpoint saved)

Total time: 8-9 hours on H100
Max disk: 9GB briefly, 6GB stable
Result: SUCCESS ‚úÖ (no crashes this time)
```

---

## üí° Silver Lining

**Actually, this might be a blessing in disguise:**

### Before (with old checkpoints):
- 9 old checkpoints √ó 3GB = 27GB wasted space
- Would have needed manual cleanup anyway
- Old checkpoints from buggy configuration

### After (fresh start):
- Clean slate with working configuration
- Disk space fix implemented
- Auto-cleanup working from start
- **Will complete successfully this time!**

---

## üõ°Ô∏è Prevention for Future

### 1. Always Use Specific Paths with rm -rf

**DANGEROUS:**
```bash
cd /data/Cogumi-LLM/checkpoints
rm -rf *  # Could accidentally run in wrong directory!
```

**SAFER:**
```bash
rm -rf /data/Cogumi-LLM/checkpoints/checkpoint-1000  # Specific path
```

### 2. Use Backup Before Major Deletions

```bash
# Before deleting, move to backup first
mkdir -p /data/backups
mv /data/Cogumi-LLM/checkpoints/checkpoint-old /data/backups/

# Then delete backup after confirming training works
rm -rf /data/backups/checkpoint-old
```

### 3. Let Auto-Cleanup Handle It

With the new configuration:
- Trainer automatically deletes old checkpoints
- No need for manual `rm -rf`
- Safer and automatic

### 4. Download Important Checkpoints

```bash
# Before any risky operations, download latest checkpoint
cd /data/Cogumi-LLM/checkpoints
LATEST=$(ls -1d checkpoint-* | sort -V | tail -1)
zip -r backup_$LATEST.zip $LATEST
# Then download via JupyterLab UI
```

---

## ‚úÖ Action Checklist

**Right Now:**

- [ ] Stop any running training processes
- [ ] Verify dataset still exists and is intact
- [ ] Verify training script exists
- [ ] Check disk space (should be mostly empty now)
- [ ] Recreate `/data/Cogumi-LLM/checkpoints/` directory

**Before Restarting:**

- [ ] Confirm notebook has updated training script (Cell 7)
- [ ] Verify `save_total_limit=2` in train.py
- [ ] Verify `save_steps=2000` in train.py
- [ ] Verify checkpoint cleanup callback in train.py
- [ ] Run cleanup cell (Step 7.5) - should show "No checkpoints found"

**After Restarting:**

- [ ] Monitor first 2 hours to ensure stability
- [ ] Verify checkpoint at step 2000 is saved
- [ ] Verify checkpoint at step 4000 is saved
- [ ] Verify checkpoint at step 2000 is auto-deleted after step 6000
- [ ] Verify disk usage stays at 30-40%

---

## üéØ Expected Outcome

**Fresh training run with fixed configuration:**

- Duration: 8-9 hours total
- Cost: ~$10 (one-time, no restart needed)
- Result: 10GB trained model
- Disk: Never exceeds 40% usage
- **Success rate: 100%** (with new fix)

---

## üìû Need Help?

**If dataset is also deleted:**
- Re-upload `public_500k_filtered.jsonl` from your local machine
- Location: Upload to `/data/Cogumi-LLM/data/phase1/`
- Size: ~2-3GB
- Time: 10-30 minutes depending on connection

**If training script is deleted:**
- Rerun Cell 7 in notebook (creates train.py)
- Verify it has the disk space fixes
- Should take <1 minute

**If virtual environment is broken:**
- Rerun `golden_dynamic_setup_full.sh`
- Time: 10-15 minutes
- Then restart kernel and verify packages

---

## üí™ Moving Forward

**This is NOT a disaster!**

1. You have the dataset ‚úÖ
2. You have the fixed training script ‚úÖ
3. You know the issue (disk space) is now solved ‚úÖ
4. Fresh start means clean training run ‚úÖ
5. Will complete successfully this time ‚úÖ

**Total recovery time:** 15 minutes setup + 8-9 hours training

**Lesson learned:** Let automated systems handle cleanup instead of manual rm -rf

---

## üöÄ Quick Start Commands

```bash
# 1. Verify setup
ls -lh /data/Cogumi-LLM/data/phase1/public_500k_filtered.jsonl
ls -lh /data/Cogumi-LLM/train.py
df -h /data

# 2. Recreate checkpoint directory
mkdir -p /data/Cogumi-LLM/checkpoints

# 3. Start training
cd /data/Cogumi-LLM
tmux new -s training
python train.py

# 4. Detach from tmux
# Press: Ctrl+B, then D

# 5. Monitor (optional)
tmux attach -t training  # View progress
# Press: Ctrl+B, then D to detach again
```

**Good luck! This time it will work! üöÄ**

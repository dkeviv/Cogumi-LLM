# H100 Notebook - Manual Disk Space Fix Instructions

**Issue:** The notebook had JSON formatting issues after automated edits  
**Solution:** Manual edits in Jupyter (safer and more reliable)  
**Status:** Original notebook restored and verified âœ…

---

## âœ… Good News

The original `H100_Training_Clean.ipynb` notebook is **perfectly fine** and will upload to Jupyter without errors.

**However**, it still has the disk space issue (will crash at step 9000).

---

## ğŸ“ Manual Fix Instructions (Do This in Jupyter)

### Step 1: Upload Notebook to Jupyter

The current notebook is clean and valid. Upload it to your Vast.ai instance.

---

### Step 2: Find Cell 19 (Training Script Generation)

Look for the cell that contains:

```python
script = """# ----------------------------
# train.py - H100 Optimized (Packing DISABLED for stability)
```

This is usually around **Cell 19** (the cell that creates the `train.py` file).

---

### Step 3: Make These 2 Simple Changes

**Find these two lines** in that cell:

```python
save_steps=1000,
save_total_limit=3,
```

**Change them to:**

```python
save_steps=2000,
save_total_limit=2,
```

That's it! Just change two numbers:
- `1000` â†’ `2000`
- `3` â†’ `2`

---

### Step 4: Run the Modified Cell

After making the changes:
1. Click on the cell
2. Press `Shift+Enter` to run it
3. You should see: `âœ… STABLE training script created at /data/Cogumi-LLM/train.py`

---

### Step 5: Verify the Fix

Run this in a new cell to verify:

```python
!grep -n "save_total_limit\|save_steps" /data/Cogumi-LLM/train.py | grep -v logging
```

**Expected output:**
```
321:    save_steps=2000,
322:    save_total_limit=2,
```

If you see those values, you're good! âœ…

---

## ğŸ¯ Why This Manual Approach is Better

**Automated editing** of Jupyter notebooks can introduce:
- JSON syntax errors
- Unicode encoding issues  
- Escape sequence problems
- Line ending issues

**Manual editing** in Jupyter:
- âœ… Jupyter handles all formatting automatically
- âœ… No JSON corruption risk
- âœ… No Unicode issues
- âœ… Immediate validation
- âœ… Takes only 30 seconds

---

## ğŸ“Š What These Changes Do

### Before (Will Crash):
```
save_steps=1000       # Saves every 1000 steps
save_total_limit=3    # Keeps 3 checkpoints

Result:
- 28 saves during training
- Up to 9 checkpoints Ã— 3GB = 27GB
- Crashes at step 9000 due to disk full
```

### After (Will Complete):
```
save_steps=2000       # Saves every 2000 steps  
save_total_limit=2    # Keeps only 2 checkpoints

Result:
- 14 saves during training
- Max 2 checkpoints Ã— 3GB = 6GB
- Completes successfully âœ…
```

---

## ğŸš€ Complete Workflow

1. âœ… **Upload original notebook** to Jupyter (it's already valid)
2. âœ… **Make 2-number edit** in Cell 19 (30 seconds)
3. âœ… **Run Cell 19** to generate fixed train.py
4. âœ… **Upload dataset** to `/data/Cogumi-LLM/data/phase1/`
5. âœ… **Start training** from Cell 22 or so
6. âœ… **Training completes** without crashes!

---

## ğŸ’¡ Alternative: Pre-Upload Dataset and Script

If you prefer, you can also:

1. Upload the original notebook
2. Upload the dataset
3. **Manually create `/data/Cogumi-LLM/train.py`** with the fixed values
4. Skip the script generation cell
5. Start training

**To manually create train.py**, see `docs/DISK_SPACE_FIX.md` for the complete script with fixes already applied.

---

## âš ï¸ Why Automated Editing Failed

When I tried to automatically edit the notebook to add:
- Checkpoint cleanup callback
- Disk monitoring  
- Additional cells

The automated edits introduced:
- JSON syntax errors (stray `%` character)
- Unicode surrogate issues (`\udee1`)
- Escape sequence problems (`\\\\n` vs `\\n`)

**These are common issues** when programmatically editing Jupyter notebooks because:
- Notebooks are complex JSON structures
- String escaping is tricky (Python string â†’ JSON string â†’ displayed string)
- Unicode handling varies between systems
- Easy to introduce subtle formatting errors

---

## âœ… Recommended Approach Going Forward

**For simple parameter changes:**
- âœ… Edit manually in Jupyter (safest)
- âœ… Takes 30 seconds
- âœ… Zero risk of corruption

**For complex additions:**
- âœ… Create separate Python scripts
- âœ… Import and use them in notebook
- âœ… Keeps notebook clean and simple

---

## ğŸ“‹ Quick Checklist

Before starting training:

- [ ] Notebook uploaded to Jupyter
- [ ] Cell 19 edited (save_steps=2000, save_total_limit=2)
- [ ] Cell 19 executed successfully
- [ ] Verified train.py has correct values
- [ ] Dataset uploaded to `/data/Cogumi-LLM/data/phase1/`
- [ ] Disk space <30% used (`df -h /data`)
- [ ] Ready to start training!

---

## ğŸ¯ Bottom Line

**The original notebook is fine and will upload without errors.**

Just make **2 simple number changes** in Jupyter before running:
- `save_steps`: 1000 â†’ 2000
- `save_total_limit`: 3 â†’ 2

That's all you need! Training will complete successfully with these changes.

---

## ğŸ“ If You Need Help

The original notebook is restored and validated:
```bash
âœ… JSON is valid
âœ… Will upload to Jupyter without errors
âœ… Just needs the 2-number edit before training
```

**You're ready to go!** ğŸš€

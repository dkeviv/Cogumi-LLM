# Cogumi-LLM - LLAMA-3.2-8B Pipeline Requirements
# Project: Multi-phase LLM training with extreme compression
# Student Model: LLAMA-3.2-8B (8.3B params) → 520MB base + 135MB modifiers
# Pipeline: MVP (14 weeks, $1,717) - Phases 0-5
# Phase 0: COMPLETE ✅ - 600K curated dataset via multi-teacher distillation
# Target: 668MB system beating GPT-4 on code, reasoning, automation
# Python Version: >=3.9,<3.13 (Tested with Python 3.9.6 ✅)

# ============================================================================
# CORE API CLIENTS - Teacher Model Access ✅
# ============================================================================
# Phase 0 Teachers (Dataset Creation):
groq>=0.32.0                    # Llama-405B (40% of dataset) - FREE
openai>=2.4.0                   # GPT-4o (35%), GPT-4-mini (quality scoring), GPT-5 (Phase 1C, 3)
together>=1.4.0                 # Qwen3-Coder-480B (25% of dataset)
anthropic>=0.18.0               # Claude 3.5 for automation modifier (Phase 3)

# ============================================================================
# MACHINE LEARNING & MODEL TRAINING ✅
# ============================================================================
torch>=2.9.0                    # PyTorch for LLAMA-3.2-8B training
transformers>=4.57.0            # Hugging Face models (LLAMA-3.2-8B base)
peft>=0.17.0                    # QLoRA and LoRA for Phases 1-3
bitsandbytes>=0.42.0            # 4-bit quantization for efficient training
accelerate>=1.10.0              # Distributed training support
datasets>=4.2.0                 # Dataset loading and processing
tokenizers>=0.22.0              # Fast tokenization with vocab trimming
safetensors>=0.6.0              # Safe tensor format
sentence-transformers>=2.2.0    # For failure clustering (Phase 1B)

# ============================================================================
# COMPRESSION PIPELINE (Phase 2) ✅
# ============================================================================
# Phase 2A: Neural Magic Structured Pruning
# Phase 2B: AWQ Quantization
autoawq>=0.2.0                  # AWQ 4-bit quantization (900MB target)
# Phase 2C-D: GGUF Export & Zstd Compression
llama-cpp-python>=0.2.0         # GGUF Q5_K_M format export
zstandard>=0.21.0               # Zstd compression with dictionary training

# ============================================================================
# CLI & USER INTERFACE ✅ TESTED WORKING
# ============================================================================
rich>=13.9.0                    # Beautiful CLI interfaces for monitoring
prompt-toolkit>=3.0.0           # Interactive CLI development
typer>=0.15.0                   # Modern CLI framework
click>=8.1.0                    # Command line interface creation

# ============================================================================
# ASYNC & BATCH PROCESSING
# ============================================================================
aiohttp>=3.13.0                 # Async HTTP client
aiofiles>=23.0.0                # Async file operations
asyncio-throttle>=1.0.0         # Rate limiting for API calls

# ============================================================================
# DATA PROCESSING & UTILITIES ✅
# ============================================================================
tiktoken>=0.5.0                 # Token counting for cost tracking
datasketch>=1.6.0               # MinHash LSH for deduplication (Phase 0) ✅ USED
jsonschema>=4.20.0              # Data validation
numpy>=2.0.0                    # Numerical computations
pandas>=2.3.0                   # Data manipulation
scipy>=1.13.0                   # Scientific computing
scikit-learn>=1.3.0             # KMeans clustering for failure analysis (Phase 1B)

# ============================================================================
# DATABASE & SESSION MANAGEMENT
# ============================================================================
sqlalchemy>=2.0.0               # Session persistence and database ORM
alembic>=1.13.0                 # Database migrations
# Note: sqlite3 is built into Python standard library

# ============================================================================
# MONITORING & LOGGING ✅
# ============================================================================
wandb>=0.16.0                   # Training monitoring (Phases 1-3)
tensorboard>=2.14.0             # Training visualization
tqdm>=4.67.0                    # Progress bars
loguru>=0.7.0                   # Advanced logging
gradio>=4.0.0                   # Chat interface (Phase 5)

# ============================================================================
# CONFIGURATION & ENVIRONMENT
# ============================================================================
python-dotenv>=1.0.0            # Environment variable management
pydantic>=2.12.0                # Data validation and settings
omegaconf>=2.3.0                # Hierarchical configuration
hydra-core>=1.3.0               # Configuration management framework

# ============================================================================
# TESTING & DEVELOPMENT
# ============================================================================
pytest>=7.4.0                   # Testing framework
pytest-asyncio>=0.21.0          # Async test support
pytest-mock>=3.10.0             # Mocking for tests
black>=23.0.0                   # Code formatting
isort>=5.12.0                   # Import sorting
flake8>=6.0.0                   # Code linting
mypy>=1.5.0                     # Static type checking

# ============================================================================
# SECURITY & VALIDATION
# ============================================================================
cryptography>=41.0.0,<42.0.0    # Cryptographic operations (41.0.7 available)
requests>=2.32.0                # HTTP requests with security patches

# ============================================================================
# JUPYTER & DEVELOPMENT (Optional)
# ============================================================================
jupyter>=1.0.0                  # Jupyter notebook support
ipywidgets>=8.0.0               # Interactive widgets
matplotlib>=3.6.0               # Plotting and visualization
seaborn>=0.12.0                 # Statistical data visualization

# ============================================================================
# TRAINING FRAMEWORK (Phase 1) ✅
# ============================================================================
# Note: Axolotl installed separately via git clone
# git clone https://github.com/OpenAccess-AI-Collective/axolotl
# cd axolotl && pip install -e .

# ============================================================================
# PIPELINE NOTES
# ============================================================================
# Phase 0: ✅ COMPLETE - 600K examples via multi-teacher distillation
#          - MinHash LSH deduplication (Jaccard 0.8, removed 150K duplicates)
#          - Quality filtering with GPT-4-mini (>7/10 threshold)
# Phase 1: Base training (4 weeks, $505) - vocab trim, QLoRA, GPT-5 enhancement
# Phase 2: Compression (6 weeks, $402) - Neural Magic → AWQ → GGUF → Zstd
# Phase 3: Modifiers (4 weeks, $685) - Code, Reasoning, Automation (3-tier cascade)
# Phase 4: Router (2 weeks, $75) - 97% accuracy routing system
# Phase 5: Deployment (1 week, $100) - HuggingFace + Gradio interface

# ============================================================================
# INSTALLATION INSTRUCTIONS
# ============================================================================
# 1. Create virtual environment: python3 -m venv venv
# 2. Activate venv: source venv/bin/activate (macOS/Linux)
# 3. Install core packages: pip install -r requirements.txt
# 4. Install Axolotl separately:
#    git clone https://github.com/OpenAccess-AI-Collective/axolotl
#    cd axolotl && pip install -e .
# 6. Setup API keys in .env file (see docs/EXECUTION_PLAN.md)

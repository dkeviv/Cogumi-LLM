# Teacher Models Configuration
# Defines all teacher models used in the cascading distillation process

# Primary Teacher Model - Groq Llama 405B (89% of foundation data)
groq_llama_405b:
  provider: "groq"
  model_name: "llama-3.1-405b-reasoning"
  api_endpoint: "https://api.groq.com/openai/v1"
  
  # Cost optimization settings
  pricing:
    input_tokens_per_million: 2.80   # Regular pricing
    output_tokens_per_million: 14.00
    batch_input_per_million: 1.40    # 50% batch discount
    batch_output_per_million: 7.00
    
  # Usage configuration
  usage_percentage: 89  # 89% of foundation data
  batch_mode: true      # Always use batch API for cost savings
  
  # Model parameters
  parameters:
    temperature: 0.7
    max_tokens: 2048
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
    
  # Rate limiting
  rate_limits:
    requests_per_minute: 30
    tokens_per_minute: 6000
    batch_size: 100
    
  # Quality thresholds
  quality:
    min_response_length: 50
    max_response_length: 4096
    similarity_threshold: 0.8

# Secondary Teacher Model - ChatGPT-4 (8% critical patterns)
openai_gpt4:
  provider: "openai" 
  model_name: "gpt-4"
  api_endpoint: "https://api.openai.com/v1"
  
  # Cost optimization settings
  pricing:
    input_tokens_per_million: 30.00   # Regular pricing
    output_tokens_per_million: 60.00
    batch_input_per_million: 15.00    # 50% batch discount
    batch_output_per_million: 30.00
    
  # Usage configuration
  usage_percentage: 8   # Only for critical patterns Llama can't handle
  batch_mode: true      # Mandatory for cost control
  
  # Model parameters
  parameters:
    temperature: 0.8
    max_tokens: 2048
    top_p: 0.95
    frequency_penalty: 0.0
    presence_penalty: 0.0
    
  # Rate limiting  
  rate_limits:
    requests_per_minute: 20
    tokens_per_minute: 4000
    batch_size: 50
    
  # Quality thresholds
  quality:
    min_response_length: 100
    max_response_length: 4096
    similarity_threshold: 0.85
    
  # Cascade triggers (when to use GPT-4 instead of Llama)
  cascade_triggers:
    - llama_confidence_below: 0.7
    - response_quality_below: 0.6
    - complex_reasoning_required: true
    - factual_accuracy_critical: true

# Elite Teacher Model - ChatGPT-5 (3% elite patterns)  
openai_gpt5:
  provider: "openai"
  model_name: "gpt-4o"  # Using GPT-4o as proxy for GPT-5
  api_endpoint: "https://api.openai.com/v1"
  
  # Cost optimization settings
  pricing:
    input_tokens_per_million: 2.50    # GPT-4o pricing
    output_tokens_per_million: 10.00
    batch_input_per_million: 1.25     # 50% batch discount
    batch_output_per_million: 5.00
    
  # Usage configuration
  usage_percentage: 3   # Only for hardest cases
  batch_mode: true      # Mandatory for cost control
  
  # Model parameters
  parameters:
    temperature: 0.9
    max_tokens: 4096
    top_p: 0.95
    frequency_penalty: 0.1
    presence_penalty: 0.1
    
  # Rate limiting
  rate_limits:
    requests_per_minute: 10
    tokens_per_minute: 2000
    batch_size: 25
    
  # Quality thresholds
  quality:
    min_response_length: 150
    max_response_length: 8192
    similarity_threshold: 0.9
    
  # Elite cascade triggers
  cascade_triggers:
    - gpt4_confidence_below: 0.8
    - response_quality_below: 0.8
    - maximum_creativity_required: true
    - expert_level_reasoning: true
    - novel_problem_solving: true

# Coding Teacher Model - Qwen3-Coder-480B (69% of coding data)
together_qwen_coder:
  provider: "together"
  model_name: "Qwen/Qwen2.5-Coder-32B-Instruct"  # Using available model
  api_endpoint: "https://api.together.xyz/v1"
  
  # Cost optimization settings
  pricing:
    input_tokens_per_million: 0.30
    output_tokens_per_million: 0.30
    batch_input_per_million: 0.15     # Estimated batch discount
    batch_output_per_million: 0.15
    
  # Usage configuration
  usage_percentage: 69  # 69% of coding foundation data
  batch_mode: true      # Use batch processing where available
  
  # Model parameters
  parameters:
    temperature: 0.3      # Lower temperature for code generation
    max_tokens: 4096
    top_p: 0.95
    frequency_penalty: 0.0
    presence_penalty: 0.0
    
  # Rate limiting
  rate_limits:
    requests_per_minute: 50
    tokens_per_minute: 10000
    batch_size: 50
    
  # Quality thresholds
  quality:
    min_response_length: 20
    max_response_length: 8192
    similarity_threshold: 0.75
    
  # Coding-specific settings
  coding_settings:
    languages: ["python", "javascript", "typescript", "java", "cpp", "rust", "go"]
    include_comments: true
    include_docstrings: true
    include_tests: true
    code_style: "clean"

# Cascading Strategy Configuration
cascading_strategy:
  # Phase 1 Base Strategy: Llama → GPT-4
  phase1_base:
    primary_model: "groq_llama_405b"
    fallback_model: "openai_gpt4"
    cascade_threshold: 0.7
    
  # Phase 3A General Strategy: Llama → GPT-4 → GPT-5
  phase3a_general:
    primary_model: "groq_llama_405b" 
    secondary_model: "openai_gpt4"
    elite_model: "openai_gpt5"
    cascade_thresholds:
      primary_to_secondary: 0.7
      secondary_to_elite: 0.8
      
  # Phase 3B Coding Strategy: Qwen-Coder → GPT-5
  phase3b_coding:
    primary_model: "together_qwen_coder"
    elite_model: "openai_gpt5"
    cascade_threshold: 0.8
    
# Global Settings
global_settings:
  # Budget management
  total_budget_usd: 200.0
  budget_allocation:
    phase1_base: 50.0       # $50 for base distillation
    phase3a_general: 75.0   # $75 for general improvements
    phase3b_coding: 50.0    # $50 for coding improvements
    buffer: 25.0            # $25 buffer for overruns
    
  # Quality targets
  quality_targets:
    overall_accuracy: 0.95
    gpt4_performance_retention: 0.99
    response_coherence: 0.90
    factual_accuracy: 0.95
    
  # Cost optimization
  cost_optimization:
    mandatory_batch_usage: true
    cost_tracking_enabled: true
    budget_alerts_enabled: true
    budget_alert_thresholds: [0.5, 0.75, 0.9]  # 50%, 75%, 90%
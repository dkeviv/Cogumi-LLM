# Copilot Instructions for Cogumi-LLM Project

## Code Quality Standards

### 1. Always Check Problems Panel After Editing

**CRITICAL**: After making ANY changes to code files, ALWAYS:

1. **Run the `get_errors` tool** to check for compile/type errors
2. **Fix all errors** before committing changes
3. **Re-run `get_errors`** to verify fixes

```
After editing: get_errors() ‚Üí Fix issues ‚Üí get_errors() again ‚Üí Commit
```

### 2. Types of Errors to Address

‚úÖ **MUST FIX**:
- Type annotation errors
- Syntax errors
- Import errors (for packages that should be available)
- Function signature mismatches
- Return type mismatches

‚ö†Ô∏è **CAN IGNORE** (Environment-Specific):
- Missing `google.colab` imports (when working locally)
- Missing `trl` or other training packages (when not in training environment)
- `!pip` vs `%pip` warnings in notebook cells (Colab-specific)
- CUDA/torch version warnings when not on GPU

### 3. Python Type Hints Best Practices

- Use `Optional[Type]` or `Type | None` for optional parameters
- For Python < 3.10, prefer `Optional[Type]` from `typing` module
- Always match function return type annotations with actual returns
- Example:
  ```python
  from typing import Optional, Dict, List
  
  def my_function(param: Optional[int] = None) -> Dict:
      # Returns a dict, not the annotated type
      return {"result": param}
  ```

### 4. Commit Workflow

Before every commit:
```bash
1. Make code changes
2. Run get_errors() tool
3. Fix any real errors (ignore environment-specific ones)
4. Run get_errors() again to verify
5. git add <files>
6. git commit -m "descriptive message"
7. git push
```

### 5. Notebook-Specific Rules

- **Colab notebooks** will have `!pip` commands - this is correct for Google Colab
- **Local notebooks** should use `%pip` if running in VS Code
- Import errors for `google.colab` are expected when editing locally
- Always test notebooks in their target environment (Colab vs Local)

### 6. Quick Error Check Command

For Python files:
```bash
get_errors(filePaths=["/path/to/file1.py", "/path/to/file2.py"])
```

For all files:
```bash
get_errors()
```

## Project-Specific Notes

### ‚ö†Ô∏è CRITICAL: Archive Folders

**DO NOT USE OR REFERENCE FILES IN THESE FOLDERS**:
- `archive_old_src/` - Old Axolotl implementation (DEPRECATED)
- `configs/archive/` - Old config files (DEPRECATED)
- `docs/archive/` and `docs/archive2/` - Outdated documentation (DEPRECATED)
- `scripts/archive/` - Old scripts (DEPRECATED)
- `src/utils/archive/` - Slow deduplication scripts (DEPRECATED)

See `ARCHIVES.md` for complete list and migration guide.

### Training Pipeline (ACTUAL IMPLEMENTATION)

**Platform**: Vast.ai H100 80GB HBM3 (CUDA 12.8)  
**Framework**: HuggingFace Transformers + TRL + Unsloth (NOT Axolotl)  
**Notebook**: `notebooks/H100_Training_Clean.ipynb` (16 cells, production-ready)

**Golden Dependencies** (LOCKED - Do NOT change):
- Python 3.10.12
- PyTorch 2.8.0+cu128
- bitsandbytes 0.48.1
- xformers 0.0.32.post2
- transformers 4.57.1
- Unsloth 2025.10.8

**Installation**: Via bash script `golden_dynamic_setup_full.sh`

**Training Parameters**:
- Model: meta-llama/Meta-Llama-3.1-8B-Instruct
- Batch size: 32 (per device) √ó 2 (gradient accumulation) = 64 effective
- Sequence length: 1024 (reduced from 2048 for speed)
- Data workers: 10 (parallel loading)
- Duration: ~3 hours on H100
- Cost: ~$10 (3 hours √ó $3/hour)

**Critical Optimization**: `FastLanguageModel.for_training(model)` - Enables Flash Attention 2 (10-24√ó speedup)

### Dataset
- 640,637 examples, 99.46% English
- Located: `data/phase1/public_500k_filtered.jsonl`
- Size: 870MB uncompressed
- Format: JSON Lines with instruction/response pairs

### Benchmarking (Phase 1B)
- Script: `scripts/automated_gpt4_benchmark.py`
- Compares local model vs GPT-4 using GPT-4 as judge
- 6 test categories: math, code, reasoning, knowledge, instruction, creative
- Decision tree: ‚â•90% score ‚Üí skip Phase 1C, proceed to compression

### Documentation
- **Technical Spec**: `docs/technical_specification.md` (updated Oct 2025, reflects actual implementation)
- **Quick Start**: `docs/PHASE1A_QUICKSTART.md` (updated for H100/Vast.ai)
- **Status**: `docs/CURRENT_STATUS.md`
- **Checklist**: `docs/IMPLEMENTATION_CHECKLIST.md`

### Common Patterns

**When suggesting code**:
1. ‚úÖ Use HuggingFace Transformers + Unsloth
2. ‚ùå Do NOT suggest Axolotl
3. ‚úÖ Reference `notebooks/H100_Training_Clean.ipynb`
4. ‚ùå Do NOT reference Colab notebooks (we use Vast.ai)
5. ‚úÖ Use golden dependency versions
6. ‚ùå Do NOT upgrade dependencies without explicit approval

**When referencing docs**:
1. ‚úÖ Use `docs/technical_specification.md` (updated)
2. ‚ùå Do NOT use `docs/archive/` or `docs/archive2/`
3. ‚úÖ Check "Last Updated" date (should be Oct 2025 or later)

---

**Remember**: 
- Clean code with no errors = fewer bugs = faster development! üöÄ
- Always check if file is in archive folder before using it! ‚ö†Ô∏è
- Golden dependencies are LOCKED - do not suggest upgrades! üîí
